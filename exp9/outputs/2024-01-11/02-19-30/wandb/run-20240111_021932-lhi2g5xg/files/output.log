
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃[1m    [22m┃[1m Name                        [22m┃[1m Type             [22m┃[1m Params [22m┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ bert                        │ BertModel        │ 89.1 M │
│ 1  │ classifiers                 │ ModuleList       │  6.3 M │
│ 2  │ hidden_layer1               │ ModuleList       │  2.1 M │
│ 3  │ hidden_layer2               │ ModuleList       │  2.1 K │
│ 4  │ sigmoid                     │ Sigmoid          │      0 │
│ 5  │ criterion                   │ BCELoss          │      0 │
│ 6  │ metrics                     │ MetricCollection │      0 │
│ 7  │ metrics_per_label_accuracy  │ MetricCollection │      0 │
│ 8  │ metrics_per_label_precision │ MetricCollection │      0 │
│ 9  │ metrics_per_label_recall    │ MetricCollection │      0 │
│ 10 │ metrics_per_label_f1score   │ MetricCollection │      0 │
└────┴─────────────────────────────┴──────────────────┴────────┘
[1mTrainable params[22m: 15.5 M
[1mNon-trainable params[22m: 82.0 M
[1mTotal params[22m: 97.5 M
[1mTotal estimated model params size (MB)[22m: 390
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of
training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower
value for log_every_n_steps if you want to see logs for the training epoch.
[37mEpoch 0/15[39m [35m━━━━━━━━━━━━━━━━━━╸[90m━━━━━━━━━[39m [37m2/3[39m [37m0:00:04 • 0:00:01[39m [37m37.09it/s[39m [37mv_num: g5xg train/loss:     

                                                                        [37m0.687                       



Epoch 1/15 [35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m3/3[39m [37m0:00:09 • 0:00:00[39m [37m0.22it/s[39m [37mv_num: g5xg train/loss: 0.685



Epoch 2/15 [35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m3/3[39m [37m0:00:09 • 0:00:00[39m [37m0.22it/s[39m [37mv_num: g5xg train/loss: 0.678




Epoch 3/15 [35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m3/3[39m [37m0:00:09 • 0:00:00[39m [37m0.22it/s[39m [37mv_num: g5xg train/loss: 0.683



Epoch 4/15 [35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m3/3[39m [37m0:00:09 • 0:00:00[39m [37m0.21it/s[39m [37mv_num: g5xg train/loss: 0.666



Epoch 5/15 [35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m3/3[39m [37m0:00:09 • 0:00:00[39m [37m0.21it/s[39m [37mv_num: g5xg train/loss: 0.664



Epoch 6/15 [35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m3/3[39m [37m0:00:09 • 0:00:00[39m [37m0.21it/s[39m [37mv_num: g5xg train/loss: 0.675



Epoch 7/15 [35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m3/3[39m [37m0:00:09 • 0:00:00[39m [37m0.22it/s[39m [37mv_num: g5xg train/loss: 0.669



Epoch 8/15 [35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m3/3[39m [37m0:00:09 • 0:00:00[39m [37m0.22it/s[39m [37mv_num: g5xg train/loss: 0.705



Epoch 9/15 [35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m3/3[39m [37m0:00:09 • 0:00:00[39m [37m0.21it/s[39m [37mv_num: g5xg train/loss: 0.672
Epoch 10/15 [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m0/3[39m [37m0:00:00 • -:--:--[39m [37m0.00it/s[39m [37mv_num: g5xg train/loss:     


                                                                        [37m0.661                       


                                                                        [37m0.660                       


                                                                        [37m0.654                       


                                                                        [37m0.674                       


                                                                        [37m0.692                       


                                                                        [37m0.646                       
Epoch 15, global step 48: 'val_loss' reached 0.66127 (best 0.66127), saving model to '/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp9/outputs/2024-01-11/02-19-30/wandb/run-20240111_021932-lhi2g5xg/files/checkpoints/epoch=15.ckpt' as top 1
`Trainer.fit` stopped: `max_epochs=16` reached.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃[1m           Test metric           [22m┃[1m          DataLoader 0           [22m┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│[36m     test/multilabelaccuracy     [39m│[35m       0.7220497131347656        [39m│
│[36m     test/multilabelf1score      [39m│[35m       0.42700397968292236       [39m│
│[36m test/multilabelmatthewscorrcoef [39m│[35m       0.4450208842754364        [39m│
│[36m    test/multilabelprecision     [39m│[35m       0.44850707054138184       [39m│
│[36m      test/multilabelrecall      [39m│[35m       0.49846476316452026       [39m│
│[36m            test_loss            [39m│[35m       0.6594064831733704        [39m│
└─────────────────────────────────┴─────────────────────────────────┘
[37mTesting[39m [35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m3/3[39m [37m0:00:00 • 0:00:00[39m [37m66.68it/s
[?25h