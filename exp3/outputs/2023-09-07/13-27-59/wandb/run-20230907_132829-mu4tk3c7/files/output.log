/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:398: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
Downloading (…)solve/main/vocab.txt: 100% 15.7k/15.7k [00:00<00:00, 22.4MB/s]
Downloading (…)okenizer_config.json: 100% 110/110 [00:00<00:00, 299kB/s]
Downloading (…)lve/main/config.json: 100% 478/478 [00:00<00:00, 1.27MB/s]
test:
    Unnamed: 0  ... binary
0          205  ...      1
1          206  ...      1
2          207  ...      1
3          208  ...      1
4          209  ...      1
5          210  ...      1
6          211  ...      1
7          212  ...      1
8          213  ...      1
9          214  ...      1
10         215  ...      1
11         216  ...      0
12         217  ...      0
13         218  ...      0
14         219  ...      1
15         220  ...      1
16         221  ...      1
17         222  ...      1
18         223  ...      1
19         224  ...      1
20         225  ...      0
21         226  ...      0
22         227  ...      0
23         228  ...      1
24         229  ...      1
25         230  ...      1
26         231  ...      1
27         232  ...      1
28         233  ...      1
29         234  ...      1
30         235  ...      0
31         236  ...      0
32         237  ...      1
33         238  ...      1
34         239  ...      1
35         240  ...      1
36         241  ...      1
37         242  ...      1
38         243  ...      1
39         244  ...      1
40         245  ...      1
41         246  ...      1
42         247  ...      1
43         248  ...      1
44         249  ...      1
45         250  ...      1
46         251  ...      1
47         252  ...      1
48         253  ...      1
49         254  ...      0
[50 rows x 3 columns]





Downloading pytorch_model.bin: 100% 359M/359M [00:09<00:00, 36.2MB/s]
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
┏━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃[1m   [22m┃[1m Name         [22m┃[1m Type             [22m┃[1m Params [22m┃
┡━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ bert         │ BertModel        │ 89.1 M │
│ 1 │ hidden_layer │ Linear           │  6.2 K │
│ 2 │ layer        │ Linear           │      9 │
│ 3 │ criterion    │ BCELoss          │      0 │
│ 4 │ metrics      │ MetricCollection │      0 │
└───┴──────────────┴──────────────────┴────────┘
[1mTrainable params[22m: 7.1 M
[1mNon-trainable params[22m: 82.0 M
[1mTotal params[22m: 89.1 M
[1mTotal estimated model params size (MB)[22m: 356

[?25l
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:281:
PossibleUserWarning: The number of training batches (6) is smaller than the
logging interval Trainer(log_every_n_steps=50). Set a lower value for
log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
[37mEpoch 0/3 [39m [35m━━━[90m╺━━━━━━━━━━━━━━[39m [37m1/6[39m [37m0:01:06 • -:--:--[39m [37m0.00it/s[39m [37mv_num: k3c7        





                                                             [37mtrain/loss: 0.669  

[37mValidation[39m [35m━━━━━━━━━[90m╺━━━━━━━━[39m [37m1/2[39m [37m0:00:52 • -:--:--[39m [37m0.00it/s






                                                             [37mtrain/loss: 0.541  
[37mValidation[39m [35m━━━━━━━━━[90m╺━━━━━━━━[39m [37m1/2[39m [37m0:00:50 • -:--:--[39m [37m0.00it/s







                                                             [37mtrain/loss: 0.458  
[37mValidation[39m [35m━━━━━━━━━[90m╺━━━━━━━━[39m [37m1/2[39m [37m0:00:51 • -:--:--[39m [37m0.00it/s







                                                             [37mtrain/loss: 0.412  

[37mValidation[39m [35m━━━━━━━━━[90m╺━━━━━━━━[39m [37m1/2[39m [37m0:00:51 • -:--:--[39m [37m0.00it/s
Epoch 3, global step 24: 'val_loss' reached 0.54386 (best 0.54386), saving model to '/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp3/outputs/2023-09-07/13-27-59/wandb/run-20230907_132829-mu4tk3c7/files/checkpoints/epoch=3.ckpt' as top 1
`Trainer.fit` stopped: `max_epochs=4` reached.

実行済み
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃[1m        Test metric        [22m┃[1m       DataLoader 0        [22m┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│[36m         test_loss         [39m│[35m    0.5223554968833923     [39m│
└───────────────────────────┴───────────────────────────┘
[37mTesting[39m [35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m2/2[39m [37m0:01:22 • 0:00:00[39m [37m0.03it/s
[?25h