{"val_loss": 0.6705890893936157, "val/multilabelaccuracy": 0.8469005227088928, "val/multilabelprecision": 0.2532697021961212, "val/multilabelrecall": 0.21353410184383392, "val/multilabelf1score": 0.21127331256866455, "val/multilabelmatthewscorrcoef": 0.5906583070755005, "val/accuracy_label_\u3042\u3044\u3065\u3061": 0.9482288956642151, "val/presicion_label_\u3042\u3044\u3065\u3061": 0.9470751881599426, "val/recall_label_\u3042\u3044\u3065\u3061": 1.0, "val/f1score_label_\u3042\u3044\u3065\u3061": 0.9728183150291443, "val/accuracy_label_\u611f\u5fc3": 0.8828337788581848, "val/presicion_label_\u611f\u5fc3": 0.8795518279075623, "val/recall_label_\u611f\u5fc3": 1.0, "val/f1score_label_\u611f\u5fc3": 0.935916543006897, "val/accuracy_label_\u8a55\u4fa1": 0.6512261629104614, "val/presicion_label_\u8a55\u4fa1": 0.0, "val/recall_label_\u8a55\u4fa1": 0.0, "val/f1score_label_\u8a55\u4fa1": 0.0, "val/accuracy_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.6866484880447388, "val/presicion_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.6528662443161011, "val/recall_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.971563994884491, "val/f1score_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.7809523940086365, "val/accuracy_label_\u540c\u610f": 0.6893733143806458, "val/presicion_label_\u540c\u610f": 0.7714285850524902, "val/recall_label_\u540c\u610f": 0.2030075192451477, "val/f1score_label_\u540c\u610f": 0.3214285671710968, "val/accuracy_label_\u7d0d\u5f97": 0.6948229074478149, "val/presicion_label_\u7d0d\u5f97": 0.0, "val/recall_label_\u7d0d\u5f97": 0.0, "val/f1score_label_\u7d0d\u5f97": 0.0, "val/accuracy_label_\u9a5a\u304d": 0.7629427909851074, "val/presicion_label_\u9a5a\u304d": 0.1428571492433548, "val/recall_label_\u9a5a\u304d": 0.025974025949835777, "val/f1score_label_\u9a5a\u304d": 0.04395604506134987, "val/accuracy_label_\u8a00\u3044\u63db\u3048": 1.0, "val/presicion_label_\u8a00\u3044\u63db\u3048": 0.0, "val/recall_label_\u8a00\u3044\u63db\u3048": 0.0, "val/f1score_label_\u8a00\u3044\u63db\u3048": 0.0, "val/accuracy_label_\u610f\u898b": 0.8692098259925842, "val/presicion_label_\u610f\u898b": 0.0, "val/recall_label_\u610f\u898b": 0.0, "val/f1score_label_\u610f\u898b": 0.0, "val/accuracy_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.9346048831939697, "val/presicion_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.0, "val/recall_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.0, "val/f1score_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.0, "val/accuracy_label_\u4e0d\u540c\u610f": 0.9564032554626465, "val/presicion_label_\u4e0d\u540c\u610f": 0.0, "val/recall_label_\u4e0d\u540c\u610f": 0.0, "val/f1score_label_\u4e0d\u540c\u610f": 0.0, "val/accuracy_label_\u88dc\u5b8c": 0.8228882551193237, "val/presicion_label_\u88dc\u5b8c": 0.0, "val/recall_label_\u88dc\u5b8c": 0.0, "val/f1score_label_\u88dc\u5b8c": 0.0, "val/accuracy_label_\u3042\u3044\u3055\u3064": 0.9700272679328918, "val/presicion_label_\u3042\u3044\u3055\u3064": 0.0, "val/recall_label_\u3042\u3044\u3055\u3064": 0.0, "val/f1score_label_\u3042\u3044\u3055\u3064": 0.0, "val/accuracy_label_\u60f3\u8d77": 0.9918256402015686, "val/presicion_label_\u60f3\u8d77": 0.0, "val/recall_label_\u60f3\u8d77": 0.0, "val/f1score_label_\u60f3\u8d77": 0.0, "val/accuracy_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.9945504069328308, "val/presicion_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "val/recall_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "val/f1score_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "val/accuracy_label_\u305d\u306e\u4ed6": 0.6948229074478149, "val/presicion_label_\u305d\u306e\u4ed6": 0.6585366129875183, "val/recall_label_\u305d\u306e\u4ed6": 0.2160000056028366, "val/f1score_label_\u305d\u306e\u4ed6": 0.3253012001514435, "epoch": 4, "trainer/global_step": 184, "_timestamp": 1704818431.7050078, "_runtime": 112.37359189987183, "_step": 11, "train_loss": 0.6722334027290344, "train/multilabelaccuracy": 0.822098970413208, "train/multilabelprecision": 0.261800080537796, "train/multilabelrecall": 0.2504628002643585, "train/multilabelf1score": 0.24296143651008606, "train/multilabelmatthewscorrcoef": 0.5347074270248413, "train/accuracy_label_\u3042\u3044\u3065\u3061": 0.9529010057449341, "train/presicion_label_\u3042\u3044\u3065\u3061": 0.9530710577964783, "train/recall_label_\u3042\u3044\u3065\u3061": 0.9992763996124268, "train/f1score_label_\u3042\u3044\u3065\u3061": 0.9756270051002502, "train/accuracy_label_\u611f\u5fc3": 0.8464163541793823, "train/presicion_label_\u611f\u5fc3": 0.8457261919975281, "train/recall_label_\u611f\u5fc3": 0.9975410103797913, "train/f1score_label_\u611f\u5fc3": 0.9153817296028137, "train/accuracy_label_\u8a55\u4fa1": 0.6368600726127625, "train/presicion_label_\u8a55\u4fa1": 0.0, "train/recall_label_\u8a55\u4fa1": 0.0, "train/f1score_label_\u8a55\u4fa1": 0.0, "train/accuracy_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.6621160507202148, "train/presicion_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.634658694267273, "train/recall_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.9906322956085205, "train/f1score_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.7736625671386719, "train/accuracy_label_\u540c\u610f": 0.6703071594238281, "train/presicion_label_\u540c\u610f": 0.4575163424015045, "train/recall_label_\u540c\u610f": 0.1489361673593521, "train/f1score_label_\u540c\u610f": 0.2247191071510315, "train/accuracy_label_\u7d0d\u5f97": 0.6976109147071838, "train/presicion_label_\u7d0d\u5f97": 0.0, "train/recall_label_\u7d0d\u5f97": 0.0, "train/f1score_label_\u7d0d\u5f97": 0.0, "train/accuracy_label_\u9a5a\u304d": 0.6764504909515381, "train/presicion_label_\u9a5a\u304d": 0.20000000298023224, "train/recall_label_\u9a5a\u304d": 0.17307692766189575, "train/f1score_label_\u9a5a\u304d": 0.1855670064687729, "train/accuracy_label_\u8a00\u3044\u63db\u3048": 0.9993174076080322, "train/presicion_label_\u8a00\u3044\u63db\u3048": 0.0, "train/recall_label_\u8a00\u3044\u63db\u3048": 0.0, "train/f1score_label_\u8a00\u3044\u63db\u3048": 0.0, "train/accuracy_label_\u610f\u898b": 0.8279863595962524, "train/presicion_label_\u610f\u898b": 0.16438356041908264, "train/recall_label_\u610f\u898b": 0.05911330133676529, "train/f1score_label_\u610f\u898b": 0.08695652335882187, "train/accuracy_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.9139931797981262, "train/presicion_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.0, "train/recall_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.0, "train/f1score_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.0, "train/accuracy_label_\u4e0d\u540c\u610f": 0.9433447122573853, "train/presicion_label_\u4e0d\u540c\u610f": 0.0, "train/recall_label_\u4e0d\u540c\u610f": 0.0, "train/f1score_label_\u4e0d\u540c\u610f": 0.0, "train/accuracy_label_\u88dc\u5b8c": 0.7897610664367676, "train/presicion_label_\u88dc\u5b8c": 0.29807692766189575, "train/recall_label_\u88dc\u5b8c": 0.11654135584831238, "train/f1score_label_\u88dc\u5b8c": 0.16756756603717804, "train/accuracy_label_\u3042\u3044\u3055\u3064": 0.9549487829208374, "train/presicion_label_\u3042\u3044\u3055\u3064": 0.10526315867900848, "train/recall_label_\u3042\u3044\u3055\u3064": 0.03921568766236305, "train/f1score_label_\u3042\u3044\u3055\u3064": 0.05714285746216774, "train/accuracy_label_\u60f3\u8d77": 0.9604095816612244, "train/presicion_label_\u60f3\u8d77": 0.025641025975346565, "train/recall_label_\u60f3\u8d77": 0.0476190485060215, "train/f1score_label_\u60f3\u8d77": 0.03333333507180214, "train/accuracy_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.9726962447166443, "train/presicion_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "train/recall_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "train/f1score_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "train/accuracy_label_\u305d\u306e\u4ed6": 0.6484641432762146, "train/presicion_label_\u305d\u306e\u4ed6": 0.5044642686843872, "train/recall_label_\u305d\u306e\u4ed6": 0.43545278906822205, "train/f1score_label_\u305d\u306e\u4ed6": 0.46742501854896545, "train/loss": 0.6733094453811646, "test_loss": 0.6694928407669067, "test/multilabelaccuracy": 0.8511996865272522, "test/multilabelprecision": 0.2354406714439392, "test/multilabelrecall": 0.2183763086795807, "test/multilabelf1score": 0.21377980709075928, "test/multilabelmatthewscorrcoef": 0.6037576794624329, "_wandb": {"runtime": 112}}