
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ[1m   [22mâ”ƒ[1m Name              [22mâ”ƒ[1m Type             [22mâ”ƒ[1m Params [22mâ”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ bert              â”‚ BertModel        â”‚ 89.1 M â”‚
â”‚ 1 â”‚ classifiers       â”‚ ModuleList       â”‚ 12.6 M â”‚
â”‚ 2 â”‚ hidden_layer      â”‚ ModuleList       â”‚ 16.4 K â”‚
â”‚ 3 â”‚ sigmoid           â”‚ Sigmoid          â”‚      0 â”‚
â”‚ 4 â”‚ criterion         â”‚ BCELoss          â”‚      0 â”‚
â”‚ 5 â”‚ metrics           â”‚ MetricCollection â”‚      0 â”‚
â”‚ 6 â”‚ metrics_per_label â”‚ MetricCollection â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[22m: 19.7 M
[1mNon-trainable params[22m: 82.0 M
[1mTotal params[22m: 101 M
[1mTotal estimated model params size (MB)[22m: 406
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of
training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a
lower value for log_every_n_steps if you want to see logs for the training epoch.
[37mEpoch 0/3 [39m [35mâ”â”â”[90mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m3/26[39m [37m0:00:02 â€¢ 0:00:13[39m [37m1.78it/s[39m [37mv_num: mz9w train/loss:     








                                                                        [37m0.692                       


        0.5000, 0.5220, 0.5173, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5021, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5129, 0.5000,
        0.5000, 0.5064, 0.5246, 0.5029, 0.5000, 0.5000, 0.5085, 0.5116, 0.5000,
        0.5006, 0.5010, 0.5000, 0.5000, 0.5072, 0.5076, 0.5051, 0.5000, 0.5049,
        0.5000, 0.5000, 0.5093, 0.5000, 0.5000, 0.5030, 0.5000, 0.5004, 0.5000,
        0.5000, 0.5067, 0.5098, 0.5000, 0.5000, 0.5000, 0.5094, 0.5000, 0.5000,
        0.5000, 0.5239, 0.5000, 0.5242, 0.5293, 0.5068, 0.5079, 0.5000, 0.5000,
        0.5117, 0.5000, 0.5000, 0.5000, 0.5000, 0.5048, 0.5000, 0.5000, 0.5015,
        0.5096, 0.5000, 0.5049, 0.5018, 0.5154, 0.5307, 0.5401, 0.5000, 0.5000,
        0.5232, 0.5182, 0.5036, 0.5119, 0.5099, 0.5000, 0.5000, 0.5000, 0.5047,
        0.5000, 0.5104, 0.5000, 0.5000, 0.5000, 0.5000, 0.5094, 0.5044, 0.5079,
        0.5000, 0.5162, 0.5000, 0.5000, 0.5000, 0.5000, 0.5082, 0.5000, 0.5000,
        0.5260, 0.5000, 0.5267, 0.5110, 0.5000, 0.5067, 0.5000, 0.5000, 0.5107,
        0.5000, 0.5253, 0.5000, 0.5000, 0.5127, 0.5000, 0.5000, 0.5050, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5172, 0.5000, 0.5000, 0.5094, 0.5418, 0.5066,
        0.5000, 0.5242, 0.5212, 0.5279, 0.5000, 0.5000, 0.5025, 0.5000, 0.5255,
        0.5063, 0.5177, 0.5000, 0.5133, 0.5224, 0.5000, 0.5000, 0.5000, 0.5220,
        0.5000, 0.5261, 0.5000, 0.5000, 0.5085, 0.5012, 0.5281, 0.5000, 0.5343,
        0.5000, 0.5094, 0.5210, 0.5000, 0.5012, 0.5090, 0.5009, 0.5087, 0.5000,
        0.5000, 0.5130, 0.5043, 0.5020, 0.5179, 0.5049, 0.5091, 0.5106, 0.5144,
        0.5389, 0.5222, 0.5122, 0.5054, 0.5213, 0.5000, 0.5389, 0.5058, 0.5147,
        0.5137, 0.5051, 0.5318, 0.5000, 0.5280, 0.5244, 0.5000, 0.5223, 0.5284,
        0.5024, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5212, 0.5164, 0.5000,
        0.5050, 0.5540, 0.5043, 0.5227, 0.5000, 0.5069, 0.5096, 0.5373, 0.5068,
        0.5000, 0.5000, 0.5145, 0.5000, 0.5447, 0.5168, 0.5131, 0.5000, 0.5086,
        0.5243, 0.5362, 0.5000, 0.5390, 0.5141, 0.5137, 0.5224, 0.5000, 0.5262,
        0.5116, 0.5000, 0.5014, 0.5126, 0.5000, 0.5455, 0.5030, 0.5170, 0.5257,
        0.5149, 0.5000, 0.5216, 0.5350, 0.5000, 0.5027, 0.5118, 0.5474, 0.5303,
        0.5278, 0.5227, 0.5227, 0.5000, 0.5077, 0.5572, 0.5113, 0.5257, 0.5108,
        0.5194, 0.5000, 0.5249, 0.5000, 0.5159, 0.5019, 0.5258, 0.5229, 0.5243,
        0.5000, 0.5269, 0.5022, 0.5000, 0.5221, 0.5157, 0.5289, 0.5323, 0.5002,
        0.5000, 0.5101, 0.5244, 0.5219, 0.5000, 0.5354, 0.5000, 0.5058, 0.5465,
        0.5191, 0.5209, 0.5091, 0.5320, 0.5046, 0.5188, 0.5426, 0.5295, 0.5255,
        0.5202, 0.5202, 0.5318, 0.5000, 0.5124, 0.5319, 0.5391, 0.5095, 0.5075,
        0.5257, 0.5335, 0.5534, 0.5351, 0.5278, 0.5000, 0.5121, 0.5312, 0.5000,
        0.5275, 0.5000, 0.5086, 0.5000, 0.5466, 0.5491, 0.5309, 0.5185, 0.5320,
        0.5471, 0.5198, 0.5310, 0.5149, 0.5018, 0.5339, 0.5298, 0.5060, 0.5210,
        0.5334, 0.5000, 0.5122, 0.5010, 0.5244, 0.5146, 0.5336, 0.5268, 0.5165,
        0.5000, 0.5347, 0.5199, 0.5287, 0.5178, 0.5329, 0.5381, 0.5020, 0.5061,
        0.5225, 0.5202, 0.5527, 0.5260, 0.5610, 0.5161, 0.5433, 0.5385, 0.5347,
        0.5343, 0.5419, 0.5487, 0.5241, 0.5591, 0.5291, 0.5227, 0.5342, 0.5313,
        0.5448, 0.5199, 0.5525, 0.5147, 0.5477, 0.5401, 0.5333, 0.5445, 0.5231,
        0.5566, 0.5396, 0.5323, 0.5288, 0.5376, 0.5289, 0.5303, 0.5447, 0.5389,
        0.5354, 0.5302, 0.5149, 0.5395, 0.5187, 0.5312, 0.5616, 0.5434, 0.5273,
        0.5217, 0.5110, 0.5252, 0.5324, 0.5446, 0.5386, 0.5477, 0.5361, 0.5371,
        0.5363, 0.5390, 0.5556, 0.5106, 0.5507, 0.5412, 0.5344, 0.5258, 0.5168,
        0.5439, 0.5172, 0.5000, 0.5297, 0.5069, 0.5582, 0.5662, 0.5350, 0.5353,
        0.5212, 0.5491, 0.5149, 0.5000, 0.5303, 0.5471, 0.5390, 0.5221, 0.5200,
        0.5385, 0.5411, 0.5319, 0.5316, 0.5252, 0.5072, 0.5483, 0.5435, 0.5341,
        0.5454, 0.5411, 0.5386, 0.5198, 0.5497, 0.5387, 0.5323, 0.5410, 0.5120,
        0.5478, 0.5345, 0.5000, 0.5542, 0.5369, 0.5442, 0.5241, 0.5291, 0.5415,
        0.5410, 0.5401, 0.5302, 0.5569, 0.5176, 0.5551, 0.5723, 0.5262, 0.5545,
        0.5457, 0.5119, 0.5593, 0.5440, 0.5409, 0.5485, 0.5418, 0.5600, 0.5536,
        0.5456, 0.5471, 0.5229, 0.5491, 0.5454, 0.5310, 0.5465, 0.5386, 0.5539,
        0.5572, 0.5448, 0.5490, 0.5604, 0.5472, 0.5510, 0.5426, 0.5317, 0.5556,
        0.5493, 0.5576, 0.5457, 0.5356, 0.5332, 0.5478, 0.5530, 0.5187, 0.5328,
        0.5273, 0.5254, 0.5459, 0.5466, 0.5385, 0.5250, 0.5214, 0.5297, 0.5519,
        0.5543, 0.5377, 0.5437, 0.5452, 0.5399, 0.5212, 0.5314, 0.5256, 0.5525,
        0.5497, 0.5448, 0.5421, 0.5526, 0.5469, 0.5527, 0.5443, 0.5586, 0.5304,
        0.5255, 0.5328, 0.5364, 0.5306, 0.5550, 0.5508, 0.5324, 0.5416, 0.5454,
        0.5436, 0.5461, 0.5479, 0.5515, 0.5426, 0.5565, 0.5559, 0.5649, 0.5356,
        0.5451, 0.5197, 0.5510, 0.5172, 0.5269, 0.5429, 0.5334, 0.5528, 0.5541,
        0.5398, 0.5475, 0.5182, 0.5549, 0.5180, 0.5372, 0.5308, 0.5375, 0.5416,
        0.5352, 0.5678, 0.5383, 0.5598, 0.5569, 0.5569, 0.5529, 0.5353, 0.5538,
        0.5324, 0.5666, 0.5630, 0.5423, 0.5390, 0.5288, 0.5460, 0.5342, 0.5523,
        0.5398, 0.5552, 0.5367, 0.5407, 0.5587, 0.5550, 0.5626, 0.5541, 0.5287,
        0.5630, 0.5414, 0.5364, 0.5296, 0.5380, 0.5574, 0.5568, 0.5675, 0.5480,
        0.5302, 0.5687, 0.5421, 0.5560, 0.5573, 0.5552, 0.5496, 0.5679, 0.5408,
        0.5354, 0.5501, 0.5535, 0.5578, 0.5643, 0.5428, 0.5371, 0.5488, 0.5369,
        0.5564, 0.5562, 0.5556, 0.5504, 0.5536, 0.5578, 0.5422, 0.5406, 0.5521,
        0.5572, 0.5590, 0.5552, 0.5481, 0.5516, 0.5171, 0.5367, 0.5297, 0.5593,
        0.5419, 0.5409, 0.5673, 0.5283, 0.5563, 0.5469, 0.5576, 0.5432, 0.5590,
        0.5677, 0.5519, 0.5608, 0.5482, 0.5499, 0.5369, 0.5544, 0.5536, 0.5402,
        0.5446, 0.5488, 0.5273, 0.5472, 0.5503, 0.5739, 0.5684, 0.5664, 0.5714,
        0.5180, 0.5443, 0.5700, 0.5421, 0.5019, 0.5555, 0.5587, 0.5452, 0.5391,
        0.5757, 0.5485, 0.5633, 0.5519, 0.5558, 0.5377, 0.5534, 0.5515, 0.5652,
        0.5621, 0.5626, 0.5464, 0.5600, 0.5629, 0.5610, 0.5330, 0.5718, 0.5719,
        0.5488, 0.5351, 0.5630, 0.5495, 0.5631, 0.5738, 0.5429, 0.5586, 0.5701,
        0.5682, 0.5561, 0.5581, 0.5548, 0.5496, 0.5421, 0.5559, 0.5422, 0.5433,
        0.5725, 0.5548, 0.5733, 0.5430, 0.5305, 0.5569, 0.5435, 0.5739, 0.5612,
        0.5552, 0.5823, 0.5510, 0.5457, 0.5788, 0.5636, 0.5597, 0.5520, 0.5690,
        0.5582, 0.5370, 0.5475, 0.5329, 0.5452, 0.5467, 0.5646, 0.5579, 0.5716,
        0.5735, 0.5520, 0.5574, 0.5720, 0.5597, 0.5589, 0.5580, 0.5191, 0.5692,
        0.5454, 0.5409, 0.5523, 0.5051, 0.5345, 0.5245, 0.5474, 0.5785, 0.5592,
        0.5680, 0.5457, 0.5675, 0.5641, 0.5584, 0.5590, 0.5693, 0.5540, 0.5469,
        0.5543, 0.5476, 0.5539, 0.5408, 0.5654, 0.5574, 0.5735, 0.5659, 0.5453,
        0.5600, 0.5596, 0.5376, 0.5573, 0.5263, 0.5646, 0.5581, 0.5522, 0.5320,
        0.5660, 0.5663, 0.5474, 0.5491, 0.5724, 0.5701, 0.5548, 0.5519, 0.5490,
        0.5441, 0.5537, 0.5776, 0.5709, 0.5416, 0.5515, 0.5539, 0.5628, 0.5658],
       device='cuda:0', grad_fn=<SelectBackward0>)
tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,
        0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,
        0, 1, 0], device='cuda:0')
[37mEpoch 0/3 [39m [35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m26/26[39m [37m0:00:28 â€¢ 0:00:00[39m [37m0.93it/s[39m [37mv_num: mz9w train/loss:     
                                                                        [37m0.692                       
[?25h
Error executing job with overrides: []
Traceback (most recent call last):
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 494, in main
    trainer.fit(model, data_module)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 989, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1035, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 203, in run
    self.on_advance_end()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 373, in on_advance_end
    call._call_lightning_module_hook(trainer, "on_train_epoch_end")
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 283, in on_train_epoch_end
    f"{mode}/accuracy_label_{i}": metrics_per_label[
TypeError: 'Tensor' object is not callable
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.