digraph {
	graph [size="120.44999999999999,120.44999999999999"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	136755320407200 [label="
 (1, 16)" fillcolor=darkolivegreen1]
	136755320812448 [label=SigmoidBackward0]
	136755320810144 -> 136755320812448
	136755320810144 [label=CatBackward0]
	136755320810384 -> 136755320810144
	136755320810384 [label=AddmmBackward0]
	136755320811920 -> 136755320810384
	136755321251024 [label="hidden_layer2.0.bias
 (1)" fillcolor=lightblue]
	136755321251024 -> 136755320811920
	136755320811920 [label=AccumulateGrad]
	136755320811872 -> 136755320810384
	136755320811872 [label=ReluBackward0]
	136755320811728 -> 136755320811872
	136755320811728 [label=AddmmBackward0]
	136755320811344 -> 136755320811728
	136755321256144 [label="hidden_layer1.0.bias
 (970)" fillcolor=lightblue]
	136755321256144 -> 136755320811344
	136755320811344 [label=AccumulateGrad]
	136755320811440 -> 136755320811728
	136755320811440 [label=ReluBackward0]
	136755320811104 -> 136755320811440
	136755320811104 [label=AddmmBackward0]
	136755320809808 -> 136755320811104
	136755367621584 [label="classifiers.0.bias
 (1018)" fillcolor=lightblue]
	136755367621584 -> 136755320809808
	136755320809808 [label=AccumulateGrad]
	136755320809760 -> 136755320811104
	136755320809760 [label=TanhBackward0]
	136755320809712 -> 136755320809760
	136755320809712 [label=AddmmBackward0]
	136755320809520 -> 136755320809712
	136755320809520 [label=SelectBackward0]
	136755320809424 -> 136755320809520
	136755320809424 [label=SliceBackward0]
	136755320809328 -> 136755320809424
	136755320809328 [label=NativeLayerNormBackward0]
	136755320809232 -> 136755320809328
	136755320809232 [label=AddBackward0]
	136755320809040 -> 136755320809232
	136755320809040 [label=ViewBackward0]
	136755320808800 -> 136755320809040
	136755320808800 [label=AddmmBackward0]
	136755320808704 -> 136755320808800
	136755321420304 [label="bert.encoder.layer.11.output.dense.bias
 (768)" fillcolor=lightblue]
	136755321420304 -> 136755320808704
	136755320808704 [label=AccumulateGrad]
	136755320808848 -> 136755320808800
	136755320808848 [label=ViewBackward0]
	136755320808608 -> 136755320808848
	136755320808608 [label=GeluBackward0]
	136755320808416 -> 136755320808608
	136755320808416 [label=ViewBackward0]
	136755320808320 -> 136755320808416
	136755320808320 [label=AddmmBackward0]
	136755320808224 -> 136755320808320
	136755321421104 [label="bert.encoder.layer.11.intermediate.dense.bias
 (3072)" fillcolor=lightblue]
	136755321421104 -> 136755320808224
	136755320808224 [label=AccumulateGrad]
	136755320808368 -> 136755320808320
	136755320808368 [label=ViewBackward0]
	136755320808992 -> 136755320808368
	136755320808992 [label=NativeLayerNormBackward0]
	136755320808080 -> 136755320808992
	136755320808080 [label=AddBackward0]
	136755320807888 -> 136755320808080
	136755320807888 [label=ViewBackward0]
	136755320807792 -> 136755320807888
	136755320807792 [label=AddmmBackward0]
	136755320807696 -> 136755320807792
	136755321420704 [label="bert.encoder.layer.11.attention.output.dense.bias
 (768)" fillcolor=lightblue]
	136755321420704 -> 136755320807696
	136755320807696 [label=AccumulateGrad]
	136755320807648 -> 136755320807792
	136755320807648 [label=ViewBackward0]
	136755320807600 -> 136755320807648
	136755320807600 [label=ViewBackward0]
	136755320807408 -> 136755320807600
	136755320807408 [label=TransposeBackward0]
	136755320807312 -> 136755320807408
	136755320807312 [label=ScaledDotProductFlashAttentionForCpuBackward0]
	136755320807216 -> 136755320807312
	136755320807216 [label=PermuteBackward0]
	136755320807024 -> 136755320807216
	136755320807024 [label=ViewBackward0]
	136755320806928 -> 136755320807024
	136755320806928 [label=ViewBackward0]
	136755320806832 -> 136755320806928
	136755320806832 [label=AddmmBackward0]
	136755320806736 -> 136755320806832
	136755321420224 [label="bert.encoder.layer.11.attention.self.query.bias
 (768)" fillcolor=lightblue]
	136755321420224 -> 136755320806736
	136755320806736 [label=AccumulateGrad]
	136755320806688 -> 136755320806832
	136755320806688 [label=TBackward0]
	136755320806592 -> 136755320806688
	136755321420144 [label="bert.encoder.layer.11.attention.self.query.weight
 (768, 768)" fillcolor=lightblue]
	136755321420144 -> 136755320806592
	136755320806592 [label=AccumulateGrad]
	136755320807168 -> 136755320807312
	136755320807168 [label=PermuteBackward0]
	136755320806784 -> 136755320807168
	136755320806784 [label=ViewBackward0]
	136755320806640 -> 136755320806784
	136755320806640 [label=ViewBackward0]
	136755320806544 -> 136755320806640
	136755320806544 [label=AddmmBackward0]
	136755320806448 -> 136755320806544
	136755321420384 [label="bert.encoder.layer.11.attention.self.key.bias
 (768)" fillcolor=lightblue]
	136755321420384 -> 136755320806448
	136755320806448 [label=AccumulateGrad]
	136755320806496 -> 136755320806544
	136755320806496 [label=TBackward0]
	136755320806304 -> 136755320806496
	136755321418704 [label="bert.encoder.layer.11.attention.self.key.weight
 (768, 768)" fillcolor=lightblue]
	136755321418704 -> 136755320806304
	136755320806304 [label=AccumulateGrad]
	136755320807504 -> 136755320807312
	136755320807504 [label=PermuteBackward0]
	136755320806400 -> 136755320807504
	136755320806400 [label=ViewBackward0]
	136755320806352 -> 136755320806400
	136755320806352 [label=ViewBackward0]
	136755320806256 -> 136755320806352
	136755320806256 [label=AddmmBackward0]
	136755320806160 -> 136755320806256
	136755321420544 [label="bert.encoder.layer.11.attention.self.value.bias
 (768)" fillcolor=lightblue]
	136755321420544 -> 136755320806160
	136755320806160 [label=AccumulateGrad]
	136755320806208 -> 136755320806256
	136755320806208 [label=TBackward0]
	136755320806016 -> 136755320806208
	136755321420464 [label="bert.encoder.layer.11.attention.self.value.weight
 (768, 768)" fillcolor=lightblue]
	136755321420464 -> 136755320806016
	136755320806016 [label=AccumulateGrad]
	136755320807984 -> 136755320807792
	136755320807984 [label=TBackward0]
	136755320807264 -> 136755320807984
	136755321420624 [label="bert.encoder.layer.11.attention.output.dense.weight
 (768, 768)" fillcolor=lightblue]
	136755321420624 -> 136755320807264
	136755320807264 [label=AccumulateGrad]
	136755320808032 -> 136755320808992
	136755321420864 [label="bert.encoder.layer.11.attention.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	136755321420864 -> 136755320808032
	136755320808032 [label=AccumulateGrad]
	136755320808176 -> 136755320808992
	136755321420784 [label="bert.encoder.layer.11.attention.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	136755321420784 -> 136755320808176
	136755320808176 [label=AccumulateGrad]
	136755320808512 -> 136755320808320
	136755320808512 [label=TBackward0]
	136755320807840 -> 136755320808512
	136755321421024 [label="bert.encoder.layer.11.intermediate.dense.weight
 (3072, 768)" fillcolor=lightblue]
	136755321421024 -> 136755320807840
	136755320807840 [label=AccumulateGrad]
	136755320808896 -> 136755320808800
	136755320808896 [label=TBackward0]
	136755320808464 -> 136755320808896
	136755321420944 [label="bert.encoder.layer.11.output.dense.weight
 (768, 3072)" fillcolor=lightblue]
	136755321420944 -> 136755320808464
	136755320808464 [label=AccumulateGrad]
	136755320808992 -> 136755320809232
	136755320809184 -> 136755320809328
	136755321421344 [label="bert.encoder.layer.11.output.LayerNorm.weight
 (768)" fillcolor=lightblue]
	136755321421344 -> 136755320809184
	136755320809184 [label=AccumulateGrad]
	136755320809616 -> 136755320809328
	136755321421184 [label="bert.encoder.layer.11.output.LayerNorm.bias
 (768)" fillcolor=lightblue]
	136755321421184 -> 136755320809616
	136755320809616 [label=AccumulateGrad]
	136755320809904 -> 136755320811104
	136755320809904 [label=TBackward0]
	136755320809376 -> 136755320809904
	136755367621664 [label="classifiers.0.weight
 (1018, 768)" fillcolor=lightblue]
	136755367621664 -> 136755320809376
	136755320809376 [label=AccumulateGrad]
	136755320811392 -> 136755320811728
	136755320811392 [label=TBackward0]
	136755320809472 -> 136755320811392
	136755321256304 [label="hidden_layer1.0.weight
 (970, 1018)" fillcolor=lightblue]
	136755321256304 -> 136755320809472
	136755320809472 [label=AccumulateGrad]
	136755320812016 -> 136755320810384
	136755320812016 [label=TBackward0]
	136755320809664 -> 136755320812016
	136755321251184 [label="hidden_layer2.0.weight
 (1, 970)" fillcolor=lightblue]
	136755321251184 -> 136755320809664
	136755320809664 [label=AccumulateGrad]
	136755320810048 -> 136755320810144
	136755320810048 [label=AddmmBackward0]
	136755320809088 -> 136755320810048
	136755321250704 [label="hidden_layer2.1.bias
 (1)" fillcolor=lightblue]
	136755321250704 -> 136755320809088
	136755320809088 [label=AccumulateGrad]
	136755320811200 -> 136755320810048
	136755320811200 [label=ReluBackward0]
	136755320811584 -> 136755320811200
	136755320811584 [label=AddmmBackward0]
	136755320808944 -> 136755320811584
	136755321255824 [label="hidden_layer1.1.bias
 (970)" fillcolor=lightblue]
	136755321255824 -> 136755320808944
	136755320808944 [label=AccumulateGrad]
	136755320809136 -> 136755320811584
	136755320809136 [label=ReluBackward0]
	136755320808560 -> 136755320809136
	136755320808560 [label=AddmmBackward0]
	136755320807552 -> 136755320808560
	136755367610064 [label="classifiers.1.bias
 (1018)" fillcolor=lightblue]
	136755367610064 -> 136755320807552
	136755320807552 [label=AccumulateGrad]
	136755320809760 -> 136755320808560
	136755320808656 -> 136755320808560
	136755320808656 [label=TBackward0]
	136755320807744 -> 136755320808656
	136755367621344 [label="classifiers.1.weight
 (1018, 768)" fillcolor=lightblue]
	136755367621344 -> 136755320807744
	136755320807744 [label=AccumulateGrad]
	136755320809568 -> 136755320811584
	136755320809568 [label=TBackward0]
	136755320808128 -> 136755320809568
	136755321255984 [label="hidden_layer1.1.weight
 (970, 1018)" fillcolor=lightblue]
	136755321255984 -> 136755320808128
	136755320808128 [label=AccumulateGrad]
	136755320811776 -> 136755320810048
	136755320811776 [label=TBackward0]
	136755320808272 -> 136755320811776
	136755321250864 [label="hidden_layer2.1.weight
 (1, 970)" fillcolor=lightblue]
	136755321250864 -> 136755320808272
	136755320808272 [label=AccumulateGrad]
	136755320810288 -> 136755320810144
	136755320810288 [label=AddmmBackward0]
	136755320807072 -> 136755320810288
	136755321250384 [label="hidden_layer2.2.bias
 (1)" fillcolor=lightblue]
	136755321250384 -> 136755320807072
	136755320807072 [label=AccumulateGrad]
	136755320808752 -> 136755320810288
	136755320808752 [label=ReluBackward0]
	136755320811008 -> 136755320808752
	136755320811008 [label=AddmmBackward0]
	136755320806976 -> 136755320811008
	136755321255504 [label="hidden_layer1.2.bias
 (970)" fillcolor=lightblue]
	136755321255504 -> 136755320806976
	136755320806976 [label=AccumulateGrad]
	136755320807456 -> 136755320811008
	136755320807456 [label=ReluBackward0]
	136755320806880 -> 136755320807456
	136755320806880 [label=AddmmBackward0]
	136755320805920 -> 136755320806880
	136755367608944 [label="classifiers.2.bias
 (1018)" fillcolor=lightblue]
	136755367608944 -> 136755320805920
	136755320805920 [label=AccumulateGrad]
	136755320809760 -> 136755320806880
	136755320805968 -> 136755320806880
	136755320805968 [label=TBackward0]
	136755320805872 -> 136755320805968
	136755367608864 [label="classifiers.2.weight
 (1018, 768)" fillcolor=lightblue]
	136755367608864 -> 136755320805872
	136755320805872 [label=AccumulateGrad]
	136755320807360 -> 136755320811008
	136755320807360 [label=TBackward0]
	136755320805728 -> 136755320807360
	136755321255664 [label="hidden_layer1.2.weight
 (970, 1018)" fillcolor=lightblue]
	136755321255664 -> 136755320805728
	136755320805728 [label=AccumulateGrad]
	136755320809280 -> 136755320810288
	136755320809280 [label=TBackward0]
	136755320805824 -> 136755320809280
	136755321250544 [label="hidden_layer2.2.weight
 (1, 970)" fillcolor=lightblue]
	136755321250544 -> 136755320805824
	136755320805824 [label=AccumulateGrad]
	136755320810000 -> 136755320810144
	136755320810000 [label=AddmmBackward0]
	136755320805680 -> 136755320810000
	136755321250064 [label="hidden_layer2.3.bias
 (1)" fillcolor=lightblue]
	136755321250064 -> 136755320805680
	136755320805680 [label=AccumulateGrad]
	136755320806112 -> 136755320810000
	136755320806112 [label=ReluBackward0]
	136755320807936 -> 136755320806112
	136755320807936 [label=AddmmBackward0]
	136755320805584 -> 136755320807936
	136755321255184 [label="hidden_layer1.3.bias
 (970)" fillcolor=lightblue]
	136755321255184 -> 136755320805584
	136755320805584 [label=AccumulateGrad]
	136755320805536 -> 136755320807936
	136755320805536 [label=ReluBackward0]
	136755320805488 -> 136755320805536
	136755320805488 [label=AddmmBackward0]
	136755320805296 -> 136755320805488
	136755367610144 [label="classifiers.3.bias
 (1018)" fillcolor=lightblue]
	136755367610144 -> 136755320805296
	136755320805296 [label=AccumulateGrad]
	136755320809760 -> 136755320805488
	136755320805248 -> 136755320805488
	136755320805248 [label=TBackward0]
	136755320805152 -> 136755320805248
	136755367612784 [label="classifiers.3.weight
 (1018, 768)" fillcolor=lightblue]
	136755367612784 -> 136755320805152
	136755320805152 [label=AccumulateGrad]
	136755320805776 -> 136755320807936
	136755320805776 [label=TBackward0]
	136755320805200 -> 136755320805776
	136755321255344 [label="hidden_layer1.3.weight
 (970, 1018)" fillcolor=lightblue]
	136755321255344 -> 136755320805200
	136755320805200 [label=AccumulateGrad]
	136755320807120 -> 136755320810000
	136755320807120 [label=TBackward0]
	136755320805392 -> 136755320807120
	136755321250224 [label="hidden_layer2.3.weight
 (1, 970)" fillcolor=lightblue]
	136755321250224 -> 136755320805392
	136755320805392 [label=AccumulateGrad]
	136755320810192 -> 136755320810144
	136755320810192 [label=AddmmBackward0]
	136755320804960 -> 136755320810192
	136755367373824 [label="hidden_layer2.4.bias
 (1)" fillcolor=lightblue]
	136755367373824 -> 136755320804960
	136755320804960 [label=AccumulateGrad]
	136755320805440 -> 136755320810192
	136755320805440 [label=ReluBackward0]
	136755320806064 -> 136755320805440
	136755320806064 [label=AddmmBackward0]
	136755320804864 -> 136755320806064
	136755321254864 [label="hidden_layer1.4.bias
 (970)" fillcolor=lightblue]
	136755321254864 -> 136755320804864
	136755320804864 [label=AccumulateGrad]
	136755320805008 -> 136755320806064
	136755320805008 [label=ReluBackward0]
	136755320804768 -> 136755320805008
	136755320804768 [label=AddmmBackward0]
	136755320804576 -> 136755320804768
	136755367616544 [label="classifiers.4.bias
 (1018)" fillcolor=lightblue]
	136755367616544 -> 136755320804576
	136755320804576 [label=AccumulateGrad]
	136755320809760 -> 136755320804768
	136755320804720 -> 136755320804768
	136755320804720 [label=TBackward0]
	136755320804624 -> 136755320804720
	136755367621264 [label="classifiers.4.weight
 (1018, 768)" fillcolor=lightblue]
	136755367621264 -> 136755320804624
	136755320804624 [label=AccumulateGrad]
	136755320805056 -> 136755320806064
	136755320805056 [label=TBackward0]
	136755320804480 -> 136755320805056
	136755321255024 [label="hidden_layer1.4.weight
 (970, 1018)" fillcolor=lightblue]
	136755321255024 -> 136755320804480
	136755320804480 [label=AccumulateGrad]
	136755320805632 -> 136755320810192
	136755320805632 [label=TBackward0]
	136755320804672 -> 136755320805632
	136755367373504 [label="hidden_layer2.4.weight
 (1, 970)" fillcolor=lightblue]
	136755367373504 -> 136755320804672
	136755320804672 [label=AccumulateGrad]
	136755320810432 -> 136755320810144
	136755320810432 [label=AddmmBackward0]
	136755320804432 -> 136755320810432
	136755367094256 [label="hidden_layer2.5.bias
 (1)" fillcolor=lightblue]
	136755367094256 -> 136755320804432
	136755320804432 [label=AccumulateGrad]
	136755320804912 -> 136755320810432
	136755320804912 [label=ReluBackward0]
	136755320805344 -> 136755320804912
	136755320805344 [label=AddmmBackward0]
	136755320804336 -> 136755320805344
	136755321254544 [label="hidden_layer1.5.bias
 (970)" fillcolor=lightblue]
	136755321254544 -> 136755320804336
	136755320804336 [label=AccumulateGrad]
	136755320804288 -> 136755320805344
	136755320804288 [label=ReluBackward0]
	136755320804240 -> 136755320804288
	136755320804240 [label=AddmmBackward0]
	136755320804048 -> 136755320804240
	136755367615664 [label="classifiers.5.bias
 (1018)" fillcolor=lightblue]
	136755367615664 -> 136755320804048
	136755320804048 [label=AccumulateGrad]
	136755320809760 -> 136755320804240
	136755320804000 -> 136755320804240
	136755320804000 [label=TBackward0]
	136755320803904 -> 136755320804000
	136755367617664 [label="classifiers.5.weight
 (1018, 768)" fillcolor=lightblue]
	136755367617664 -> 136755320803904
	136755320803904 [label=AccumulateGrad]
	136755320804528 -> 136755320805344
	136755320804528 [label=TBackward0]
	136755320803952 -> 136755320804528
	136755321254704 [label="hidden_layer1.5.weight
 (970, 1018)" fillcolor=lightblue]
	136755321254704 -> 136755320803952
	136755320803952 [label=AccumulateGrad]
	136755320805104 -> 136755320810432
	136755320805104 [label=TBackward0]
	136755320804144 -> 136755320805104
	136755367094656 [label="hidden_layer2.5.weight
 (1, 970)" fillcolor=lightblue]
	136755367094656 -> 136755320804144
	136755320804144 [label=AccumulateGrad]
	136755320810480 -> 136755320810144
	136755320810480 [label=AddmmBackward0]
	136755320803712 -> 136755320810480
	136755367094576 [label="hidden_layer2.6.bias
 (1)" fillcolor=lightblue]
	136755367094576 -> 136755320803712
	136755320803712 [label=AccumulateGrad]
	136755320804192 -> 136755320810480
	136755320804192 [label=ReluBackward0]
	136755320804816 -> 136755320804192
	136755320804816 [label=AddmmBackward0]
	136755320803616 -> 136755320804816
	136755321254224 [label="hidden_layer1.6.bias
 (970)" fillcolor=lightblue]
	136755321254224 -> 136755320803616
	136755320803616 [label=AccumulateGrad]
	136755320803760 -> 136755320804816
	136755320803760 [label=ReluBackward0]
	136755320803520 -> 136755320803760
	136755320803520 [label=AddmmBackward0]
	136755320803328 -> 136755320803520
	136755367615984 [label="classifiers.6.bias
 (1018)" fillcolor=lightblue]
	136755367615984 -> 136755320803328
	136755320803328 [label=AccumulateGrad]
	136755320809760 -> 136755320803520
	136755320803472 -> 136755320803520
	136755320803472 [label=TBackward0]
	136755320803376 -> 136755320803472
	136755367615584 [label="classifiers.6.weight
 (1018, 768)" fillcolor=lightblue]
	136755367615584 -> 136755320803376
	136755320803376 [label=AccumulateGrad]
	136755320803808 -> 136755320804816
	136755320803808 [label=TBackward0]
	136755320803232 -> 136755320803808
	136755321254384 [label="hidden_layer1.6.weight
 (970, 1018)" fillcolor=lightblue]
	136755321254384 -> 136755320803232
	136755320803232 [label=AccumulateGrad]
	136755320804384 -> 136755320810480
	136755320804384 [label=TBackward0]
	136755320803424 -> 136755320804384
	136755367094816 [label="hidden_layer2.6.weight
 (1, 970)" fillcolor=lightblue]
	136755367094816 -> 136755320803424
	136755320803424 [label=AccumulateGrad]
	136755320810336 -> 136755320810144
	136755320810336 [label=AddmmBackward0]
	136755320803184 -> 136755320810336
	136755367096336 [label="hidden_layer2.7.bias
 (1)" fillcolor=lightblue]
	136755367096336 -> 136755320803184
	136755320803184 [label=AccumulateGrad]
	136755320803664 -> 136755320810336
	136755320803664 [label=ReluBackward0]
	136755320804096 -> 136755320803664
	136755320804096 [label=AddmmBackward0]
	136755320803088 -> 136755320804096
	136755321253904 [label="hidden_layer1.7.bias
 (970)" fillcolor=lightblue]
	136755321253904 -> 136755320803088
	136755320803088 [label=AccumulateGrad]
	136755320803040 -> 136755320804096
	136755320803040 [label=ReluBackward0]
	136755320802992 -> 136755320803040
	136755320802992 [label=AddmmBackward0]
	136755320802752 -> 136755320802992
	136755367618144 [label="classifiers.7.bias
 (1018)" fillcolor=lightblue]
	136755367618144 -> 136755320802752
	136755320802752 [label=AccumulateGrad]
	136755320809760 -> 136755320802992
	136755320802896 -> 136755320802992
	136755320802896 [label=TBackward0]
	136755320802704 -> 136755320802896
	136755367612864 [label="classifiers.7.weight
 (1018, 768)" fillcolor=lightblue]
	136755367612864 -> 136755320802704
	136755320802704 [label=AccumulateGrad]
	136755320803280 -> 136755320804096
	136755320803280 [label=TBackward0]
	136755320802560 -> 136755320803280
	136755321254064 [label="hidden_layer1.7.weight
 (970, 1018)" fillcolor=lightblue]
	136755321254064 -> 136755320802560
	136755320802560 [label=AccumulateGrad]
	136755320803856 -> 136755320810336
	136755320803856 [label=TBackward0]
	136755320802656 -> 136755320803856
	136755367094496 [label="hidden_layer2.7.weight
 (1, 970)" fillcolor=lightblue]
	136755367094496 -> 136755320802656
	136755320802656 [label=AccumulateGrad]
	136755320810576 -> 136755320810144
	136755320810576 [label=AddmmBackward0]
	136755320802512 -> 136755320810576
	136755367095856 [label="hidden_layer2.8.bias
 (1)" fillcolor=lightblue]
	136755367095856 -> 136755320802512
	136755320802512 [label=AccumulateGrad]
	136755320802944 -> 136755320810576
	136755320802944 [label=ReluBackward0]
	136755320803568 -> 136755320802944
	136755320803568 [label=AddmmBackward0]
	136755320802416 -> 136755320803568
	136755321253584 [label="hidden_layer1.8.bias
 (970)" fillcolor=lightblue]
	136755321253584 -> 136755320802416
	136755320802416 [label=AccumulateGrad]
	136755320802368 -> 136755320803568
	136755320802368 [label=ReluBackward0]
	136755320802320 -> 136755320802368
	136755320802320 [label=AddmmBackward0]
	136755320802128 -> 136755320802320
	136755367612624 [label="classifiers.8.bias
 (1018)" fillcolor=lightblue]
	136755367612624 -> 136755320802128
	136755320802128 [label=AccumulateGrad]
	136755320809760 -> 136755320802320
	136755320802080 -> 136755320802320
	136755320802080 [label=TBackward0]
	136755320801984 -> 136755320802080
	136755367620944 [label="classifiers.8.weight
 (1018, 768)" fillcolor=lightblue]
	136755367620944 -> 136755320801984
	136755320801984 [label=AccumulateGrad]
	136755320802608 -> 136755320803568
	136755320802608 [label=TBackward0]
	136755320802032 -> 136755320802608
	136755321253744 [label="hidden_layer1.8.weight
 (970, 1018)" fillcolor=lightblue]
	136755321253744 -> 136755320802032
	136755320802032 [label=AccumulateGrad]
	136755320803136 -> 136755320810576
	136755320803136 [label=TBackward0]
	136755320802224 -> 136755320803136
	136755367095936 [label="hidden_layer2.8.weight
 (1, 970)" fillcolor=lightblue]
	136755367095936 -> 136755320802224
	136755320802224 [label=AccumulateGrad]
	136755320810672 -> 136755320810144
	136755320810672 [label=AddmmBackward0]
	136755320801792 -> 136755320810672
	136755367096176 [label="hidden_layer2.9.bias
 (1)" fillcolor=lightblue]
	136755367096176 -> 136755320801792
	136755320801792 [label=AccumulateGrad]
	136755320802272 -> 136755320810672
	136755320802272 [label=ReluBackward0]
	136755320802848 -> 136755320802272
	136755320802848 [label=AddmmBackward0]
	136755320801696 -> 136755320802848
	136755321253264 [label="hidden_layer1.9.bias
 (970)" fillcolor=lightblue]
	136755321253264 -> 136755320801696
	136755320801696 [label=AccumulateGrad]
	136755320801840 -> 136755320802848
	136755320801840 [label=ReluBackward0]
	136755320801600 -> 136755320801840
	136755320801600 [label=AddmmBackward0]
	136755320801408 -> 136755320801600
	136755367621104 [label="classifiers.9.bias
 (1018)" fillcolor=lightblue]
	136755367621104 -> 136755320801408
	136755320801408 [label=AccumulateGrad]
	136755320809760 -> 136755320801600
	136755320801552 -> 136755320801600
	136755320801552 [label=TBackward0]
	136755320801456 -> 136755320801552
	136755367620864 [label="classifiers.9.weight
 (1018, 768)" fillcolor=lightblue]
	136755367620864 -> 136755320801456
	136755320801456 [label=AccumulateGrad]
	136755320801888 -> 136755320802848
	136755320801888 [label=TBackward0]
	136755320801312 -> 136755320801888
	136755321253424 [label="hidden_layer1.9.weight
 (970, 1018)" fillcolor=lightblue]
	136755321253424 -> 136755320801312
	136755320801312 [label=AccumulateGrad]
	136755320802464 -> 136755320810672
	136755320802464 [label=TBackward0]
	136755320801504 -> 136755320802464
	136755367095776 [label="hidden_layer2.9.weight
 (1, 970)" fillcolor=lightblue]
	136755367095776 -> 136755320801504
	136755320801504 [label=AccumulateGrad]
	136755320810528 -> 136755320810144
	136755320810528 [label=AddmmBackward0]
	136755320801264 -> 136755320810528
	136755367094416 [label="hidden_layer2.10.bias
 (1)" fillcolor=lightblue]
	136755367094416 -> 136755320801264
	136755320801264 [label=AccumulateGrad]
	136755320801744 -> 136755320810528
	136755320801744 [label=ReluBackward0]
	136755320802176 -> 136755320801744
	136755320802176 [label=AddmmBackward0]
	136755320801168 -> 136755320802176
	136755321252944 [label="hidden_layer1.10.bias
 (970)" fillcolor=lightblue]
	136755321252944 -> 136755320801168
	136755320801168 [label=AccumulateGrad]
	136755320801120 -> 136755320802176
	136755320801120 [label=ReluBackward0]
	136755320801072 -> 136755320801120
	136755320801072 [label=AddmmBackward0]
	136755320800880 -> 136755320801072
	136755367609024 [label="classifiers.10.bias
 (1018)" fillcolor=lightblue]
	136755367609024 -> 136755320800880
	136755320800880 [label=AccumulateGrad]
	136755320809760 -> 136755320801072
	136755320800832 -> 136755320801072
	136755320800832 [label=TBackward0]
	136755320800736 -> 136755320800832
	136755367609824 [label="classifiers.10.weight
 (1018, 768)" fillcolor=lightblue]
	136755367609824 -> 136755320800736
	136755320800736 [label=AccumulateGrad]
	136755320801360 -> 136755320802176
	136755320801360 [label=TBackward0]
	136755320800784 -> 136755320801360
	136755321253104 [label="hidden_layer1.10.weight
 (970, 1018)" fillcolor=lightblue]
	136755321253104 -> 136755320800784
	136755320800784 [label=AccumulateGrad]
	136755320801936 -> 136755320810528
	136755320801936 [label=TBackward0]
	136755320800976 -> 136755320801936
	136755367095696 [label="hidden_layer2.10.weight
 (1, 970)" fillcolor=lightblue]
	136755367095696 -> 136755320800976
	136755320800976 [label=AccumulateGrad]
	136755320810624 -> 136755320810144
	136755320810624 [label=AddmmBackward0]
	136755320800544 -> 136755320810624
	136755367096016 [label="hidden_layer2.11.bias
 (1)" fillcolor=lightblue]
	136755367096016 -> 136755320800544
	136755320800544 [label=AccumulateGrad]
	136755320801024 -> 136755320810624
	136755320801024 [label=ReluBackward0]
	136755320801648 -> 136755320801024
	136755320801648 [label=AddmmBackward0]
	136755320800448 -> 136755320801648
	136755321252624 [label="hidden_layer1.11.bias
 (970)" fillcolor=lightblue]
	136755321252624 -> 136755320800448
	136755320800448 [label=AccumulateGrad]
	136755320800592 -> 136755320801648
	136755320800592 [label=ReluBackward0]
	136755320800352 -> 136755320800592
	136755320800352 [label=AddmmBackward0]
	136755320800160 -> 136755320800352
	136755367609184 [label="classifiers.11.bias
 (1018)" fillcolor=lightblue]
	136755367609184 -> 136755320800160
	136755320800160 [label=AccumulateGrad]
	136755320809760 -> 136755320800352
	136755320800304 -> 136755320800352
	136755320800304 [label=TBackward0]
	136755320800208 -> 136755320800304
	136755367609424 [label="classifiers.11.weight
 (1018, 768)" fillcolor=lightblue]
	136755367609424 -> 136755320800208
	136755320800208 [label=AccumulateGrad]
	136755320800640 -> 136755320801648
	136755320800640 [label=TBackward0]
	136755320800064 -> 136755320800640
	136755321252784 [label="hidden_layer1.11.weight
 (970, 1018)" fillcolor=lightblue]
	136755321252784 -> 136755320800064
	136755320800064 [label=AccumulateGrad]
	136755320801216 -> 136755320810624
	136755320801216 [label=TBackward0]
	136755320800256 -> 136755320801216
	136755367096256 [label="hidden_layer2.11.weight
 (1, 970)" fillcolor=lightblue]
	136755367096256 -> 136755320800256
	136755320800256 [label=AccumulateGrad]
	136755320813264 -> 136755320810144
	136755320813264 [label=AddmmBackward0]
	136755320800016 -> 136755320813264
	136755367094096 [label="hidden_layer2.12.bias
 (1)" fillcolor=lightblue]
	136755367094096 -> 136755320800016
	136755320800016 [label=AccumulateGrad]
	136755320800496 -> 136755320813264
	136755320800496 [label=ReluBackward0]
	136755320800928 -> 136755320800496
	136755320800928 [label=AddmmBackward0]
	136755320799920 -> 136755320800928
	136755321252304 [label="hidden_layer1.12.bias
 (970)" fillcolor=lightblue]
	136755321252304 -> 136755320799920
	136755320799920 [label=AccumulateGrad]
	136755320799872 -> 136755320800928
	136755320799872 [label=ReluBackward0]
	136755320799824 -> 136755320799872
	136755320799824 [label=AddmmBackward0]
	136755320799632 -> 136755320799824
	136755321257424 [label="classifiers.12.bias
 (1018)" fillcolor=lightblue]
	136755321257424 -> 136755320799632
	136755320799632 [label=AccumulateGrad]
	136755320809760 -> 136755320799824
	136755320799584 -> 136755320799824
	136755320799584 [label=TBackward0]
	136755320799488 -> 136755320799584
	136755321257584 [label="classifiers.12.weight
 (1018, 768)" fillcolor=lightblue]
	136755321257584 -> 136755320799488
	136755320799488 [label=AccumulateGrad]
	136755320800112 -> 136755320800928
	136755320800112 [label=TBackward0]
	136755320799536 -> 136755320800112
	136755321252464 [label="hidden_layer1.12.weight
 (970, 1018)" fillcolor=lightblue]
	136755321252464 -> 136755320799536
	136755320799536 [label=AccumulateGrad]
	136755320800688 -> 136755320813264
	136755320800688 [label=TBackward0]
	136755320799728 -> 136755320800688
	136755367094176 [label="hidden_layer2.12.weight
 (1, 970)" fillcolor=lightblue]
	136755367094176 -> 136755320799728
	136755320799728 [label=AccumulateGrad]
	136755320812304 -> 136755320810144
	136755320812304 [label=AddmmBackward0]
	136755320799296 -> 136755320812304
	136755367094336 [label="hidden_layer2.13.bias
 (1)" fillcolor=lightblue]
	136755367094336 -> 136755320799296
	136755320799296 [label=AccumulateGrad]
	136755320799776 -> 136755320812304
	136755320799776 [label=ReluBackward0]
	136755320800400 -> 136755320799776
	136755320800400 [label=AddmmBackward0]
	136755320799344 -> 136755320800400
	136755321251984 [label="hidden_layer1.13.bias
 (970)" fillcolor=lightblue]
	136755321251984 -> 136755320799344
	136755320799344 [label=AccumulateGrad]
	136755320799392 -> 136755320800400
	136755320799392 [label=ReluBackward0]
	136755320451856 -> 136755320799392
	136755320451856 [label=AddmmBackward0]
	136755320452288 -> 136755320451856
	136755321257104 [label="classifiers.13.bias
 (1018)" fillcolor=lightblue]
	136755321257104 -> 136755320452288
	136755320452288 [label=AccumulateGrad]
	136755320809760 -> 136755320451856
	136755320452096 -> 136755320451856
	136755320452096 [label=TBackward0]
	136755320455120 -> 136755320452096
	136755321257264 [label="classifiers.13.weight
 (1018, 768)" fillcolor=lightblue]
	136755321257264 -> 136755320455120
	136755320455120 [label=AccumulateGrad]
	136755320443360 -> 136755320800400
	136755320443360 [label=TBackward0]
	136755320452768 -> 136755320443360
	136755321252144 [label="hidden_layer1.13.weight
 (970, 1018)" fillcolor=lightblue]
	136755321252144 -> 136755320452768
	136755320452768 [label=AccumulateGrad]
	136755320799968 -> 136755320812304
	136755320799968 [label=TBackward0]
	136755320799680 -> 136755320799968
	136755367094736 [label="hidden_layer2.13.weight
 (1, 970)" fillcolor=lightblue]
	136755367094736 -> 136755320799680
	136755320799680 [label=AccumulateGrad]
	136755320812160 -> 136755320810144
	136755320812160 [label=AddmmBackward0]
	136755320799440 -> 136755320812160
	136755367098256 [label="hidden_layer2.14.bias
 (1)" fillcolor=lightblue]
	136755367098256 -> 136755320799440
	136755320799440 [label=AccumulateGrad]
	136755320452912 -> 136755320812160
	136755320452912 [label=ReluBackward0]
	136755320439136 -> 136755320452912
	136755320439136 [label=AddmmBackward0]
	136755320452816 -> 136755320439136
	136755321251664 [label="hidden_layer1.14.bias
 (970)" fillcolor=lightblue]
	136755321251664 -> 136755320452816
	136755320452816 [label=AccumulateGrad]
	136755320452864 -> 136755320439136
	136755320452864 [label=ReluBackward0]
	136755320453248 -> 136755320452864
	136755320453248 [label=AddmmBackward0]
	136755320453344 -> 136755320453248
	136755321256784 [label="classifiers.14.bias
 (1018)" fillcolor=lightblue]
	136755321256784 -> 136755320453344
	136755320453344 [label=AccumulateGrad]
	136755320809760 -> 136755320453248
	136755320453440 -> 136755320453248
	136755320453440 [label=TBackward0]
	136755320453632 -> 136755320453440
	136755321256944 [label="classifiers.14.weight
 (1018, 768)" fillcolor=lightblue]
	136755321256944 -> 136755320453632
	136755320453632 [label=AccumulateGrad]
	136755320452576 -> 136755320439136
	136755320452576 [label=TBackward0]
	136755320453584 -> 136755320452576
	136755321251824 [label="hidden_layer1.14.weight
 (970, 1018)" fillcolor=lightblue]
	136755321251824 -> 136755320453584
	136755320453584 [label=AccumulateGrad]
	136755320452000 -> 136755320812160
	136755320452000 [label=TBackward0]
	136755320453488 -> 136755320452000
	136755367098336 [label="hidden_layer2.14.weight
 (1, 970)" fillcolor=lightblue]
	136755367098336 -> 136755320453488
	136755320453488 [label=AccumulateGrad]
	136755320812208 -> 136755320810144
	136755320812208 [label=AddmmBackward0]
	136755320453728 -> 136755320812208
	136755367608624 [label="hidden_layer2.15.bias
 (1)" fillcolor=lightblue]
	136755367608624 -> 136755320453728
	136755320453728 [label=AccumulateGrad]
	136755320453296 -> 136755320812208
	136755320453296 [label=ReluBackward0]
	136755320452384 -> 136755320453296
	136755320452384 [label=AddmmBackward0]
	136755320453920 -> 136755320452384
	136755321251344 [label="hidden_layer1.15.bias
 (970)" fillcolor=lightblue]
	136755321251344 -> 136755320453920
	136755320453920 [label=AccumulateGrad]
	136755320453680 -> 136755320452384
	136755320453680 [label=ReluBackward0]
	136755320453824 -> 136755320453680
	136755320453824 [label=AddmmBackward0]
	136755320454256 -> 136755320453824
	136755321256464 [label="classifiers.15.bias
 (1018)" fillcolor=lightblue]
	136755321256464 -> 136755320454256
	136755320454256 [label=AccumulateGrad]
	136755320809760 -> 136755320453824
	136755320453968 -> 136755320453824
	136755320453968 [label=TBackward0]
	136755320454208 -> 136755320453968
	136755321256624 [label="classifiers.15.weight
 (1018, 768)" fillcolor=lightblue]
	136755321256624 -> 136755320454208
	136755320454208 [label=AccumulateGrad]
	136755320453536 -> 136755320452384
	136755320453536 [label=TBackward0]
	136755320454112 -> 136755320453536
	136755321251504 [label="hidden_layer1.15.weight
 (970, 1018)" fillcolor=lightblue]
	136755321251504 -> 136755320454112
	136755320454112 [label=AccumulateGrad]
	136755320452720 -> 136755320812208
	136755320452720 [label=TBackward0]
	136755320454016 -> 136755320452720
	136755367608464 [label="hidden_layer2.15.weight
 (1, 970)" fillcolor=lightblue]
	136755367608464 -> 136755320454016
	136755320454016 [label=AccumulateGrad]
	136755320812448 -> 136755320407200
}
