
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
tokenizer_config.json: 100% 110/110 [00:00<00:00, 271kB/s]
vocab.txt: 100% 15.7k/15.7k [00:00<00:00, 17.8MB/s]
config.json: 100% 478/478 [00:00<00:00, 1.13MB/s]












pytorch_model.bin: 100% 359M/359M [00:22<00:00, 15.7MB/s]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
‚îè‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ[1m   [22m‚îÉ[1m Name              [22m‚îÉ[1m Type             [22m‚îÉ[1m Params [22m‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ 0 ‚îÇ bert              ‚îÇ BertModel        ‚îÇ 89.1 M ‚îÇ
‚îÇ 1 ‚îÇ classifiers       ‚îÇ ModuleList       ‚îÇ 12.6 M ‚îÇ
‚îÇ 2 ‚îÇ hidden_layer      ‚îÇ ModuleList       ‚îÇ 16.4 K ‚îÇ
‚îÇ 3 ‚îÇ sigmoid           ‚îÇ Sigmoid          ‚îÇ      0 ‚îÇ
‚îÇ 4 ‚îÇ criterion         ‚îÇ BCELoss          ‚îÇ      0 ‚îÇ
‚îÇ 5 ‚îÇ metrics           ‚îÇ MetricCollection ‚îÇ      0 ‚îÇ
‚îÇ 6 ‚îÇ metrics_per_label ‚îÇ MetricCollection ‚îÇ      0 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
[1mTrainable params[22m: 19.7 M
[1mNon-trainable params[22m: 82.0 M
[1mTotal params[22m: 101 M
[1mTotal estimated model params size (MB)[22m: 406
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of
training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a
lower value for log_every_n_steps if you want to see logs for the training epoch.
[37mEpoch 0/3 [39m [35m‚îÅ‚îÅ[90m‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[39m [37m2/26[39m [37m0:00:01 ‚Ä¢ 0:00:02[39m [37m22.50it/s[39m [37mv_num: sxzw train/loss:     









                                                                        [37m0.687                       


        0.5040, 0.5276, 0.5073, 0.5155, 0.5313, 0.5076, 0.5096, 0.5132, 0.5209,
        0.5308, 0.5187, 0.5405, 0.5217, 0.5039, 0.5710, 0.5129, 0.5076, 0.5354,
        0.5270, 0.5034, 0.5293, 0.5285, 0.5194, 0.5003, 0.5177, 0.5213, 0.5338,
        0.5352, 0.5077, 0.5518, 0.5350, 0.5265, 0.5353, 0.5440, 0.5306, 0.5453,
        0.5272, 0.5219, 0.5030, 0.5351, 0.5261, 0.5154, 0.5467, 0.5506, 0.5280,
        0.5524, 0.5077, 0.5145, 0.5334, 0.5447, 0.5134, 0.5151, 0.5174, 0.5245,
        0.5381, 0.5289, 0.5000, 0.5359, 0.5230, 0.5166, 0.5161, 0.5380, 0.5439,
        0.5222, 0.5405, 0.5362, 0.5279, 0.5141, 0.5530, 0.5275, 0.5337, 0.5180,
        0.5259, 0.5235, 0.5000, 0.5403, 0.5403, 0.5221, 0.5403, 0.5224, 0.5110,
        0.5076, 0.5904, 0.5276, 0.5493, 0.5424, 0.5172, 0.5694, 0.5299, 0.5291,
        0.5240, 0.5421, 0.5195, 0.5293, 0.5235, 0.5005, 0.5366, 0.5303, 0.5292,
        0.5519, 0.5497, 0.5258, 0.5117, 0.5175, 0.5656, 0.5221, 0.5659, 0.5321,
        0.5434, 0.5747, 0.5459, 0.5556, 0.5362, 0.5295, 0.5230, 0.5623, 0.5406,
        0.5211, 0.5168, 0.5245, 0.5483, 0.5155, 0.5396, 0.5660, 0.5304, 0.5214,
        0.5392, 0.5284, 0.5207, 0.5622, 0.5511, 0.5457, 0.5207, 0.5402, 0.5686,
        0.5345, 0.5057, 0.5249, 0.5198, 0.5239, 0.5188, 0.5558, 0.5289, 0.5395,
        0.5282, 0.5254, 0.5299, 0.5705, 0.5295, 0.5212, 0.5179, 0.5641, 0.5253,
        0.5348, 0.5402, 0.5788, 0.5346, 0.5057, 0.5132, 0.5017, 0.5450, 0.5529,
        0.5506, 0.5258, 0.5559, 0.5630, 0.5206, 0.5301, 0.5518, 0.5641, 0.5397,
        0.5472, 0.5969, 0.5370, 0.5271, 0.5495, 0.5555, 0.5142, 0.5576, 0.5108,
        0.5709, 0.5250, 0.5261, 0.5339, 0.5304, 0.5276, 0.5111, 0.5387, 0.5352,
        0.5247, 0.5306, 0.5492, 0.5693, 0.5462, 0.5425, 0.5446, 0.5410, 0.5264,
        0.5317, 0.5649, 0.5554, 0.5227, 0.5258, 0.5722, 0.5556, 0.5487, 0.5467,
        0.5413, 0.5465, 0.5325, 0.5922, 0.5390, 0.5658, 0.5389, 0.5378, 0.5738,
        0.5360, 0.5405, 0.5689, 0.5470, 0.5070, 0.5360, 0.5516, 0.5807, 0.5375,
        0.5509, 0.5404, 0.5838, 0.5132, 0.5440, 0.5475, 0.5347, 0.5469, 0.5188,
        0.5263, 0.5413, 0.5533, 0.5067, 0.5505, 0.5618, 0.6065, 0.5650, 0.5195,
        0.5301, 0.5445, 0.5383, 0.5423, 0.5361, 0.5156, 0.5381, 0.5530, 0.5446,
        0.5685, 0.5386, 0.5660, 0.5473, 0.5572, 0.5707, 0.5304, 0.5194, 0.5518,
        0.5758, 0.5777, 0.5251, 0.5576, 0.5445, 0.5475, 0.5617, 0.5621, 0.5825,
        0.5555, 0.5601, 0.5267, 0.5470, 0.5474, 0.5516, 0.5256, 0.5505, 0.5363,
        0.5640, 0.5358, 0.5594, 0.5335, 0.5707, 0.5377, 0.5286, 0.5267, 0.5342,
        0.5444, 0.5491, 0.6010, 0.5382, 0.5129, 0.5970, 0.5000, 0.5065, 0.5564,
        0.5461, 0.5000, 0.5666, 0.5382, 0.5343, 0.5655, 0.5663, 0.5161, 0.5687,
        0.5250, 0.5676, 0.5401, 0.5203, 0.5387, 0.5339, 0.5239, 0.5602, 0.5186,
        0.5652, 0.5255, 0.5518, 0.5373, 0.5146, 0.5329, 0.5451, 0.5359, 0.5345,
        0.5850, 0.5637, 0.5462, 0.5355, 0.5527, 0.5491, 0.5227, 0.5582, 0.5498,
        0.5493, 0.5313, 0.5299, 0.5180, 0.5409, 0.5675, 0.5161, 0.5412, 0.5492,
        0.5169, 0.5370, 0.5263, 0.5543, 0.5342, 0.5524, 0.5512, 0.5443, 0.5000,
        0.5409, 0.5364, 0.5733, 0.5216, 0.5412, 0.5245, 0.5439, 0.5260, 0.5451,
        0.5910, 0.5826, 0.5490, 0.5286, 0.5353, 0.5639, 0.5684, 0.5439, 0.5400,
        0.5363, 0.5560, 0.5794, 0.5199, 0.5460, 0.5362, 0.5486, 0.5527, 0.5334,
        0.5339, 0.5229, 0.5309, 0.5490, 0.5447, 0.5749, 0.5514, 0.5464, 0.5619,
        0.5323, 0.5102, 0.5464, 0.5669, 0.5514, 0.5396, 0.5376, 0.5453, 0.5100,
        0.5447, 0.5675, 0.5769, 0.5511, 0.5450, 0.5291, 0.5485, 0.5208, 0.5201,
        0.5840, 0.5986, 0.5278, 0.5527, 0.5467, 0.5460, 0.5421, 0.5115, 0.5434,
        0.5342, 0.5512, 0.5517, 0.5650, 0.5296, 0.5516, 0.5588, 0.5598, 0.5623,
        0.5541, 0.5411, 0.5259, 0.5410, 0.5636, 0.5335, 0.5417, 0.5527, 0.5343,
        0.5510, 0.5295, 0.5600, 0.5582, 0.5450, 0.5504, 0.5057, 0.5513, 0.5586,
        0.5468, 0.5485, 0.5483, 0.5347, 0.5845, 0.5504, 0.5329, 0.5401, 0.5451,
        0.5532, 0.5074, 0.5559, 0.5258, 0.5754, 0.5459, 0.5281, 0.5390, 0.5392,
        0.5436, 0.5412, 0.5367, 0.5527, 0.5478, 0.5316, 0.5558, 0.5413, 0.5401,
        0.5337, 0.5451, 0.5818, 0.5457, 0.5533, 0.5521, 0.5493, 0.5504, 0.5459,
        0.5295, 0.5246, 0.5764, 0.5323, 0.5466, 0.5461, 0.5525, 0.5479, 0.5695,
        0.5447, 0.5560, 0.5651, 0.5640, 0.5504, 0.5446, 0.5508, 0.5519, 0.5532,
        0.5420, 0.5547, 0.5484, 0.5480, 0.5428, 0.5236, 0.5339, 0.5466, 0.5586,
        0.5207, 0.5437, 0.5492, 0.5311, 0.5206, 0.5483, 0.5246, 0.5527, 0.5475,
        0.5337, 0.5641, 0.5713, 0.5528, 0.5340, 0.5193, 0.5553, 0.5187, 0.5619,
        0.5419, 0.5496, 0.5483, 0.5876, 0.5405, 0.5441, 0.5521, 0.5325, 0.5461,
        0.5650, 0.5646, 0.5426, 0.5788, 0.5437, 0.5557, 0.5607, 0.5550, 0.5540,
        0.5366, 0.5507, 0.5182, 0.5558, 0.5548, 0.5670, 0.5398, 0.5658, 0.5406,
        0.5480, 0.5578, 0.5410, 0.5759, 0.5340, 0.5612, 0.5419, 0.5558, 0.5184,
        0.5633, 0.5566, 0.5573, 0.5454, 0.5445, 0.5476, 0.5489, 0.5517, 0.5471,
        0.5530, 0.5573, 0.5645, 0.5554, 0.5548, 0.5772, 0.5660, 0.5371, 0.5278,
        0.5860, 0.5506, 0.5501, 0.5619, 0.5559, 0.5689, 0.5803, 0.5499, 0.5698,
        0.5323, 0.5248, 0.5548, 0.5550, 0.5467, 0.5646, 0.5508, 0.5220, 0.5453,
        0.5492, 0.5262, 0.5417, 0.6023, 0.5536, 0.5693, 0.5352, 0.5486, 0.5628,
        0.5198, 0.5478, 0.5722, 0.5486, 0.5677, 0.5333, 0.5412, 0.5443, 0.5645,
        0.5609, 0.5291, 0.5571, 0.5660, 0.5563, 0.5846, 0.5598, 0.5602, 0.5274,
        0.5491, 0.5657, 0.5653, 0.5464, 0.5634, 0.5679, 0.6074, 0.5634, 0.5517,
        0.5710, 0.5437, 0.5479, 0.5413, 0.5502, 0.5577, 0.5491, 0.5325, 0.5608,
        0.5695, 0.5591, 0.5471, 0.5536, 0.5748, 0.5584, 0.5387, 0.5658, 0.5631,
        0.5554, 0.5488, 0.5676, 0.5377, 0.5558, 0.5858, 0.5577, 0.5365, 0.5586,
        0.5515, 0.5771, 0.5639, 0.5840, 0.5607, 0.5578, 0.5578, 0.5729, 0.5738,
        0.5860, 0.5642, 0.5837, 0.5632, 0.5497, 0.5465, 0.5732, 0.5456, 0.5671,
        0.5789, 0.5478, 0.5692, 0.5751, 0.5892, 0.5631, 0.5604, 0.5589, 0.5851,
        0.5192, 0.5549, 0.5787, 0.5574, 0.5560, 0.5600, 0.5750, 0.5918, 0.5440,
        0.5656, 0.5730, 0.5770, 0.5532, 0.5622, 0.5691, 0.5628, 0.5598, 0.5731,
        0.5590, 0.5314, 0.5423, 0.5649, 0.5774, 0.5460, 0.5581, 0.5727, 0.5584,
        0.5476, 0.5675, 0.5813, 0.5572, 0.5568, 0.5471, 0.5591, 0.5659, 0.5767,
        0.5633, 0.5722, 0.5870, 0.5691, 0.5878, 0.5694, 0.5779, 0.5714, 0.5824,
        0.5404, 0.5780, 0.5649, 0.5850, 0.5395, 0.5708, 0.5764, 0.5751, 0.5589,
        0.5703, 0.5827, 0.5714, 0.5925, 0.5711, 0.5772, 0.5494, 0.5897, 0.5582,
        0.5790, 0.5652, 0.5569, 0.5534, 0.5663, 0.5538, 0.5490, 0.5845, 0.5702,
        0.5696, 0.5603, 0.5954, 0.5653, 0.5793, 0.5573, 0.5689, 0.5887, 0.5802,
        0.5796, 0.5679, 0.5726, 0.5652, 0.5702, 0.5485, 0.5771, 0.5734, 0.5969,
        0.5648, 0.5699, 0.5722, 0.5467, 0.5488, 0.5882, 0.5774, 0.5621, 0.5733,
        0.5827, 0.5510, 0.5784, 0.5792, 0.5709, 0.5833, 0.5782, 0.5643, 0.5879,
        0.5847, 0.5690, 0.5728, 0.5751, 0.5681, 0.5682, 0.5769, 0.5712, 0.5414],
       device='cuda:0', grad_fn=<SelectBackward0>)
tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,
        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,
        0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1], device='cuda:0')
[37mEpoch 0/3 [39m [35m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[39m [37m26/26[39m [37m0:00:29 ‚Ä¢ 0:00:00[39m [37m0.88it/s[39m [37mv_num: sxzw train/loss:     
                                                                        [37m0.687                       
[?25h
Error executing job with overrides: []
Traceback (most recent call last):
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 494, in main
    trainer.fit(model, data_module)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 989, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1035, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 203, in run
    self.on_advance_end()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 373, in on_advance_end
    call._call_lightning_module_hook(trainer, "on_train_epoch_end")
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 283, in on_train_epoch_end
    f"{mode}/accuracy_label_{i}": metrics_per_label[
TypeError: 'Tensor' object is not callable
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.