
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ[1m   [22mâ”ƒ[1m Name                        [22mâ”ƒ[1m Type             [22mâ”ƒ[1m Params [22mâ”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ bert                        â”‚ BertModel        â”‚ 89.1 M â”‚
â”‚ 1 â”‚ classifiers                 â”‚ ModuleList       â”‚ 12.6 M â”‚
â”‚ 2 â”‚ hidden_layer                â”‚ ModuleList       â”‚ 16.4 K â”‚
â”‚ 3 â”‚ sigmoid                     â”‚ Sigmoid          â”‚      0 â”‚
â”‚ 4 â”‚ criterion                   â”‚ BCELoss          â”‚      0 â”‚
â”‚ 5 â”‚ metrics                     â”‚ MetricCollection â”‚      0 â”‚
â”‚ 6 â”‚ metrics_per_label_accuracy  â”‚ MetricCollection â”‚      0 â”‚
â”‚ 7 â”‚ metrics_per_label_precision â”‚ MetricCollection â”‚      0 â”‚
â”‚ 8 â”‚ metrics_per_label_recall    â”‚ MetricCollection â”‚      0 â”‚
â”‚ 9 â”‚ metrics_per_label_f1score   â”‚ MetricCollection â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[22m: 19.7 M
[1mNon-trainable params[22m: 82.0 M
[1mTotal params[22m: 101 M
[1mTotal estimated model params size (MB)[22m: 406
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of
training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a
lower value for log_every_n_steps if you want to see logs for the training epoch.
[37mEpoch 0/3 [39m [35mâ”â”[90mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m2/26[39m [37m0:00:01 â€¢ 0:00:02[39m [37m22.56it/s[39m [37mv_num: zju0 train/loss:     











                                                                        [37m0.694                       


[37mValidation[39m [35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[90mâ•ºâ”â”â”[39m [37m6/7  [39m [37m0:00:03 â€¢ 0:00:01[39m [37m1.45it/s
Error executing job with overrides: []
Traceback (most recent call last):
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 513, in main
    trainer.fit(model, data_module)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 989, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1035, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 203, in run
    self.on_advance_end()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 373, in on_advance_end
    call._call_lightning_module_hook(trainer, "on_train_epoch_end")
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 294, in on_train_epoch_end
    metrics_per_label_accuracy = self.metrics_per_label(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1695, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'MaltiLabelClassifierModel' object has no attribute 'metrics_per_label'
        0.5000, 0.5120, 0.5026, 0.5120, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5269, 0.5000, 0.5062, 0.5082, 0.5006, 0.5028, 0.5064, 0.5166, 0.5215,
        0.5273, 0.5217, 0.5135, 0.5000, 0.5118, 0.5087, 0.5106, 0.5004, 0.5051,
        0.5000, 0.5085, 0.5178, 0.5231, 0.5243, 0.5018, 0.5000, 0.5000, 0.5000,
        0.5104, 0.5007, 0.5000, 0.5000, 0.5000, 0.5000, 0.5118, 0.5000, 0.5146,
        0.5161, 0.5199, 0.5101, 0.5000, 0.5233, 0.5094, 0.5000, 0.5021, 0.5178,
        0.5034, 0.5028, 0.5099, 0.5141, 0.5000, 0.5323, 0.5072, 0.5018, 0.5000,
        0.5211, 0.5000, 0.5289, 0.5070, 0.5252, 0.5001, 0.5386, 0.5000, 0.5244,
        0.5291, 0.5000, 0.5000, 0.5064, 0.5230, 0.5154, 0.5004, 0.5306, 0.5219,
        0.5096, 0.5239, 0.5000, 0.5000, 0.5113, 0.5069, 0.5006, 0.5231, 0.5000,
        0.5000, 0.5150, 0.5043, 0.5000, 0.5161, 0.5101, 0.5060, 0.5133, 0.5000,
        0.5072, 0.5185, 0.5007, 0.5105, 0.5001, 0.5000, 0.5000, 0.5052, 0.5127,
        0.5235, 0.5000, 0.5000, 0.5096, 0.5156, 0.5000, 0.5006, 0.5000, 0.5080,
        0.5207, 0.5130, 0.5000, 0.5009, 0.5000, 0.5000, 0.5000, 0.5032, 0.5000,
        0.5089, 0.5005, 0.5000, 0.5000, 0.5362, 0.5000, 0.5083, 0.5262, 0.5000,
        0.5172, 0.5281, 0.5220, 0.5087, 0.5107, 0.5229, 0.5184, 0.5171, 0.5087,
        0.5147, 0.5000, 0.5000, 0.5117, 0.5122, 0.5229, 0.5155, 0.5031, 0.5000,
        0.5000, 0.5036, 0.5105, 0.5224, 0.5099, 0.5012, 0.5213, 0.5294, 0.5037,
        0.5000, 0.5043, 0.5312, 0.5133, 0.5000, 0.5212, 0.5008, 0.5038, 0.5000,
        0.5072, 0.5000, 0.5000, 0.5222, 0.5073, 0.5000, 0.5000, 0.5084, 0.5283,
        0.5012, 0.5036, 0.5000, 0.5000, 0.5014, 0.5000, 0.5210, 0.5294, 0.5097,
        0.5040, 0.5039, 0.5064, 0.5000, 0.5098, 0.5068, 0.5142, 0.5163, 0.5000,
        0.5000, 0.5082, 0.5000, 0.5000, 0.5024, 0.5020, 0.5173, 0.5136, 0.5022,
        0.5097, 0.5184, 0.5046, 0.5000, 0.5000, 0.5000, 0.5000, 0.5174, 0.5004,
        0.5016, 0.5076, 0.5016, 0.5049, 0.5224, 0.5250, 0.5000, 0.5092, 0.5066,
        0.5015, 0.5000, 0.5043, 0.5129, 0.5133, 0.5114, 0.5164, 0.5178, 0.5000,
        0.5098, 0.5025, 0.5181, 0.5147, 0.5000, 0.5177, 0.5030, 0.5220, 0.5000,
        0.5000, 0.5176, 0.5280, 0.5006, 0.5000, 0.5293, 0.5209, 0.5000, 0.5199,
        0.5000, 0.5027, 0.5162, 0.5000, 0.5000, 0.5012, 0.5147, 0.5063, 0.5021,
        0.5221, 0.5214, 0.5080, 0.5046, 0.5098, 0.5004, 0.5153, 0.5038, 0.5177,
        0.5166, 0.5000, 0.5179, 0.5185, 0.5238, 0.5000, 0.5116, 0.5055, 0.5094,
        0.5000, 0.5083, 0.5011, 0.5006, 0.5366, 0.5000, 0.5312, 0.5093, 0.5225,
        0.5058, 0.5123, 0.5027, 0.5274, 0.5125, 0.5255, 0.5002, 0.5000, 0.5134,
        0.5000, 0.5125, 0.5000, 0.5000, 0.5000, 0.5091, 0.5095, 0.5280, 0.5116,
        0.5000, 0.5085, 0.5210, 0.5230, 0.5039, 0.5097, 0.5048, 0.5000, 0.5000,
        0.5263, 0.5208, 0.5020, 0.5068, 0.5011, 0.5127, 0.5015, 0.5248, 0.5266,
        0.5000, 0.5103, 0.5118, 0.5039, 0.5056, 0.5104, 0.5120, 0.5145, 0.5199,
        0.5000, 0.5082, 0.5114, 0.5066, 0.5026, 0.5120, 0.5252, 0.5007, 0.5099,
        0.5000, 0.5243, 0.5168, 0.5000, 0.5126, 0.5070, 0.5254, 0.5000, 0.5283,
        0.5059, 0.5000, 0.5105, 0.5074, 0.5250, 0.5278, 0.5170, 0.5145, 0.5057,
        0.5069, 0.5140, 0.5117, 0.5083, 0.5059, 0.5089, 0.5099, 0.5163, 0.5016,
        0.5058, 0.5158, 0.5204, 0.5163, 0.5000, 0.5152, 0.5181, 0.5096, 0.5000,
        0.5130, 0.5177, 0.5000, 0.5128, 0.5125, 0.5156, 0.5209, 0.5160, 0.5013,
        0.5146, 0.5096, 0.5201, 0.5147, 0.5000, 0.5214, 0.5100, 0.5075, 0.5124,
        0.5000, 0.5235, 0.5362, 0.5171, 0.5000, 0.5154, 0.5035, 0.5206, 0.5025,
        0.5039, 0.5055, 0.5162, 0.5029, 0.5243, 0.5068, 0.5257, 0.5104, 0.5292,
        0.5262, 0.5221, 0.5000, 0.5120, 0.5244, 0.5242, 0.5056, 0.5057, 0.5010,
        0.5101, 0.5131, 0.5227, 0.5000, 0.5000, 0.5134, 0.5000, 0.5020, 0.5000,
        0.5082, 0.5230, 0.5126, 0.5058, 0.5000, 0.5009, 0.5138, 0.5226, 0.5275,
        0.5214, 0.5079, 0.5000, 0.5227, 0.5065, 0.5125, 0.5301, 0.5009, 0.5197,
        0.5024, 0.5000, 0.5122, 0.5072, 0.5219, 0.5000, 0.5027, 0.5275, 0.5169,
        0.5111, 0.5193, 0.5121, 0.5171, 0.5182, 0.5169, 0.5240, 0.5079, 0.5162,
        0.5193, 0.5268, 0.5160, 0.5078, 0.5352, 0.5045, 0.5282, 0.5000, 0.5100,
        0.5253, 0.5099, 0.5220, 0.5016, 0.5167, 0.5243, 0.5029, 0.5203, 0.5162,
        0.5241, 0.5153, 0.5071, 0.5278, 0.5158, 0.5177, 0.5230, 0.5119, 0.5358,
        0.5202, 0.5248, 0.5193, 0.5072, 0.5227, 0.5230, 0.5052, 0.5070, 0.5146,
        0.5066, 0.5115, 0.5343, 0.5221, 0.5000, 0.5297, 0.5289, 0.5142, 0.5403,
        0.5146, 0.5073, 0.5219, 0.5181, 0.5011, 0.5136, 0.5080, 0.5046, 0.5235,
        0.5221, 0.5000, 0.5265, 0.5028, 0.5314, 0.5185, 0.5007, 0.5100, 0.5238,
        0.5387, 0.5254, 0.5275, 0.5000, 0.5170, 0.5183, 0.5187, 0.5228, 0.5273,
        0.5328, 0.5130, 0.5262, 0.5120, 0.5356, 0.5133, 0.5331, 0.5031, 0.5169,
        0.5153, 0.5132, 0.5230, 0.5313, 0.5066, 0.5246, 0.5321, 0.5010, 0.5249,
        0.5353, 0.5228, 0.5176, 0.5009, 0.5057, 0.5328, 0.5386, 0.5077, 0.5198,
        0.5000, 0.5000, 0.5224, 0.5226, 0.5158, 0.5268, 0.5116, 0.5204, 0.5325,
        0.5256, 0.5226, 0.5098, 0.5313, 0.5171, 0.5235, 0.5224, 0.5352, 0.5092,
        0.5282, 0.5130, 0.5140, 0.5157, 0.5050, 0.5173, 0.5449, 0.5122, 0.5309,
        0.5260, 0.5244, 0.5294, 0.5019, 0.5335, 0.5171, 0.5260, 0.5074, 0.5151,
        0.5080, 0.5350, 0.5224, 0.5302, 0.5257, 0.5091, 0.5484, 0.5066, 0.5090,
        0.5282, 0.5200, 0.5336, 0.5464, 0.5138, 0.5138, 0.5233, 0.5164, 0.5304,
        0.5211, 0.5188, 0.5249, 0.5000, 0.5368, 0.5250, 0.5364, 0.5398, 0.5281,
        0.5194, 0.5000, 0.5140, 0.5291, 0.5533, 0.5312, 0.5362, 0.5170, 0.5303,
        0.5380, 0.5361, 0.5247, 0.5196, 0.5371, 0.5051, 0.5390, 0.5167, 0.5389,
        0.5085, 0.5386, 0.5149, 0.5560, 0.5288, 0.5443, 0.5366, 0.5168, 0.5198,
        0.5017, 0.5431, 0.5339, 0.5150, 0.5144, 0.5211, 0.5164, 0.5219, 0.5476,
        0.5226, 0.5238, 0.5262, 0.5220, 0.5341, 0.5216, 0.5159, 0.5258, 0.5113,
        0.5313, 0.5234, 0.5325, 0.5559, 0.5335, 0.5263, 0.5226, 0.5055, 0.5277,
        0.5508, 0.5315, 0.5345, 0.5265, 0.5469, 0.5345, 0.5261, 0.5243, 0.5375,
        0.5396, 0.5205, 0.5297, 0.5127, 0.5094, 0.5359, 0.5274, 0.5345, 0.5361,
        0.5388, 0.5173, 0.5393, 0.5406, 0.5324, 0.5321, 0.5247, 0.5167, 0.5450,
        0.5270, 0.5386, 0.5276, 0.5520, 0.5341, 0.5273, 0.5252, 0.5412, 0.5333,
        0.5444, 0.5547, 0.5378, 0.5485, 0.5358, 0.5358, 0.5255, 0.5369, 0.5399,
        0.5086, 0.5103, 0.5236, 0.5322, 0.5439, 0.5230, 0.5294, 0.5437, 0.5336,
        0.5417, 0.5398, 0.5244, 0.5171, 0.5216, 0.5348, 0.5299, 0.5520, 0.5226,
        0.5345, 0.5258, 0.5477, 0.5429, 0.5434, 0.5278, 0.5208, 0.5213, 0.5338,
        0.5460, 0.5232, 0.5377, 0.5231, 0.5470, 0.5588, 0.5252, 0.5205, 0.5332,
        0.5457, 0.5387, 0.5292, 0.5241, 0.5397, 0.5353, 0.5342, 0.5349, 0.5411,
        0.5302, 0.5371, 0.5334, 0.5365, 0.5180, 0.5484, 0.5306, 0.5341, 0.5467,
        0.5358, 0.5266, 0.5295, 0.5542, 0.5324, 0.5201, 0.5216, 0.5000, 0.5322,
        0.5369, 0.5524, 0.5367, 0.5375, 0.5416, 0.5404, 0.5442, 0.5390, 0.5220,
        0.5331, 0.5136, 0.5340, 0.5236, 0.5280, 0.5476, 0.5499, 0.5317, 0.5143],
       device='cuda:0', grad_fn=<SelectBackward0>)
tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,
        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1,
        1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,
        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,
        0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1,
        1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,
        0, 1, 1], device='cuda:0')
[37mEpoch 0/3 [39m [35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m26/26[39m [37m0:00:27 â€¢ 0:00:00[39m [37m0.95it/s[39m [37mv_num: zju0 train/loss:     
                                                                        [37m0.694                       
[?25h