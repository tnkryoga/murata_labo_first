
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ[1m   [22mâ”ƒ[1m Name                        [22mâ”ƒ[1m Type             [22mâ”ƒ[1m Params [22mâ”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ bert                        â”‚ BertModel        â”‚ 89.1 M â”‚
â”‚ 1 â”‚ classifiers                 â”‚ ModuleList       â”‚ 12.6 M â”‚
â”‚ 2 â”‚ hidden_layer                â”‚ ModuleList       â”‚ 16.4 K â”‚
â”‚ 3 â”‚ sigmoid                     â”‚ Sigmoid          â”‚      0 â”‚
â”‚ 4 â”‚ criterion                   â”‚ BCELoss          â”‚      0 â”‚
â”‚ 5 â”‚ metrics                     â”‚ MetricCollection â”‚      0 â”‚
â”‚ 6 â”‚ metrics_per_label_accuracy  â”‚ MetricCollection â”‚      0 â”‚
â”‚ 7 â”‚ metrics_per_label_precision â”‚ MetricCollection â”‚      0 â”‚
â”‚ 8 â”‚ metrics_per_label_recall    â”‚ MetricCollection â”‚      0 â”‚
â”‚ 9 â”‚ metrics_per_label_f1score   â”‚ MetricCollection â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[22m: 19.7 M
[1mNon-trainable params[22m: 82.0 M
[1mTotal params[22m: 101 M
[1mTotal estimated model params size (MB)[22m: 406
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of
training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a
lower value for log_every_n_steps if you want to see logs for the training epoch.
[37mEpoch 0/3 [39m [35mâ”â”â”[90mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m3/26[39m [37m0:00:02 â€¢ 0:00:14[39m [37m1.68it/s[39m [37mv_num: lth2 train/loss:     












                                                                        [37m0.693                       


        0.5198, 0.5229, 0.5539, 0.5216, 0.5506, 0.5258, 0.5370, 0.5366, 0.5419,
        0.5284, 0.5532, 0.5255, 0.5215, 0.5390, 0.5461, 0.5297, 0.5145, 0.5253,
        0.5118, 0.5268, 0.5193, 0.5294, 0.5366, 0.5202, 0.5172, 0.5322, 0.5418,
        0.5000, 0.5200, 0.5450, 0.5617, 0.5084, 0.5329, 0.5330, 0.5006, 0.5498,
        0.5058, 0.5464, 0.5234, 0.5329, 0.5031, 0.5409, 0.5471, 0.5015, 0.5242,
        0.5234, 0.5280, 0.5397, 0.5063, 0.5362, 0.5425, 0.5459, 0.5512, 0.5202,
        0.5196, 0.5297, 0.5350, 0.5296, 0.5466, 0.5271, 0.5177, 0.5043, 0.5171,
        0.5142, 0.5242, 0.5304, 0.5337, 0.5264, 0.5476, 0.5388, 0.5250, 0.5286,
        0.5198, 0.5169, 0.5380, 0.5000, 0.5196, 0.5157, 0.5319, 0.5368, 0.5306,
        0.5446, 0.5231, 0.5250, 0.5401, 0.5453, 0.5105, 0.5292, 0.5227, 0.5304,
        0.5284, 0.5193, 0.5457, 0.5344, 0.5285, 0.5445, 0.5476, 0.5192, 0.5320,
        0.5327, 0.5419, 0.5413, 0.5631, 0.5314, 0.5364, 0.5267, 0.5511, 0.5323,
        0.5291, 0.5234, 0.5322, 0.5377, 0.5526, 0.5328, 0.5387, 0.5315, 0.5348,
        0.5151, 0.5517, 0.5515, 0.5181, 0.5605, 0.5190, 0.5200, 0.5247, 0.5575,
        0.5208, 0.5154, 0.5196, 0.5335, 0.5316, 0.5335, 0.5365, 0.5275, 0.5496,
        0.5318, 0.5868, 0.5154, 0.5443, 0.5272, 0.5550, 0.5295, 0.5260, 0.5402,
        0.5311, 0.5316, 0.5431, 0.5374, 0.5436, 0.5371, 0.5189, 0.5268, 0.5335,
        0.5146, 0.5151, 0.5205, 0.5497, 0.5526, 0.5290, 0.5348, 0.5478, 0.5293,
        0.5586, 0.5214, 0.5428, 0.5352, 0.5468, 0.5385, 0.5409, 0.5014, 0.5492,
        0.5256, 0.5404, 0.5235, 0.5250, 0.5249, 0.5298, 0.5351, 0.5178, 0.5329,
        0.5515, 0.5522, 0.5482, 0.5470, 0.5290, 0.5448, 0.5000, 0.5379, 0.5316,
        0.5451, 0.5723, 0.5370, 0.5294, 0.5569, 0.5382, 0.5457, 0.5398, 0.5670,
        0.5222, 0.5268, 0.5613, 0.5269, 0.5278, 0.5495, 0.5358, 0.5426, 0.5426,
        0.5592, 0.5220, 0.5247, 0.5186, 0.5479, 0.5555, 0.5410, 0.5541, 0.5322,
        0.5500, 0.5044, 0.5427, 0.5481, 0.5462, 0.5271, 0.5512, 0.5286, 0.5536,
        0.5475, 0.5304, 0.5512, 0.5243, 0.5173, 0.5304, 0.5397, 0.5681, 0.5426,
        0.5044, 0.5302, 0.5223, 0.5441, 0.5628, 0.5622, 0.5367, 0.5308, 0.5010,
        0.5673, 0.5508, 0.5402, 0.5394, 0.5494, 0.5349, 0.5258, 0.5387, 0.5113,
        0.5408, 0.5554, 0.5367, 0.5233, 0.5410, 0.5533, 0.5628, 0.5620, 0.5194,
        0.5512, 0.5459, 0.5337, 0.5468, 0.5425, 0.5353, 0.5224, 0.5197, 0.5503,
        0.5623, 0.5388, 0.5778, 0.5137, 0.5281, 0.5163, 0.5355, 0.5382, 0.5235,
        0.5584, 0.5370, 0.5260, 0.5415, 0.5530, 0.5386, 0.5560, 0.5312, 0.5623,
        0.5516, 0.5260, 0.5450, 0.5530, 0.5211, 0.5386, 0.5317, 0.5626, 0.5439,
        0.5000, 0.5489, 0.5403, 0.5351, 0.5681, 0.5531, 0.5354, 0.5505, 0.5140,
        0.5715, 0.5428, 0.5548, 0.5209, 0.5306, 0.5185, 0.5221, 0.5406, 0.5251,
        0.5433, 0.5431, 0.5441, 0.5505, 0.5430, 0.5695, 0.5182, 0.5412, 0.5576,
        0.5384, 0.5338, 0.5796, 0.5700, 0.5279, 0.5396, 0.5475, 0.5243, 0.5203,
        0.5557, 0.5505, 0.5647, 0.5403, 0.5629, 0.5416, 0.5188, 0.5224, 0.5585,
        0.5431, 0.5476, 0.5484, 0.5174, 0.5403, 0.5448, 0.5531, 0.5535, 0.5568,
        0.5330, 0.5543, 0.5413, 0.5304, 0.5381, 0.5341, 0.5442, 0.5508, 0.5364,
        0.5340, 0.5525, 0.5605, 0.5249, 0.5355, 0.5510, 0.5593, 0.5408, 0.5322,
        0.5519, 0.5447, 0.5620, 0.5293, 0.5363, 0.5567, 0.5000, 0.5650, 0.5126,
        0.5293, 0.5292, 0.5538, 0.5431, 0.5199, 0.5484, 0.5460, 0.5550, 0.5357,
        0.5425, 0.5301, 0.5409, 0.5270, 0.5416, 0.5581, 0.5533, 0.5526, 0.5639,
        0.5354, 0.5725, 0.5269, 0.5433, 0.5001, 0.5358, 0.5364, 0.5170, 0.5323,
        0.5380, 0.5327, 0.5118, 0.5543, 0.5430, 0.5483, 0.5530, 0.5491, 0.5563,
        0.5489, 0.5306, 0.5449, 0.5255, 0.5486, 0.5341, 0.5473, 0.5001, 0.5430,
        0.5438, 0.5526, 0.5373, 0.5296, 0.5706, 0.5179, 0.5349, 0.5385, 0.5865,
        0.5585, 0.5451, 0.5725, 0.5575, 0.5445, 0.5582, 0.5228, 0.5388, 0.5467,
        0.5172, 0.5704, 0.5312, 0.5391, 0.5524, 0.5424, 0.5420, 0.5440, 0.5431,
        0.5909, 0.5482, 0.5663, 0.5709, 0.5457, 0.5247, 0.5400, 0.5739, 0.5427,
        0.5435, 0.5592, 0.5137, 0.5434, 0.5763, 0.5726, 0.5550, 0.5172, 0.5457,
        0.5102, 0.5771, 0.5413, 0.5454, 0.5470, 0.5573, 0.5297, 0.5504, 0.5291,
        0.5288, 0.5394, 0.5463, 0.5324, 0.5421, 0.5650, 0.5541, 0.5549, 0.5267,
        0.5485, 0.5449, 0.5727, 0.5494, 0.5516, 0.5542, 0.5365, 0.5509, 0.5607,
        0.5459, 0.5437, 0.5380, 0.5706, 0.5307, 0.5342, 0.5432, 0.5149, 0.5492,
        0.5354, 0.5555, 0.5672, 0.5623, 0.5421, 0.5384, 0.5401, 0.5551, 0.5647,
        0.5562, 0.5504, 0.5401, 0.5688, 0.5468, 0.5494, 0.5279, 0.5613, 0.5466,
        0.5142, 0.5456, 0.5535, 0.5542, 0.5298, 0.5794, 0.5386, 0.5338, 0.5531,
        0.5474, 0.5163, 0.5293, 0.5350, 0.5614, 0.5205, 0.5311, 0.5626, 0.5412,
        0.5618, 0.5499, 0.5415, 0.5588, 0.5415, 0.5406, 0.5278, 0.5552, 0.5623,
        0.5434, 0.5673, 0.5409, 0.5489, 0.5503, 0.5597, 0.5576, 0.5735, 0.5383,
        0.5425, 0.5435, 0.5331, 0.5646, 0.5429, 0.5231, 0.5427, 0.5226, 0.5610,
        0.5609, 0.5370, 0.5697, 0.5598, 0.5317, 0.5792, 0.5559, 0.5582, 0.5680,
        0.5336, 0.5487, 0.5538, 0.5375, 0.5704, 0.5685, 0.5475, 0.5320, 0.5409,
        0.5258, 0.5824, 0.5487, 0.5719, 0.5441, 0.5203, 0.5683, 0.5496, 0.5494,
        0.5573, 0.5543, 0.5444, 0.5557, 0.5556, 0.5505, 0.5531, 0.5385, 0.5752,
        0.5558, 0.5557, 0.5644, 0.5723, 0.5620, 0.5551, 0.5695, 0.5475, 0.5580,
        0.5451, 0.5364, 0.5699, 0.5440, 0.5591, 0.5602, 0.5608, 0.5310, 0.5506,
        0.5442, 0.5451, 0.5686, 0.5574, 0.5561, 0.5635, 0.5774, 0.5606, 0.5428,
        0.5436, 0.5649, 0.5544, 0.5484, 0.5689, 0.5733, 0.5612, 0.5713, 0.5545,
        0.5539, 0.5380, 0.5532, 0.5635, 0.5497, 0.5606, 0.5610, 0.5456, 0.5564,
        0.5321, 0.5605, 0.5516, 0.5392, 0.5648, 0.5632, 0.5708, 0.5225, 0.5605,
        0.5813, 0.5525, 0.5648, 0.5315, 0.5510, 0.5430, 0.5721, 0.5737, 0.5710,
        0.5413, 0.5500, 0.5549, 0.5425, 0.5560, 0.5597, 0.5669, 0.5303, 0.5115,
        0.5496, 0.5403, 0.5524, 0.5835, 0.5633, 0.5678, 0.5711, 0.5582, 0.5768,
        0.5605, 0.5593, 0.5351, 0.5699, 0.5392, 0.5584, 0.5405, 0.5676, 0.5629,
        0.5625, 0.5747, 0.5596, 0.5605, 0.5621, 0.5526, 0.5633, 0.5270, 0.5558,
        0.5566, 0.5650, 0.5669, 0.5457, 0.5791, 0.5546, 0.5670, 0.5583, 0.5650,
        0.5572, 0.5425, 0.5216, 0.5500, 0.5558, 0.5341, 0.5825, 0.5818, 0.5615,
        0.5439, 0.5669, 0.5694, 0.5568, 0.5336, 0.5608, 0.5412, 0.5617, 0.5530,
        0.5589, 0.5338, 0.5688, 0.5640, 0.5528, 0.5721, 0.5216, 0.5516, 0.5484,
        0.5669, 0.5430, 0.5414, 0.5841, 0.5505, 0.5496, 0.5714, 0.5500, 0.5641,
        0.5593, 0.5471, 0.5631, 0.5482, 0.5524, 0.5682, 0.5710, 0.5573, 0.5507,
        0.5674, 0.5739, 0.5461, 0.5638, 0.5920, 0.5684, 0.5512, 0.5335, 0.5697,
        0.5771, 0.5808, 0.5652, 0.5538, 0.5599, 0.5775, 0.5530, 0.5662, 0.5676,
        0.5767, 0.5432, 0.5657, 0.5314, 0.5385, 0.5597, 0.5629, 0.5722, 0.5857,
        0.5738, 0.5629, 0.5427, 0.5814, 0.5446, 0.5592, 0.5669, 0.5786, 0.5787,
        0.5414, 0.5449, 0.5617, 0.5828, 0.5775, 0.5526, 0.5648, 0.5640, 0.5652],
       device='cuda:0', grad_fn=<SelectBackward0>)
tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,
        1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,
        1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,
        1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0,
        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,
        0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,
        0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,
        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,
        0, 0, 1], device='cuda:0')
[37mEpoch 0/3 [39m [35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m26/26[39m [37m0:00:27 â€¢ 0:00:00[39m [37m0.95it/s[39m [37mv_num: lth2 train/loss:     
                                                                        [37m0.693                       
[?25h
Error executing job with overrides: []
Traceback (most recent call last):
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 515, in main
    trainer.fit(model, data_module)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 989, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1035, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 203, in run
    self.on_advance_end()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 373, in on_advance_end
    call._call_lightning_module_hook(trainer, "on_train_epoch_end")
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 297, in on_train_epoch_end
    metrics_per_label_precision = self.metrics_per_label_presicion(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1695, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'MaltiLabelClassifierModel' object has no attribute 'metrics_per_label_presicion'
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.