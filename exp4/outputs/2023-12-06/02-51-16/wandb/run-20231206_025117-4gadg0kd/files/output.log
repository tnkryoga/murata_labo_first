
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ[1m   [22mâ”ƒ[1m Name              [22mâ”ƒ[1m Type             [22mâ”ƒ[1m Params [22mâ”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ bert              â”‚ BertModel        â”‚ 89.1 M â”‚
â”‚ 1 â”‚ classifiers       â”‚ ModuleList       â”‚ 12.6 M â”‚
â”‚ 2 â”‚ hidden_layer      â”‚ ModuleList       â”‚ 16.4 K â”‚
â”‚ 3 â”‚ sigmoid           â”‚ Sigmoid          â”‚      0 â”‚
â”‚ 4 â”‚ criterion         â”‚ BCELoss          â”‚      0 â”‚
â”‚ 5 â”‚ metrics           â”‚ MetricCollection â”‚      0 â”‚
â”‚ 6 â”‚ metrics_per_label â”‚ MetricCollection â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[22m: 19.7 M
[1mNon-trainable params[22m: 82.0 M
[1mTotal params[22m: 101 M
[1mTotal estimated model params size (MB)[22m: 406
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of
training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a
lower value for log_every_n_steps if you want to see logs for the training epoch.
[37mEpoch 0/3 [39m [35mâ”â”â”[90mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m3/26[39m [37m0:00:02 â€¢ 0:00:15[39m [37m1.60it/s[39m [37mv_num: g0kd train/loss:     













                                                                        [37m0.690                       


        0.5175, 0.5000, 0.5000, 0.5058, 0.5196, 0.5218, 0.5000, 0.5161, 0.5332,
        0.5352, 0.5072, 0.5184, 0.5000, 0.5000, 0.5120, 0.5110, 0.5075, 0.5000,
        0.5201, 0.5141, 0.5184, 0.5064, 0.5301, 0.5000, 0.5178, 0.5199, 0.5225,
        0.5000, 0.5180, 0.5000, 0.5000, 0.5059, 0.5000, 0.5246, 0.5142, 0.5006,
        0.5388, 0.5000, 0.5022, 0.5214, 0.5207, 0.5125, 0.5197, 0.5000, 0.5222,
        0.5078, 0.5241, 0.5165, 0.5214, 0.5127, 0.5206, 0.5140, 0.5174, 0.5012,
        0.5113, 0.5261, 0.5171, 0.5229, 0.5250, 0.5111, 0.5320, 0.5257, 0.5170,
        0.5230, 0.5203, 0.5000, 0.5213, 0.5178, 0.5250, 0.5000, 0.5000, 0.5107,
        0.5089, 0.5088, 0.5000, 0.5092, 0.5000, 0.5085, 0.5000, 0.5000, 0.5196,
        0.5146, 0.5193, 0.5059, 0.5226, 0.5175, 0.5210, 0.5000, 0.5190, 0.5000,
        0.5038, 0.5000, 0.5076, 0.5149, 0.5150, 0.5356, 0.5118, 0.5071, 0.5000,
        0.5111, 0.5192, 0.5125, 0.5106, 0.5000, 0.5314, 0.5157, 0.5258, 0.5221,
        0.5217, 0.5000, 0.5073, 0.5261, 0.5000, 0.5118, 0.5167, 0.5242, 0.5211,
        0.5068, 0.5000, 0.5027, 0.5093, 0.5037, 0.5000, 0.5534, 0.5000, 0.5000,
        0.5141, 0.5365, 0.5233, 0.5084, 0.5000, 0.5186, 0.5191, 0.5064, 0.5127,
        0.5063, 0.5257, 0.5000, 0.5000, 0.5200, 0.5333, 0.5363, 0.5241, 0.5193,
        0.5026, 0.5302, 0.5125, 0.5418, 0.5237, 0.5000, 0.5000, 0.5019, 0.5000,
        0.5112, 0.5238, 0.5234, 0.5019, 0.5000, 0.5158, 0.5391, 0.5063, 0.5000,
        0.5185, 0.5206, 0.5003, 0.5114, 0.5065, 0.5000, 0.5171, 0.5130, 0.5057,
        0.5116, 0.5217, 0.5000, 0.5369, 0.5000, 0.5112, 0.5024, 0.5205, 0.5100,
        0.5264, 0.5411, 0.5231, 0.5170, 0.5000, 0.5245, 0.5028, 0.5152, 0.5261,
        0.5254, 0.5000, 0.5000, 0.5240, 0.5404, 0.5392, 0.5201, 0.5140, 0.5184,
        0.5180, 0.5247, 0.5000, 0.5119, 0.5373, 0.5220, 0.5184, 0.5000, 0.5332,
        0.5324, 0.5194, 0.5000, 0.5000, 0.5232, 0.5395, 0.5000, 0.5384, 0.5332,
        0.5229, 0.5282, 0.5000, 0.5319, 0.5000, 0.5000, 0.5336, 0.5108, 0.5127,
        0.5000, 0.5230, 0.5164, 0.5000, 0.5317, 0.5262, 0.5137, 0.5152, 0.5190,
        0.5000, 0.5000, 0.5236, 0.5091, 0.5261, 0.5305, 0.5026, 0.5331, 0.5263,
        0.5038, 0.5000, 0.5333, 0.5000, 0.5255, 0.5281, 0.5158, 0.5178, 0.5177,
        0.5186, 0.5255, 0.5277, 0.5295, 0.5000, 0.5085, 0.5260, 0.5241, 0.5085,
        0.5274, 0.5312, 0.5282, 0.5079, 0.5000, 0.5295, 0.5422, 0.5315, 0.5081,
        0.5243, 0.5278, 0.5159, 0.5206, 0.5258, 0.5295, 0.5353, 0.5222, 0.5333,
        0.5174, 0.5308, 0.5182, 0.5477, 0.5498, 0.5236, 0.5173, 0.5221, 0.5000,
        0.5189, 0.5226, 0.5385, 0.5051, 0.5230, 0.5199, 0.5137, 0.5300, 0.5320,
        0.5336, 0.5006, 0.5304, 0.5419, 0.5394, 0.5307, 0.5414, 0.5444, 0.5462,
        0.5296, 0.5259, 0.5417, 0.5000, 0.5261, 0.5019, 0.5401, 0.5581, 0.5320,
        0.5334, 0.5027, 0.5130, 0.5195, 0.5416, 0.5324, 0.5435, 0.5389, 0.5254,
        0.5622, 0.5423, 0.5235, 0.5498, 0.5538, 0.5078, 0.5518, 0.5367, 0.5407,
        0.5266, 0.5226, 0.5233, 0.5423, 0.5305, 0.5377, 0.5279, 0.5238, 0.5157,
        0.5406, 0.5152, 0.5168, 0.5485, 0.5278, 0.5373, 0.5432, 0.5000, 0.5000,
        0.5088, 0.5481, 0.5297, 0.5362, 0.5269, 0.5372, 0.5165, 0.5322, 0.5312,
        0.5179, 0.5234, 0.5412, 0.5000, 0.5560, 0.5430, 0.5226, 0.5226, 0.5000,
        0.5402, 0.5559, 0.5552, 0.5080, 0.5026, 0.5174, 0.5383, 0.5425, 0.5380,
        0.5449, 0.5435, 0.5259, 0.5236, 0.5251, 0.5327, 0.5208, 0.5108, 0.5272,
        0.5303, 0.5432, 0.5281, 0.5389, 0.5396, 0.5287, 0.5245, 0.5222, 0.5176,
        0.5197, 0.5083, 0.5176, 0.5348, 0.5521, 0.5245, 0.5452, 0.5367, 0.5229,
        0.5234, 0.5432, 0.5300, 0.5000, 0.5506, 0.5370, 0.5445, 0.5392, 0.5633,
        0.5300, 0.5469, 0.5211, 0.5521, 0.5171, 0.5566, 0.5269, 0.5000, 0.5456,
        0.5400, 0.5301, 0.5542, 0.5530, 0.5397, 0.5057, 0.5387, 0.5540, 0.5364,
        0.5603, 0.5526, 0.5229, 0.5443, 0.5458, 0.5425, 0.5457, 0.5233, 0.5465,
        0.5072, 0.5304, 0.5471, 0.5000, 0.5433, 0.5462, 0.5303, 0.5439, 0.5510,
        0.5172, 0.5212, 0.5315, 0.5385, 0.5458, 0.5294, 0.5203, 0.5247, 0.5184,
        0.5000, 0.5106, 0.5591, 0.5265, 0.5347, 0.5223, 0.5408, 0.5340, 0.5412,
        0.5319, 0.5443, 0.5527, 0.5504, 0.5542, 0.5404, 0.5429, 0.5496, 0.5392,
        0.5211, 0.5689, 0.5415, 0.5330, 0.5495, 0.5310, 0.5592, 0.5625, 0.5213,
        0.5498, 0.5504, 0.5317, 0.5359, 0.5243, 0.5289, 0.5196, 0.5095, 0.5337,
        0.5223, 0.5495, 0.5233, 0.5503, 0.5346, 0.5266, 0.5572, 0.5527, 0.5388,
        0.5596, 0.5541, 0.5688, 0.5508, 0.5466, 0.5458, 0.5441, 0.5422, 0.5432,
        0.5000, 0.5384, 0.5455, 0.5693, 0.5499, 0.5356, 0.5445, 0.5000, 0.5240,
        0.5526, 0.5071, 0.5302, 0.5398, 0.5472, 0.5624, 0.5401, 0.5594, 0.5606,
        0.5470, 0.5152, 0.5537, 0.5615, 0.5418, 0.5300, 0.5216, 0.5318, 0.5316,
        0.5651, 0.5419, 0.5536, 0.5361, 0.5391, 0.5295, 0.5229, 0.5332, 0.5465,
        0.5421, 0.5167, 0.5610, 0.5405, 0.5234, 0.5431, 0.5555, 0.5602, 0.5534,
        0.5591, 0.5317, 0.5555, 0.5269, 0.5125, 0.5241, 0.5098, 0.5671, 0.5376,
        0.5497, 0.5587, 0.5593, 0.5443, 0.5531, 0.5049, 0.5673, 0.5169, 0.5721,
        0.5321, 0.5474, 0.5523, 0.5511, 0.5522, 0.5410, 0.5493, 0.5445, 0.5382,
        0.5407, 0.5594, 0.5755, 0.5263, 0.5133, 0.5258, 0.5683, 0.5305, 0.5368,
        0.5420, 0.5514, 0.5397, 0.5511, 0.5646, 0.5438, 0.5496, 0.5597, 0.5628,
        0.5282, 0.5263, 0.5554, 0.5764, 0.5656, 0.5418, 0.5207, 0.5519, 0.5435,
        0.5500, 0.5491, 0.5635, 0.5452, 0.5460, 0.5356, 0.5265, 0.5529, 0.5418,
        0.5531, 0.5595, 0.5671, 0.5259, 0.5619, 0.5549, 0.5515, 0.5434, 0.5634,
        0.5416, 0.5560, 0.5559, 0.5625, 0.5201, 0.5066, 0.5652, 0.5503, 0.5585,
        0.5125, 0.5338, 0.5479, 0.5275, 0.5619, 0.5524, 0.5613, 0.5466, 0.5585,
        0.5266, 0.5579, 0.5576, 0.5624, 0.5553, 0.5206, 0.5538, 0.5504, 0.5406,
        0.5281, 0.5389, 0.5601, 0.5203, 0.5762, 0.5467, 0.5511, 0.5507, 0.5458,
        0.5109, 0.5480, 0.5636, 0.5551, 0.5618, 0.5335, 0.5159, 0.5604, 0.5493,
        0.5276, 0.5203, 0.5319, 0.5597, 0.5378, 0.5468, 0.5548, 0.5235, 0.5435,
        0.5482, 0.5668, 0.5517, 0.5728, 0.5638, 0.5604, 0.5778, 0.5505, 0.5508,
        0.5337, 0.5537, 0.5425, 0.5169, 0.5600, 0.5514, 0.5833, 0.5479, 0.5521,
        0.5777, 0.5722, 0.5633, 0.5785, 0.5403, 0.5595, 0.5753, 0.5538, 0.5150,
        0.5628, 0.5204, 0.5648, 0.5593, 0.5580, 0.5167, 0.5717, 0.5582, 0.5512,
        0.5118, 0.5506, 0.5732, 0.5723, 0.5719, 0.5512, 0.5329, 0.5242, 0.5401,
        0.5175, 0.5572, 0.5849, 0.5465, 0.5274, 0.5292, 0.5123, 0.5673, 0.5694,
        0.5654, 0.5409, 0.5468, 0.5586, 0.5766, 0.5447, 0.5416, 0.5454, 0.5558,
        0.5380, 0.5575, 0.5778, 0.5536, 0.5685, 0.5147, 0.5728, 0.5498, 0.5689,
        0.5598, 0.5062, 0.5586, 0.5247, 0.5441, 0.5635, 0.5814, 0.5416, 0.5499,
        0.5323, 0.5409, 0.5455, 0.5595, 0.5447, 0.5427, 0.5701, 0.5770, 0.5634,
        0.5425, 0.5758, 0.5502, 0.5381, 0.5350, 0.5675, 0.5525, 0.5649, 0.5354,
        0.5581, 0.5779, 0.5480, 0.5751, 0.5488, 0.5865, 0.5395, 0.5428, 0.5728,
        0.5850, 0.5570, 0.5406, 0.5684, 0.5722, 0.5465, 0.5353, 0.5718, 0.5265],
       device='cuda:0', grad_fn=<SelectBackward0>)
tensor([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0,
        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,
        0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0,
        1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,
        0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0,
        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,
        1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,
        1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
        1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 1, 0], device='cuda:0')
[37mEpoch 0/3 [39m [35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m26/26[39m [37m0:00:31 â€¢ 0:00:00[39m [37m0.79it/s[39m [37mv_num: g0kd train/loss:     
                                                                        [37m0.690                       
[?25h
Error executing job with overrides: []
Traceback (most recent call last):
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 492, in main
    trainer.fit(model, data_module)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 989, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1035, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 203, in run
    self.on_advance_end()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 373, in on_advance_end
    call._call_lightning_module_hook(trainer, "on_train_epoch_end")
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 282, in on_train_epoch_end
    self.log(
TypeError: LightningModule.log() got an unexpected keyword argument 'commit'
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.