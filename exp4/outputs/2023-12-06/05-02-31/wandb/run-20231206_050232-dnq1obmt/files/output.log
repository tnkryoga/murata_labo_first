
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ[1m   [22mâ”ƒ[1m Name                        [22mâ”ƒ[1m Type             [22mâ”ƒ[1m Params [22mâ”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ bert                        â”‚ BertModel        â”‚ 89.1 M â”‚
â”‚ 1 â”‚ classifiers                 â”‚ ModuleList       â”‚ 12.6 M â”‚
â”‚ 2 â”‚ hidden_layer                â”‚ ModuleList       â”‚ 16.4 K â”‚
â”‚ 3 â”‚ sigmoid                     â”‚ Sigmoid          â”‚      0 â”‚
â”‚ 4 â”‚ criterion                   â”‚ BCELoss          â”‚      0 â”‚
â”‚ 5 â”‚ metrics                     â”‚ MetricCollection â”‚      0 â”‚
â”‚ 6 â”‚ metrics_per_label_accuracy  â”‚ MetricCollection â”‚      0 â”‚
â”‚ 7 â”‚ metrics_per_label_precision â”‚ MetricCollection â”‚      0 â”‚
â”‚ 8 â”‚ metrics_per_label_recall    â”‚ MetricCollection â”‚      0 â”‚
â”‚ 9 â”‚ metrics_per_label_f1score   â”‚ MetricCollection â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[22m: 19.7 M
[1mNon-trainable params[22m: 82.0 M
[1mTotal params[22m: 101 M
[1mTotal estimated model params size (MB)[22m: 406
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of
training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a
lower value for log_every_n_steps if you want to see logs for the training epoch.
[37mEpoch 0/3 [39m [35mâ”â”[90mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m2/26[39m [37m0:00:01 â€¢ 0:00:02[39m [37m23.28it/s[39m [37mv_num: obmt train/loss:     








                                                                        [37m0.693                       


[37mValidation[39m [35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[90mâ•ºâ”â”â”[39m [37m6/7  [39m [37m0:00:03 â€¢ 0:00:01[39m [37m1.44it/s
        0.5143, 0.5238, 0.5000, 0.5000, 0.5150, 0.5000, 0.5000, 0.5000, 0.5503,
        0.5257, 0.5000, 0.5200, 0.5017, 0.5000, 0.5126, 0.5000, 0.5080, 0.5000,
        0.5000, 0.5290, 0.5000, 0.5000, 0.5000, 0.5347, 0.5000, 0.5288, 0.5000,
        0.5300, 0.5000, 0.5000, 0.5055, 0.5059, 0.5000, 0.5200, 0.5250, 0.5031,
        0.5000, 0.5371, 0.5000, 0.5107, 0.5041, 0.5004, 0.5017, 0.5000, 0.5214,
        0.5016, 0.5027, 0.5000, 0.5072, 0.5017, 0.5000, 0.5016, 0.5055, 0.5000,
        0.5000, 0.5182, 0.5000, 0.5240, 0.5000, 0.5287, 0.5024, 0.5067, 0.5046,
        0.5000, 0.5187, 0.5321, 0.5068, 0.5109, 0.5000, 0.5245, 0.5000, 0.5000,
        0.5225, 0.5043, 0.5153, 0.5017, 0.5015, 0.5000, 0.5000, 0.5007, 0.5038,
        0.5063, 0.5059, 0.5000, 0.5252, 0.5198, 0.5152, 0.5144, 0.5180, 0.5000,
        0.5000, 0.5196, 0.5240, 0.5111, 0.5384, 0.5406, 0.5000, 0.5103, 0.5000,
        0.5033, 0.5126, 0.5202, 0.5071, 0.5178, 0.5000, 0.5283, 0.5154, 0.5000,
        0.5000, 0.5039, 0.5132, 0.5340, 0.5000, 0.5000, 0.5195, 0.5000, 0.5000,
        0.5410, 0.5211, 0.5000, 0.5134, 0.5002, 0.5092, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5014, 0.5174, 0.5000, 0.5000, 0.5198, 0.5041,
        0.5257, 0.5000, 0.5316, 0.5000, 0.5138, 0.5161, 0.5011, 0.5324, 0.5000,
        0.5153, 0.5185, 0.5019, 0.5000, 0.5087, 0.5081, 0.5000, 0.5073, 0.5000,
        0.5017, 0.5188, 0.5000, 0.5002, 0.5000, 0.5126, 0.5155, 0.5000, 0.5000,
        0.5000, 0.5006, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5065, 0.5038,
        0.5233, 0.5083, 0.5142, 0.5000, 0.5000, 0.5039, 0.5131, 0.5018, 0.5000,
        0.5032, 0.5000, 0.5166, 0.5209, 0.5274, 0.5000, 0.5000, 0.5000, 0.5025,
        0.5220, 0.5000, 0.5000, 0.5000, 0.5108, 0.5069, 0.5000, 0.5000, 0.5101,
        0.5000, 0.5022, 0.5200, 0.5031, 0.5000, 0.5009, 0.5268, 0.5000, 0.5012,
        0.5000, 0.5128, 0.5133, 0.5141, 0.5000, 0.5000, 0.5072, 0.5000, 0.5000,
        0.5000, 0.5019, 0.5000, 0.5000, 0.5220, 0.5092, 0.5000, 0.5012, 0.5132,
        0.5009, 0.5000, 0.5167, 0.5000, 0.5064, 0.5020, 0.5116, 0.5036, 0.5254,
        0.5000, 0.5218, 0.5000, 0.5022, 0.5000, 0.5000, 0.5165, 0.5000, 0.5061,
        0.5054, 0.5288, 0.5000, 0.5000, 0.5133, 0.5000, 0.5082, 0.5043, 0.5030,
        0.5047, 0.5267, 0.5000, 0.5000, 0.5185, 0.5000, 0.5116, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5408, 0.5172, 0.5000, 0.5000, 0.5000,
        0.5133, 0.5000, 0.5000, 0.5181, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5023, 0.5092, 0.5285, 0.5000, 0.5152, 0.5004, 0.5000, 0.5000, 0.5215,
        0.5000, 0.5000, 0.5000, 0.5208, 0.5070, 0.5026, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5274, 0.5054, 0.5000, 0.5142, 0.5000, 0.5001, 0.5032,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5035, 0.5000,
        0.5000, 0.5012, 0.5000, 0.5127, 0.5079, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5068, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5220, 0.5000,
        0.5000, 0.5048, 0.5149, 0.5006, 0.5000, 0.5081, 0.5000, 0.5059, 0.5000,
        0.5097, 0.5000, 0.5000, 0.5000, 0.5055, 0.5000, 0.5000, 0.5158, 0.5012,
        0.5000, 0.5000, 0.5000, 0.5051, 0.5000, 0.5007, 0.5000, 0.5262, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5045, 0.5045, 0.5000, 0.5212, 0.5000,
        0.5000, 0.5000, 0.5039, 0.5031, 0.5073, 0.5257, 0.5000, 0.5000, 0.5000,
        0.5048, 0.5000, 0.5000, 0.5000, 0.5000, 0.5133, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5012, 0.5000, 0.5068, 0.5000, 0.5117, 0.5007, 0.5000,
        0.5000, 0.5011, 0.5067, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5019, 0.5011, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5137, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5107, 0.5000, 0.5087, 0.5000, 0.5000, 0.5000, 0.5006,
        0.5000, 0.5229, 0.5000, 0.5054, 0.5000, 0.5000, 0.5226, 0.5041, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5077, 0.5000, 0.5000, 0.5182,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5039, 0.5000, 0.5000, 0.5198, 0.5000,
        0.5000, 0.5131, 0.5010, 0.5000, 0.5000, 0.5048, 0.5000, 0.5037, 0.5264,
        0.5156, 0.5077, 0.5082, 0.5012, 0.5000, 0.5000, 0.5000, 0.5000, 0.5053,
        0.5023, 0.5025, 0.5018, 0.5120, 0.5096, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5101, 0.5061, 0.5000, 0.5000, 0.5000,
        0.5174, 0.5000, 0.5000, 0.5000, 0.5030, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5091, 0.5028, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5123, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5006, 0.5000, 0.5003, 0.5000, 0.5000, 0.5098, 0.5000,
        0.5012, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5008, 0.5094,
        0.5041, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5036, 0.5000,
        0.5000, 0.5023, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5024, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5077, 0.5000, 0.5000, 0.5000, 0.5000, 0.5133, 0.5000,
        0.5000, 0.5083, 0.5000, 0.5000, 0.5011, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5042, 0.5000,
        0.5000, 0.5000, 0.5056, 0.5000, 0.5000, 0.5000, 0.5012, 0.5000, 0.5000,
        0.5066, 0.5000, 0.5000, 0.5027, 0.5000, 0.5000, 0.5114, 0.5000, 0.5000,
        0.5000, 0.5118, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5006, 0.5000, 0.5000, 0.5000, 0.5000, 0.5069,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5024, 0.5021, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5018, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5103, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5026, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5064, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5132, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5083, 0.5000, 0.5000, 0.5006, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5126, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5005, 0.5000, 0.5037, 0.5144, 0.5000,
        0.5020, 0.5000, 0.5098, 0.5000, 0.5000, 0.5022, 0.5000, 0.5000, 0.5000,
        0.5034, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5153, 0.5037,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5036, 0.5000, 0.5000, 0.5025, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5031, 0.5000, 0.5000, 0.5000, 0.5000, 0.5134, 0.5036, 0.5000,
        0.5056, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5101],
       device='cuda:0', grad_fn=<SelectBackward0>)
tensor([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,
        0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,
        0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,
        0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1,
        0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0,
        1, 0, 1], device='cuda:0')
[37mEpoch 0/3 [39m [35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m26/26[39m [37m0:00:27 â€¢ 0:00:00[39m [37m0.95it/s[39m [37mv_num: obmt train/loss:     
                                                                        [37m0.693                       
[?25h
Traceback (most recent call last):
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 515, in main
    trainer.fit(model, data_module)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 989, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1035, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 203, in run
    self.on_advance_end()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 373, in on_advance_end
    call._call_lightning_module_hook(trainer, "on_train_epoch_end")
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 297, in on_train_epoch_end
    metrics_per_label_precision = self.metrics_per_label_presicion(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1695, in __getattr__
    raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")
AttributeError: 'MaltiLabelClassifierModel' object has no attribute 'metrics_per_label_presicion'
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.