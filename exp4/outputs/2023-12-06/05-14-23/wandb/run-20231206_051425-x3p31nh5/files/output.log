
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ[1m   [22mâ”ƒ[1m Name                        [22mâ”ƒ[1m Type             [22mâ”ƒ[1m Params [22mâ”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ bert                        â”‚ BertModel        â”‚ 89.1 M â”‚
â”‚ 1 â”‚ classifiers                 â”‚ ModuleList       â”‚ 12.6 M â”‚
â”‚ 2 â”‚ hidden_layer                â”‚ ModuleList       â”‚ 16.4 K â”‚
â”‚ 3 â”‚ sigmoid                     â”‚ Sigmoid          â”‚      0 â”‚
â”‚ 4 â”‚ criterion                   â”‚ BCELoss          â”‚      0 â”‚
â”‚ 5 â”‚ metrics                     â”‚ MetricCollection â”‚      0 â”‚
â”‚ 6 â”‚ metrics_per_label_accuracy  â”‚ MetricCollection â”‚      0 â”‚
â”‚ 7 â”‚ metrics_per_label_precision â”‚ MetricCollection â”‚      0 â”‚
â”‚ 8 â”‚ metrics_per_label_recall    â”‚ MetricCollection â”‚      0 â”‚
â”‚ 9 â”‚ metrics_per_label_f1score   â”‚ MetricCollection â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[22m: 19.7 M
[1mNon-trainable params[22m: 82.0 M
[1mTotal params[22m: 101 M
[1mTotal estimated model params size (MB)[22m: 406
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:293: The number of
training batches (26) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a
lower value for log_every_n_steps if you want to see logs for the training epoch.
[37mEpoch 0/3 [39m [35mâ”â”â”[90mâ•ºâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m3/26[39m [37m0:00:02 â€¢ 0:00:14[39m [37m1.76it/s[39m [37mv_num: 1nh5 train/loss:     









                                                                        [37m0.692                       


        0.5031, 0.5140, 0.5000, 0.5284, 0.5000, 0.5011, 0.5389, 0.5218, 0.5190,
        0.5000, 0.5194, 0.5000, 0.5079, 0.5000, 0.5247, 0.5025, 0.5000, 0.5000,
        0.5000, 0.5060, 0.5031, 0.5000, 0.5266, 0.5022, 0.5156, 0.5268, 0.5000,
        0.5000, 0.5055, 0.5401, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5147, 0.5000, 0.5000, 0.5000, 0.5201, 0.5000, 0.5000,
        0.5186, 0.5107, 0.5134, 0.5000, 0.5000, 0.5000, 0.5075, 0.5177, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5091, 0.5000, 0.5000, 0.5025,
        0.5000, 0.5000, 0.5245, 0.5000, 0.5038, 0.5102, 0.5127, 0.5000, 0.5068,
        0.5074, 0.5423, 0.5000, 0.5000, 0.5000, 0.5182, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5135, 0.5402, 0.5000,
        0.5000, 0.5126, 0.5055, 0.5000, 0.5000, 0.5000, 0.5450, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5112, 0.5000, 0.5000, 0.5000, 0.5067, 0.5000, 0.5000,
        0.5278, 0.5042, 0.5072, 0.5000, 0.5049, 0.5000, 0.5159, 0.5025, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5030, 0.5000, 0.5000,
        0.5104, 0.5090, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5150, 0.5045, 0.5000, 0.5000, 0.5035, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5054, 0.5000,
        0.5095, 0.5000, 0.5223, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5189,
        0.5000, 0.5000, 0.5000, 0.5043, 0.5000, 0.5000, 0.5057, 0.5000, 0.5000,
        0.5000, 0.5047, 0.5205, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5275,
        0.5339, 0.5000, 0.5229, 0.5000, 0.5000, 0.5070, 0.5000, 0.5000, 0.5194,
        0.5000, 0.5000, 0.5029, 0.5490, 0.5000, 0.5000, 0.5000, 0.5000, 0.5434,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5023, 0.5000,
        0.5000, 0.5000, 0.5463, 0.5000, 0.5068, 0.5000, 0.5000, 0.5149, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5017, 0.5000, 0.5103, 0.5169, 0.5467, 0.5000,
        0.5000, 0.5126, 0.5000, 0.5125, 0.5147, 0.5000, 0.5000, 0.5000, 0.5145,
        0.5355, 0.5000, 0.5248, 0.5000, 0.5000, 0.5378, 0.5001, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5009, 0.5000, 0.5000,
        0.5061, 0.5000, 0.5064, 0.5000, 0.5033, 0.5086, 0.5000, 0.5000, 0.5040,
        0.5000, 0.5032, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5357, 0.5000,
        0.5000, 0.5009, 0.5187, 0.5000, 0.5000, 0.5000, 0.5000, 0.5171, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5032, 0.5000, 0.5000, 0.5175, 0.5000, 0.5000,
        0.5050, 0.5000, 0.5264, 0.5000, 0.5292, 0.5036, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5284, 0.5000, 0.5000, 0.5000, 0.5041, 0.5000, 0.5239, 0.5165,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5072, 0.5039, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5033, 0.5006, 0.5150, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5168, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5084,
        0.5000, 0.5000, 0.5041, 0.5000, 0.5000, 0.5351, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5064, 0.5000, 0.5020, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5009, 0.5000, 0.5000, 0.5000, 0.5197, 0.5067,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5212, 0.5000, 0.5189, 0.5000, 0.5000,
        0.5000, 0.5095, 0.5190, 0.5000, 0.5000, 0.5000, 0.5135, 0.5003, 0.5376,
        0.5033, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5434, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5162, 0.5000, 0.5000, 0.5000,
        0.5127, 0.5059, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5268,
        0.5000, 0.5000, 0.5000, 0.5049, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5186, 0.5000, 0.5000, 0.5159, 0.5264, 0.5000, 0.5336, 0.5000, 0.5081,
        0.5212, 0.5000, 0.5054, 0.5000, 0.5000, 0.5000, 0.5198, 0.5050, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5024, 0.5208,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5011, 0.5175, 0.5163, 0.5111, 0.5000,
        0.5000, 0.5261, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5306, 0.5000, 0.5000, 0.5000, 0.5032, 0.5000, 0.5089, 0.5144,
        0.5080, 0.5205, 0.5334, 0.5181, 0.5099, 0.5000, 0.5000, 0.5063, 0.5000,
        0.5009, 0.5600, 0.5000, 0.5376, 0.5000, 0.5000, 0.5000, 0.5000, 0.5176,
        0.5000, 0.5000, 0.5307, 0.5000, 0.5000, 0.5218, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5069, 0.5108, 0.5000, 0.5000, 0.5000, 0.5000, 0.5061, 0.5440,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5063, 0.5220,
        0.5011, 0.5000, 0.5000, 0.5296, 0.5028, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5553, 0.5120, 0.5194, 0.5524, 0.5000, 0.5211, 0.5235, 0.5000, 0.5002,
        0.5415, 0.5005, 0.5000, 0.5237, 0.5000, 0.5002, 0.5000, 0.5000, 0.5382,
        0.5180, 0.5000, 0.5132, 0.5297, 0.5000, 0.5119, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5034, 0.5220, 0.5000, 0.5013, 0.5286, 0.5000, 0.5000, 0.5000,
        0.5190, 0.5000, 0.5091, 0.5000, 0.5051, 0.5000, 0.5000, 0.5154, 0.5000,
        0.5171, 0.5000, 0.5000, 0.5000, 0.5001, 0.5000, 0.5136, 0.5000, 0.5061,
        0.5000, 0.5031, 0.5169, 0.5167, 0.5186, 0.5000, 0.5000, 0.5011, 0.5000,
        0.5031, 0.5000, 0.5000, 0.5155, 0.5131, 0.5496, 0.5090, 0.5000, 0.5097,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5487, 0.5000, 0.5062, 0.5000, 0.5000,
        0.5000, 0.5550, 0.5053, 0.5407, 0.5000, 0.5000, 0.5198, 0.5093, 0.5101,
        0.5334, 0.5000, 0.5000, 0.5000, 0.5117, 0.5099, 0.5076, 0.5204, 0.5000,
        0.5175, 0.5000, 0.5296, 0.5000, 0.5000, 0.5000, 0.5162, 0.5000, 0.5092,
        0.5161, 0.5052, 0.5000, 0.5116, 0.5103, 0.5000, 0.5326, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5054, 0.5137, 0.5505, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5117, 0.5000, 0.5049, 0.5052, 0.5024, 0.5000, 0.5219, 0.5000, 0.5000,
        0.5000, 0.5061, 0.5291, 0.5019, 0.5358, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5022, 0.5445, 0.5082, 0.5098, 0.5345, 0.5048, 0.5145, 0.5120, 0.5000,
        0.5471, 0.5563, 0.5052, 0.5000, 0.5134, 0.5011, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5026, 0.5073, 0.5066, 0.5000, 0.5098, 0.5000, 0.5024, 0.5000,
        0.5000, 0.5056, 0.5115, 0.5055, 0.5494, 0.5000, 0.5152, 0.5124, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5003, 0.5000, 0.5284, 0.5139, 0.5000, 0.5066,
        0.5000, 0.5243, 0.5000, 0.5163, 0.5238, 0.5000, 0.5211, 0.5000, 0.5000,
        0.5000, 0.5225, 0.5000, 0.5325, 0.5063, 0.5261, 0.5135, 0.5474, 0.5000,
        0.5168, 0.5000, 0.5290, 0.5000, 0.5000, 0.5000, 0.5019, 0.5000, 0.5160,
        0.5114, 0.5247, 0.5025, 0.5000, 0.5180, 0.5000, 0.5000, 0.5256, 0.5000,
        0.5028, 0.5033, 0.5000, 0.5000, 0.5000, 0.5000, 0.5150, 0.5000, 0.5000,
        0.5066, 0.5294, 0.5171, 0.5076, 0.5000, 0.5000, 0.5144, 0.5105, 0.5215,
        0.5525, 0.5319, 0.5080, 0.5233, 0.5060, 0.5379, 0.5000, 0.5166, 0.5000,
        0.5031, 0.5680, 0.5196, 0.5000, 0.5107, 0.5000, 0.5129, 0.5215, 0.5000],
       device='cuda:0', grad_fn=<SelectBackward0>)
tensor([1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,
        1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,
        0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1,
        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,
        1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,
        0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,
        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,
        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,
        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,
        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1,
        0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1,
        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1,
        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
        1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,
        1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1,
        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,
        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,
        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,
        1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,
        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1], device='cuda:0')
[37mEpoch 0/3 [39m [35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m26/26[39m [37m0:00:27 â€¢ 0:00:00[39m [37m0.95it/s[39m [37mv_num: 1nh5 train/loss:     
                                                                        [37m0.692                       
[?25h
Error executing job with overrides: []
Traceback (most recent call last):
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 515, in main
    trainer.fit(model, data_module)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 989, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1035, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 203, in run
    self.on_advance_end()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py", line 373, in on_advance_end
    call._call_lightning_module_hook(trainer, "on_train_epoch_end")
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 318, in on_train_epoch_end
    metrics_per_label_recall[f"recall_label_{i}"].item(),
KeyError: 'recall_label_0'
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.