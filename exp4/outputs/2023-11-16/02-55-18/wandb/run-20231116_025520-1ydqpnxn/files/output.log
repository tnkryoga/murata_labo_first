
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃[1m   [22m┃[1m Name        [22m┃[1m Type             [22m┃[1m Params [22m┃
┡━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ bert        │ BertModel        │ 89.1 M │
│ 1 │ classifiers │ ModuleList       │ 12.3 K │
│ 2 │ sigmoid     │ Sigmoid          │      0 │
│ 3 │ criterion   │ BCELoss          │      0 │
│ 4 │ metrics     │ MetricCollection │      0 │
└───┴─────────────┴──────────────────┴────────┘
[1mTrainable params[22m: 7.1 M
[1mNon-trainable params[22m: 82.0 M
[1mTotal params[22m: 89.1 M
[1mTotal estimated model params size (MB)[22m: 356
tensor([0.5287, 0.5445, 0.5000, 0.5000, 0.5000, 0.5000, 0.6154, 0.5000, 0.5056,
        0.5484, 0.5187, 0.5023, 0.5247, 0.5000, 0.5504, 0.5000, 0.5802, 0.5176,
        0.5000, 0.5000, 0.5000, 0.5977, 0.6332, 0.5000, 0.5000, 0.5364, 0.5921,
        0.5000, 0.5256, 0.5000, 0.5000, 0.5000, 0.5675, 0.5117, 0.5000, 0.5000,
        0.5000, 0.6145, 0.6521, 0.5000, 0.5248, 0.6172, 0.5735, 0.5000, 0.5813,
        0.5000, 0.5019, 0.5000, 0.5499, 0.5343, 0.5000, 0.5000, 0.5000, 0.6229,
        0.6659, 0.5000, 0.5000, 0.5727, 0.6051, 0.5189, 0.5097, 0.5000, 0.5069,
        0.5000, 0.5533, 0.5000, 0.5000, 0.5000, 0.5000, 0.5139, 0.6578, 0.5000,
        0.5000, 0.5467, 0.6126, 0.5270, 0.5205, 0.5000, 0.5915, 0.5000, 0.5225,
        0.5611, 0.5099, 0.5000, 0.5000, 0.5020, 0.6348, 0.5000, 0.5000, 0.5000,
        0.5209, 0.5000, 0.5529, 0.5000, 0.5709, 0.5000, 0.5567, 0.5299, 0.5000,
        0.5000, 0.5000, 0.6258, 0.6501, 0.5000, 0.5000, 0.5367, 0.5930, 0.5000,
        0.5577, 0.5000, 0.5000, 0.5000, 0.5324, 0.5437, 0.5000, 0.5000, 0.5000,
        0.5000, 0.6228, 0.5084, 0.5068, 0.5350, 0.5174, 0.5000, 0.5678, 0.5000,
        0.6215, 0.5000, 0.5222, 0.5000, 0.5000, 0.5000, 0.5000, 0.6840, 0.6773,
        0.5000, 0.5000, 0.6418, 0.5000, 0.5205, 0.5615, 0.5000, 0.5000, 0.5000,
        0.5077, 0.5185, 0.5000, 0.5000, 0.5000, 0.5000, 0.6635, 0.5000, 0.5000,
        0.6273, 0.5690, 0.5000, 0.5585, 0.5000, 0.6288, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.6192, 0.5087, 0.5000, 0.5944, 0.5399,
        0.5000, 0.5930, 0.5000, 0.6200, 0.5000, 0.5639, 0.5540, 0.5000, 0.5000,
        0.5000, 0.5000, 0.6111, 0.5000, 0.5424, 0.5000, 0.5051, 0.5017, 0.5499,
        0.5000, 0.5770, 0.5000, 0.5368, 0.5000, 0.5000, 0.5000, 0.5000, 0.5137,
        0.5558, 0.5240, 0.5000, 0.5000, 0.5986, 0.5918, 0.5635, 0.5000, 0.5895,
        0.5000, 0.5250, 0.5000, 0.5329, 0.5000, 0.5000, 0.6086, 0.6330, 0.5079,
        0.6143, 0.6351, 0.5144, 0.5541, 0.5479, 0.5000, 0.5142, 0.5000, 0.5557,
        0.5057, 0.5313, 0.5000, 0.5000, 0.6055, 0.6468, 0.5000, 0.5532, 0.5476,
        0.5498, 0.5435, 0.5032, 0.5000, 0.5235, 0.5000, 0.5415, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5268, 0.5992, 0.5000, 0.5000, 0.5584, 0.6081, 0.5223,
        0.5754, 0.5000, 0.5633, 0.5000, 0.5000, 0.5587, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5358, 0.5000, 0.5000, 0.5150, 0.5000, 0.5072, 0.5683, 0.5000,
        0.6318, 0.5000, 0.5000, 0.5010, 0.5000, 0.5000, 0.5000, 0.5424, 0.6231,
        0.5346, 0.5000, 0.5775, 0.6127, 0.5306, 0.5404, 0.5000, 0.5180, 0.5000,
        0.6150, 0.5335, 0.5417, 0.5000, 0.5000, 0.5929, 0.7016, 0.5000, 0.5603,
        0.5612, 0.5964, 0.5000, 0.5610, 0.5000, 0.5000, 0.5000, 0.6113, 0.5000,
        0.5282, 0.5000, 0.5000, 0.5784, 0.6731, 0.5000, 0.5133, 0.5305, 0.5619,
        0.5257, 0.5346, 0.5000, 0.5416, 0.5000, 0.5103, 0.5303, 0.5000, 0.5000,
        0.5000, 0.5009, 0.6317, 0.5000, 0.5000, 0.5627, 0.5923, 0.5000, 0.5681,
        0.5000, 0.5773, 0.5000, 0.5216, 0.5608, 0.5000, 0.5000, 0.5000, 0.5000,
        0.6216, 0.5000, 0.5222, 0.5137, 0.5000, 0.5000, 0.5122, 0.5000, 0.6368,
        0.5000, 0.5213, 0.5000, 0.5000, 0.5000, 0.5000, 0.5077, 0.6293, 0.5000,
        0.5000, 0.5474, 0.5581, 0.5437, 0.5558, 0.5000, 0.5570, 0.5000, 0.5000,
        0.6151, 0.5078, 0.5000, 0.5000, 0.5000, 0.5646, 0.5000, 0.5688, 0.5061,
        0.5464, 0.5000, 0.5435, 0.5000, 0.6322, 0.5000, 0.5000, 0.5556, 0.5000,
        0.5000, 0.5000, 0.5000, 0.6170, 0.5032, 0.5084, 0.5095, 0.5457, 0.5000,
        0.5534, 0.5000, 0.5994, 0.5000, 0.6095, 0.5123, 0.5832, 0.5000, 0.5000,
        0.5520, 0.6145, 0.5000, 0.5177, 0.5359, 0.5343, 0.5120, 0.6460, 0.5000,
        0.5475, 0.5000, 0.5234, 0.5800, 0.5000, 0.5000, 0.5000, 0.5000, 0.5710,
        0.5000, 0.5337, 0.5000, 0.5296, 0.5000, 0.5676, 0.5000, 0.6580, 0.5000,
        0.5032, 0.5362, 0.5000, 0.5000, 0.5000, 0.5372, 0.5761, 0.5403, 0.5219,
        0.5478, 0.6678, 0.5426, 0.5518, 0.5000, 0.5360, 0.5221, 0.5985, 0.5544,
        0.5000, 0.5000, 0.5000, 0.5518, 0.7054, 0.5000, 0.5000, 0.6224, 0.5711,
        0.5000, 0.5343, 0.5000, 0.5150, 0.5000, 0.5265, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5929, 0.7019, 0.5000, 0.5000, 0.5098, 0.5838, 0.5261, 0.5060,
        0.5000, 0.5000, 0.5000, 0.5519, 0.5486, 0.5448, 0.5000, 0.5000, 0.6090,
        0.6237, 0.5000, 0.5005, 0.5117, 0.5944, 0.5000, 0.5894, 0.5000, 0.5138,
        0.5000, 0.5278, 0.5367, 0.5054, 0.5000, 0.5000, 0.5703, 0.6337, 0.5000,
        0.5000, 0.5362, 0.5450, 0.5000, 0.5649, 0.5000, 0.5691, 0.5000],
       device='cuda:0')
[?25h
Error executing job with overrides: []
Traceback (most recent call last):
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 426, in main
    trainer.fit(model, data_module)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 989, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
    self._run_sanity_check()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1062, in _run_sanity_check
    val_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py", line 134, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py", line 391, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py", line 403, in validation_step
    return self.lightning_module.validation_step(*args, **kwargs)
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 208, in validation_step
    loss, preds = self.forward(
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp4/main.py", line 188, in forward
    loss = self.criterion(
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py", line 618, in forward
    return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)
  File "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py", line 3113, in binary_cross_entropy
    raise ValueError(
ValueError: Using a target size (torch.Size([32, 16])) that is different to the input size (torch.Size([512])) is deprecated. Please ensure they have the same size.
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.