{"val_loss": 0.6927298307418823, "val/multilabelaccuracy": 0.4711538553237915, "val/multilabelprecision": 0.08138173073530197, "val/multilabelrecall": 0.1265285313129425, "val/multilabelf1score": 0.09166666865348816, "val/multilabelmatthewscorrcoef": -0.04748711735010147, "val/accuracy_label_\u3042\u3044\u3065\u3061": 0.07692307978868484, "val/presicion_label_\u3042\u3044\u3065\u3061": 0.0, "val/recall_label_\u3042\u3044\u3065\u3061": 0.0, "val/f1score_label_\u3042\u3044\u3065\u3061": 0.0, "val/accuracy_label_\u611f\u5fc3": 0.13846154510974884, "val/presicion_label_\u611f\u5fc3": 0.0, "val/recall_label_\u611f\u5fc3": 0.0, "val/f1score_label_\u611f\u5fc3": 0.0, "val/accuracy_label_\u8a55\u4fa1": 0.5692307949066162, "val/presicion_label_\u8a55\u4fa1": 0.1428571492433548, "val/recall_label_\u8a55\u4fa1": 0.043478261679410934, "val/f1score_label_\u8a55\u4fa1": 0.06666667014360428, "val/accuracy_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.4000000059604645, "val/presicion_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/recall_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/f1score_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/accuracy_label_\u540c\u610f": 0.6153846383094788, "val/presicion_label_\u540c\u610f": 0.0, "val/recall_label_\u540c\u610f": 0.0, "val/f1score_label_\u540c\u610f": 0.0, "val/accuracy_label_\u7d0d\u5f97": 0.6461538672447205, "val/presicion_label_\u7d0d\u5f97": 0.0, "val/recall_label_\u7d0d\u5f97": 0.0, "val/f1score_label_\u7d0d\u5f97": 0.0, "val/accuracy_label_\u9a5a\u304d": 0.800000011920929, "val/presicion_label_\u9a5a\u304d": 0.0, "val/recall_label_\u9a5a\u304d": 0.0, "val/f1score_label_\u9a5a\u304d": 0.0, "val/accuracy_label_\u305d\u306e\u4ed6": 0.5230769515037537, "val/presicion_label_\u305d\u306e\u4ed6": 0.5081967115402222, "val/recall_label_\u305d\u306e\u4ed6": 0.96875, "val/f1score_label_\u305d\u306e\u4ed6": 0.6666666865348816, "epoch": 4, "trainer/global_step": 12, "_timestamp": 1704874297.1138783, "_runtime": 33.20812916755676, "_step": 8, "train_loss": 0.6923040747642517, "train/multilabelaccuracy": 0.4751945734024048, "train/multilabelprecision": 0.1497996747493744, "train/multilabelrecall": 0.13953399658203125, "train/multilabelf1score": 0.11391299962997437, "train/multilabelmatthewscorrcoef": -0.038222458213567734, "train/accuracy_label_\u3042\u3044\u3065\u3061": 0.04280155524611473, "train/presicion_label_\u3042\u3044\u3065\u3061": 0.0, "train/recall_label_\u3042\u3044\u3065\u3061": 0.0, "train/f1score_label_\u3042\u3044\u3065\u3061": 0.0, "train/accuracy_label_\u611f\u5fc3": 0.1284046620130539, "train/presicion_label_\u611f\u5fc3": 0.0, "train/recall_label_\u611f\u5fc3": 0.0, "train/f1score_label_\u611f\u5fc3": 0.0, "train/accuracy_label_\u8a55\u4fa1": 0.5680933594703674, "train/presicion_label_\u8a55\u4fa1": 0.26923078298568726, "train/recall_label_\u8a55\u4fa1": 0.07070706784725189, "train/f1score_label_\u8a55\u4fa1": 0.1120000034570694, "train/accuracy_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.3813229501247406, "train/presicion_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "train/recall_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "train/f1score_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "train/accuracy_label_\u540c\u610f": 0.7003890872001648, "train/presicion_label_\u540c\u610f": 0.0, "train/recall_label_\u540c\u610f": 0.0, "train/f1score_label_\u540c\u610f": 0.0, "train/accuracy_label_\u7d0d\u5f97": 0.6070038676261902, "train/presicion_label_\u7d0d\u5f97": 0.1666666716337204, "train/recall_label_\u7d0d\u5f97": 0.02150537632405758, "train/f1score_label_\u7d0d\u5f97": 0.03809523954987526, "train/accuracy_label_\u9a5a\u304d": 0.844357967376709, "train/presicion_label_\u9a5a\u304d": 0.25, "train/recall_label_\u9a5a\u304d": 0.0555555559694767, "train/f1score_label_\u9a5a\u304d": 0.09090909361839294, "train/accuracy_label_\u305d\u306e\u4ed6": 0.5291828513145447, "train/presicion_label_\u305d\u306e\u4ed6": 0.512499988079071, "train/recall_label_\u305d\u306e\u4ed6": 0.9685039520263672, "train/f1score_label_\u305d\u306e\u4ed6": 0.6702997088432312, "test_loss": 0.6920669674873352, "test/multilabelaccuracy": 0.4825310707092285, "test/multilabelprecision": 0.1535547822713852, "test/multilabelrecall": 0.13610228896141052, "test/multilabelf1score": 0.11225461959838867, "test/multilabelmatthewscorrcoef": -0.016147634014487267, "_wandb": {"runtime": 33}}