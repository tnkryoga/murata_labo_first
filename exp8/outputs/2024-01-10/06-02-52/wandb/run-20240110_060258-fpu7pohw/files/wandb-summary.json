{"train/loss": 0.6823538541793823, "epoch": 4, "trainer/global_step": 468, "_timestamp": 1704867426.058656, "_runtime": 847.914293050766, "_step": 17, "val_loss": 0.6848755478858948, "val/multilabelaccuracy": 0.8837530016899109, "val/multilabelprecision": 0.08388545364141464, "val/multilabelrecall": 0.1180095300078392, "val/multilabelf1score": 0.09806368499994278, "val/multilabelmatthewscorrcoef": 0.51146000623703, "val/accuracy_label_\u3042\u3044\u3065\u3061": 0.6628063321113586, "val/presicion_label_\u3042\u3044\u3065\u3061": 0.6710836291313171, "val/recall_label_\u3042\u3044\u3065\u3061": 0.9440762400627136, "val/f1score_label_\u3042\u3044\u3065\u3061": 0.7845094799995422, "val/accuracy_label_\u611f\u5fc3": 0.7007810473442078, "val/presicion_label_\u611f\u5fc3": 0.0, "val/recall_label_\u611f\u5fc3": 0.0, "val/f1score_label_\u611f\u5fc3": 0.0, "val/accuracy_label_\u8a55\u4fa1": 0.957177460193634, "val/presicion_label_\u8a55\u4fa1": 0.0, "val/recall_label_\u8a55\u4fa1": 0.0, "val/f1score_label_\u8a55\u4fa1": 0.0, "val/accuracy_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.9240506291389465, "val/presicion_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/recall_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/f1score_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/accuracy_label_\u540c\u610f": 0.9539455771446228, "val/presicion_label_\u540c\u610f": 0.0, "val/recall_label_\u540c\u610f": 0.0, "val/f1score_label_\u540c\u610f": 0.0, "val/accuracy_label_\u7d0d\u5f97": 0.9711823463439941, "val/presicion_label_\u7d0d\u5f97": 0.0, "val/recall_label_\u7d0d\u5f97": 0.0, "val/f1score_label_\u7d0d\u5f97": 0.0, "val/accuracy_label_\u9a5a\u304d": 0.9709129929542542, "val/presicion_label_\u9a5a\u304d": 0.0, "val/recall_label_\u9a5a\u304d": 0.0, "val/f1score_label_\u9a5a\u304d": 0.0, "val/accuracy_label_\u305d\u306e\u4ed6": 0.9291678071022034, "val/presicion_label_\u305d\u306e\u4ed6": 0.0, "val/recall_label_\u305d\u306e\u4ed6": 0.0, "val/f1score_label_\u305d\u306e\u4ed6": 0.0, "train_loss": 0.6850705742835999, "train/multilabelaccuracy": 0.8794107437133789, "train/multilabelprecision": 0.11332999169826508, "train/multilabelrecall": 0.11685189604759216, "train/multilabelf1score": 0.1035398542881012, "train/multilabelmatthewscorrcoef": 0.49069744348526, "train/accuracy_label_\u3042\u3044\u3065\u3061": 0.6619528532028198, "train/presicion_label_\u3042\u3044\u3065\u3061": 0.6808294057846069, "train/recall_label_\u3042\u3044\u3065\u3061": 0.9024061560630798, "train/f1score_label_\u3042\u3044\u3065\u3061": 0.7761127352714539, "train/accuracy_label_\u611f\u5fc3": 0.7006060481071472, "train/presicion_label_\u611f\u5fc3": 0.07999999821186066, "train/recall_label_\u611f\u5fc3": 0.0004519773938227445, "train/f1score_label_\u611f\u5fc3": 0.0008988764020614326, "train/accuracy_label_\u8a55\u4fa1": 0.9550168514251709, "train/presicion_label_\u8a55\u4fa1": 0.02777777798473835, "train/recall_label_\u8a55\u4fa1": 0.0015772870974615216, "train/f1score_label_\u8a55\u4fa1": 0.0029850746504962444, "train/accuracy_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.9237710237503052, "train/presicion_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "train/recall_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "train/f1score_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "train/accuracy_label_\u540c\u610f": 0.9450504779815674, "train/presicion_label_\u540c\u610f": 0.0, "train/recall_label_\u540c\u610f": 0.0, "train/f1score_label_\u540c\u610f": 0.0, "train/accuracy_label_\u7d0d\u5f97": 0.9703030586242676, "train/presicion_label_\u7d0d\u5f97": 0.0, "train/recall_label_\u7d0d\u5f97": 0.0, "train/f1score_label_\u7d0d\u5f97": 0.0, "train/accuracy_label_\u9a5a\u304d": 0.9740740656852722, "train/presicion_label_\u9a5a\u304d": 0.0, "train/recall_label_\u9a5a\u304d": 0.0, "train/f1score_label_\u9a5a\u304d": 0.0, "train/accuracy_label_\u305d\u306e\u4ed6": 0.9045118093490601, "train/presicion_label_\u305d\u306e\u4ed6": 0.1180327832698822, "train/recall_label_\u305d\u306e\u4ed6": 0.030379746109247208, "train/f1score_label_\u305d\u306e\u4ed6": 0.048322148621082306, "test_loss": 0.6864821314811707, "test/multilabelaccuracy": 0.8837107419967651, "test/multilabelprecision": 0.08099924027919769, "test/multilabelrecall": 0.11886247247457504, "test/multilabelf1score": 0.09634432196617126, "test/multilabelmatthewscorrcoef": 0.5032587647438049, "_wandb": {"runtime": 847}}