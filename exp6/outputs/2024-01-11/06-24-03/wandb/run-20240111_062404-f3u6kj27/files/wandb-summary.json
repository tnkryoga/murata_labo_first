{"train/loss": 0.7433124780654907, "epoch": 4, "trainer/global_step": 2184, "_timestamp": 1704955206.9412389, "_runtime": 962.5000548362732, "_step": 51, "val_loss": 0.7569906711578369, "val/multilabelaccuracy": 0.9388879537582397, "val/multilabelprecision": 0.07013069838285446, "val/multilabelrecall": 0.0974554717540741, "val/multilabelf1score": 0.08119150996208191, "val/multilabelmatthewscorrcoef": 0.5905476212501526, "val/accuracy_label_\u3042\u3044\u3065\u3061": 0.6446886658668518, "val/presicion_label_\u3042\u3044\u3065\u3061": 0.6440872550010681, "val/recall_label_\u3042\u3044\u3065\u3061": 0.999287486076355, "val/f1score_label_\u3042\u3044\u3065\u3061": 0.7833007574081421, "val/accuracy_label_\u611f\u5fc3": 0.6870421171188354, "val/presicion_label_\u611f\u5fc3": 0.47800394892692566, "val/recall_label_\u611f\u5fc3": 0.5600000023841858, "val/f1score_label_\u611f\u5fc3": 0.5157634019851685, "val/accuracy_label_\u8a55\u4fa1": 0.9555860757827759, "val/presicion_label_\u8a55\u4fa1": 0.0, "val/recall_label_\u8a55\u4fa1": 0.0, "val/f1score_label_\u8a55\u4fa1": 0.0, "val/accuracy_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.9175823926925659, "val/presicion_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/recall_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/f1score_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/accuracy_label_\u540c\u610f": 0.9558150172233582, "val/presicion_label_\u540c\u610f": 0.0, "val/recall_label_\u540c\u610f": 0.0, "val/f1score_label_\u540c\u610f": 0.0, "val/accuracy_label_\u7d0d\u5f97": 0.9686355590820312, "val/presicion_label_\u7d0d\u5f97": 0.0, "val/recall_label_\u7d0d\u5f97": 0.0, "val/f1score_label_\u7d0d\u5f97": 0.0, "val/accuracy_label_\u9a5a\u304d": 0.9736721515655518, "val/presicion_label_\u9a5a\u304d": 0.0, "val/recall_label_\u9a5a\u304d": 0.0, "val/f1score_label_\u9a5a\u304d": 0.0, "val/accuracy_label_\u8a00\u3044\u63db\u3048": 1.0, "val/presicion_label_\u8a00\u3044\u63db\u3048": 0.0, "val/recall_label_\u8a00\u3044\u63db\u3048": 0.0, "val/f1score_label_\u8a00\u3044\u63db\u3048": 0.0, "val/accuracy_label_\u610f\u898b": 0.9913003444671631, "val/presicion_label_\u610f\u898b": 0.0, "val/recall_label_\u610f\u898b": 0.0, "val/f1score_label_\u610f\u898b": 0.0, "val/accuracy_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.997710645198822, "val/presicion_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.0, "val/recall_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.0, "val/f1score_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.0, "val/accuracy_label_\u4e0d\u540c\u610f": 0.9926739931106567, "val/presicion_label_\u4e0d\u540c\u610f": 0.0, "val/recall_label_\u4e0d\u540c\u610f": 0.0, "val/f1score_label_\u4e0d\u540c\u610f": 0.0, "val/accuracy_label_\u88dc\u5b8c": 0.9919871687889099, "val/presicion_label_\u88dc\u5b8c": 0.0, "val/recall_label_\u88dc\u5b8c": 0.0, "val/f1score_label_\u88dc\u5b8c": 0.0, "val/accuracy_label_\u3042\u3044\u3055\u3064": 0.997710645198822, "val/presicion_label_\u3042\u3044\u3055\u3064": 0.0, "val/recall_label_\u3042\u3044\u3055\u3064": 0.0, "val/f1score_label_\u3042\u3044\u3055\u3064": 0.0, "val/accuracy_label_\u60f3\u8d77": 0.9988552927970886, "val/presicion_label_\u60f3\u8d77": 0.0, "val/recall_label_\u60f3\u8d77": 0.0, "val/f1score_label_\u60f3\u8d77": 0.0, "val/accuracy_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.9990842342376709, "val/presicion_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "val/recall_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "val/f1score_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "val/accuracy_label_\u305d\u306e\u4ed6": 0.9498626589775085, "val/presicion_label_\u305d\u306e\u4ed6": 0.0, "val/recall_label_\u305d\u306e\u4ed6": 0.0, "val/f1score_label_\u305d\u306e\u4ed6": 0.0, "train_loss": 0.7569818496704102, "train/multilabelaccuracy": 0.9319840669631958, "train/multilabelprecision": 0.11751426011323929, "train/multilabelrecall": 0.12499332427978516, "train/multilabelf1score": 0.10941437631845474, "train/multilabelmatthewscorrcoef": 0.5702223777770996, "train/accuracy_label_\u3042\u3044\u3065\u3061": 0.6483516693115234, "train/presicion_label_\u3042\u3044\u3065\u3061": 0.6482363343238831, "train/recall_label_\u3042\u3044\u3065\u3061": 0.999027669429779, "train/f1score_label_\u3042\u3044\u3065\u3061": 0.7862808108329773, "train/accuracy_label_\u611f\u5fc3": 0.6990041136741638, "train/presicion_label_\u611f\u5fc3": 0.4930255711078644, "train/recall_label_\u611f\u5fc3": 0.5737197995185852, "train/f1score_label_\u611f\u5fc3": 0.5303206443786621, "train/accuracy_label_\u8a55\u4fa1": 0.9484317898750305, "train/presicion_label_\u8a55\u4fa1": 0.13333334028720856, "train/recall_label_\u8a55\u4fa1": 0.0476190485060215, "train/f1score_label_\u8a55\u4fa1": 0.07017543911933899, "train/accuracy_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.914777934551239, "train/presicion_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.16475096344947815, "train/recall_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.03272450715303421, "train/f1score_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.054603174328804016, "train/accuracy_label_\u540c\u610f": 0.939445972442627, "train/presicion_label_\u540c\u610f": 0.1510574072599411, "train/recall_label_\u540c\u610f": 0.06045949086546898, "train/f1score_label_\u540c\u610f": 0.0863557830452919, "train/accuracy_label_\u7d0d\u5f97": 0.9606227278709412, "train/presicion_label_\u7d0d\u5f97": 0.08799999952316284, "train/recall_label_\u7d0d\u5f97": 0.0456431545317173, "train/f1score_label_\u7d0d\u5f97": 0.06010929122567177, "train/accuracy_label_\u9a5a\u304d": 0.963598906993866, "train/presicion_label_\u9a5a\u304d": 0.06859205663204193, "train/recall_label_\u9a5a\u304d": 0.04785894230008125, "train/f1score_label_\u9a5a\u304d": 0.05637982115149498, "train/accuracy_label_\u8a00\u3044\u63db\u3048": 0.9808264374732971, "train/presicion_label_\u8a00\u3044\u63db\u3048": 0.0, "train/recall_label_\u8a00\u3044\u63db\u3048": 0.0, "train/f1score_label_\u8a00\u3044\u63db\u3048": 0.0, "train/accuracy_label_\u610f\u898b": 0.9838598966598511, "train/presicion_label_\u610f\u898b": 0.008771929889917374, "train/recall_label_\u610f\u898b": 0.0058823530562222, "train/f1score_label_\u610f\u898b": 0.007042253389954567, "train/accuracy_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.985805869102478, "train/presicion_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.012738853693008423, "train/recall_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.021052632480859756, "train/f1score_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.01587301678955555, "train/accuracy_label_\u4e0d\u540c\u610f": 0.9942193031311035, "train/presicion_label_\u4e0d\u540c\u610f": 0.0, "train/recall_label_\u4e0d\u540c\u610f": 0.0, "train/f1score_label_\u4e0d\u540c\u610f": 0.0, "train/accuracy_label_\u88dc\u5b8c": 0.9888392686843872, "train/presicion_label_\u88dc\u5b8c": 0.0, "train/recall_label_\u88dc\u5b8c": 0.0, "train/f1score_label_\u88dc\u5b8c": 0.0, "train/accuracy_label_\u3042\u3044\u3055\u3064": 0.996623158454895, "train/presicion_label_\u3042\u3044\u3055\u3064": 0.0, "train/recall_label_\u3042\u3044\u3055\u3064": 0.0, "train/f1score_label_\u3042\u3044\u3055\u3064": 0.0, "train/accuracy_label_\u60f3\u8d77": 0.982085645198822, "train/presicion_label_\u60f3\u8d77": 0.0, "train/recall_label_\u60f3\u8d77": 0.0, "train/f1score_label_\u60f3\u8d77": 0.0, "train/accuracy_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.9899839758872986, "train/presicion_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0059523810632526875, "train/recall_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.1111111119389534, "train/f1score_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.011299435049295425, "train/accuracy_label_\u305d\u306e\u4ed6": 0.9352678656578064, "train/presicion_label_\u305d\u306e\u4ed6": 0.10576923191547394, "train/recall_label_\u305d\u306e\u4ed6": 0.054794520139694214, "train/f1score_label_\u305d\u306e\u4ed6": 0.07219032198190689, "test_loss": 0.7588584423065186, "test/multilabelaccuracy": 0.9390888214111328, "test/multilabelprecision": 0.0696084052324295, "test/multilabelrecall": 0.09917648136615753, "test/multilabelf1score": 0.08148421347141266, "test/multilabelmatthewscorrcoef": 0.5927180647850037, "_wandb": {"runtime": 962}}