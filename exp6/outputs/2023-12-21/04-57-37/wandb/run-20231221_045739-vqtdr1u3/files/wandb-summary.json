{"train/loss": 0.749238133430481, "epoch": 4, "trainer/global_step": 2184, "_timestamp": 1703135637.3597207, "_runtime": 977.9802806377411, "_step": 51, "val_loss": 0.7572275996208191, "val/multilabelaccuracy": 0.9406049251556396, "val/multilabelprecision": 0.07314225286245346, "val/multilabelrecall": 0.09267003834247589, "val/multilabelf1score": 0.08043356239795685, "val/multilabelmatthewscorrcoef": 0.5910813808441162, "val/accuracy_label_\u3042\u3044\u3065\u3061": 0.6437729001045227, "val/presicion_label_\u3042\u3044\u3065\u3061": 0.6434304118156433, "val/recall_label_\u3042\u3044\u3065\u3061": 0.9996437430381775, "val/f1score_label_\u3042\u3044\u3065\u3061": 0.7829241156578064, "val/accuracy_label_\u611f\u5fc3": 0.7170329689979553, "val/presicion_label_\u611f\u5fc3": 0.5268456339836121, "val/recall_label_\u611f\u5fc3": 0.48307693004608154, "val/f1score_label_\u611f\u5fc3": 0.5040128231048584, "val/accuracy_label_\u8a55\u4fa1": 0.9539835453033447, "val/presicion_label_\u8a55\u4fa1": 0.0, "val/recall_label_\u8a55\u4fa1": 0.0, "val/f1score_label_\u8a55\u4fa1": 0.0, "val/accuracy_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.9175823926925659, "val/presicion_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/recall_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/f1score_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/accuracy_label_\u540c\u610f": 0.9558150172233582, "val/presicion_label_\u540c\u610f": 0.0, "val/recall_label_\u540c\u610f": 0.0, "val/f1score_label_\u540c\u610f": 0.0, "val/accuracy_label_\u7d0d\u5f97": 0.9686355590820312, "val/presicion_label_\u7d0d\u5f97": 0.0, "val/recall_label_\u7d0d\u5f97": 0.0, "val/f1score_label_\u7d0d\u5f97": 0.0, "val/accuracy_label_\u9a5a\u304d": 0.9736721515655518, "val/presicion_label_\u9a5a\u304d": 0.0, "val/recall_label_\u9a5a\u304d": 0.0, "val/f1score_label_\u9a5a\u304d": 0.0, "val/accuracy_label_\u8a00\u3044\u63db\u3048": 1.0, "val/presicion_label_\u8a00\u3044\u63db\u3048": 0.0, "val/recall_label_\u8a00\u3044\u63db\u3048": 0.0, "val/f1score_label_\u8a00\u3044\u63db\u3048": 0.0, "val/accuracy_label_\u610f\u898b": 0.9910714030265808, "val/presicion_label_\u610f\u898b": 0.0, "val/recall_label_\u610f\u898b": 0.0, "val/f1score_label_\u610f\u898b": 0.0, "val/accuracy_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.997710645198822, "val/presicion_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.0, "val/recall_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.0, "val/f1score_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.0, "val/accuracy_label_\u4e0d\u540c\u610f": 0.9926739931106567, "val/presicion_label_\u4e0d\u540c\u610f": 0.0, "val/recall_label_\u4e0d\u540c\u610f": 0.0, "val/f1score_label_\u4e0d\u540c\u610f": 0.0, "val/accuracy_label_\u88dc\u5b8c": 0.9919871687889099, "val/presicion_label_\u88dc\u5b8c": 0.0, "val/recall_label_\u88dc\u5b8c": 0.0, "val/f1score_label_\u88dc\u5b8c": 0.0, "val/accuracy_label_\u3042\u3044\u3055\u3064": 0.997710645198822, "val/presicion_label_\u3042\u3044\u3055\u3064": 0.0, "val/recall_label_\u3042\u3044\u3055\u3064": 0.0, "val/f1score_label_\u3042\u3044\u3055\u3064": 0.0, "val/accuracy_label_\u60f3\u8d77": 0.9988552927970886, "val/presicion_label_\u60f3\u8d77": 0.0, "val/recall_label_\u60f3\u8d77": 0.0, "val/f1score_label_\u60f3\u8d77": 0.0, "val/accuracy_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.9990842342376709, "val/presicion_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "val/recall_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "val/f1score_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "val/accuracy_label_\u305d\u306e\u4ed6": 0.9500916004180908, "val/presicion_label_\u305d\u306e\u4ed6": 0.0, "val/recall_label_\u305d\u306e\u4ed6": 0.0, "val/f1score_label_\u305d\u306e\u4ed6": 0.0, "train_loss": 0.7574429512023926, "train/multilabelaccuracy": 0.9313008189201355, "train/multilabelprecision": 0.11332441866397858, "train/multilabelrecall": 0.1165948361158371, "train/multilabelf1score": 0.09890920668840408, "train/multilabelmatthewscorrcoef": 0.5685595870018005, "train/accuracy_label_\u3042\u3044\u3065\u3061": 0.6478365659713745, "train/presicion_label_\u3042\u3044\u3065\u3061": 0.6477832794189453, "train/recall_label_\u3042\u3044\u3065\u3061": 0.9996464252471924, "train/f1score_label_\u3042\u3044\u3065\u3061": 0.7861388325691223, "train/accuracy_label_\u611f\u5fc3": 0.6914491653442383, "train/presicion_label_\u611f\u5fc3": 0.48318803310394287, "train/recall_label_\u611f\u5fc3": 0.5998067855834961, "train/f1score_label_\u611f\u5fc3": 0.5352185368537903, "train/accuracy_label_\u8a55\u4fa1": 0.9485462307929993, "train/presicion_label_\u8a55\u4fa1": 0.05314009636640549, "train/recall_label_\u8a55\u4fa1": 0.015406162478029728, "train/f1score_label_\u8a55\u4fa1": 0.02388707920908928, "train/accuracy_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.9149496555328369, "train/presicion_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.17424242198467255, "train/recall_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.03500761091709137, "train/f1score_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0583016462624073, "train/accuracy_label_\u540c\u610f": 0.9486607313156128, "train/presicion_label_\u540c\u610f": 0.03947368264198303, "train/recall_label_\u540c\u610f": 0.0036275694146752357, "train/f1score_label_\u540c\u610f": 0.006644518114626408, "train/accuracy_label_\u7d0d\u5f97": 0.9696085453033447, "train/presicion_label_\u7d0d\u5f97": 0.019607843831181526, "train/recall_label_\u7d0d\u5f97": 0.002074688905850053, "train/f1score_label_\u7d0d\u5f97": 0.0037523452192544937, "train/accuracy_label_\u9a5a\u304d": 0.957932710647583, "train/presicion_label_\u9a5a\u304d": 0.06666667014360428, "train/recall_label_\u9a5a\u304d": 0.06549118459224701, "train/f1score_label_\u9a5a\u304d": 0.0660737007856369, "train/accuracy_label_\u8a00\u3044\u63db\u3048": 0.9779075384140015, "train/presicion_label_\u8a00\u3044\u63db\u3048": 0.0, "train/recall_label_\u8a00\u3044\u63db\u3048": 0.0, "train/f1score_label_\u8a00\u3044\u63db\u3048": 0.0, "train/accuracy_label_\u610f\u898b": 0.9686355590820312, "train/presicion_label_\u610f\u898b": 0.01785714365541935, "train/recall_label_\u610f\u898b": 0.04117647185921669, "train/f1score_label_\u610f\u898b": 0.024911031126976013, "train/accuracy_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.9681203961372375, "train/presicion_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.0021551724057644606, "train/recall_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.010526316240429878, "train/f1score_label_\u8003\u3048\u3066\u3044\u308b\u6700\u4e2d": 0.003577817464247346, "train/accuracy_label_\u4e0d\u540c\u610f": 0.9897550344467163, "train/presicion_label_\u4e0d\u540c\u610f": 0.0, "train/recall_label_\u4e0d\u540c\u610f": 0.0, "train/f1score_label_\u4e0d\u540c\u610f": 0.0, "train/accuracy_label_\u88dc\u5b8c": 0.9889537692070007, "train/presicion_label_\u88dc\u5b8c": 0.20000000298023224, "train/recall_label_\u88dc\u5b8c": 0.005263158120214939, "train/f1score_label_\u88dc\u5b8c": 0.010256410576403141, "train/accuracy_label_\u3042\u3044\u3055\u3064": 0.9871794581413269, "train/presicion_label_\u3042\u3044\u3055\u3064": 0.016393441706895828, "train/recall_label_\u3042\u3044\u3055\u3064": 0.06382978707551956, "train/f1score_label_\u3042\u3044\u3055\u3064": 0.0260869562625885, "train/accuracy_label_\u60f3\u8d77": 0.9990842342376709, "train/presicion_label_\u60f3\u8d77": 0.0, "train/recall_label_\u60f3\u8d77": 0.0, "train/f1score_label_\u60f3\u8d77": 0.0, "train/accuracy_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.997710645198822, "train/presicion_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "train/recall_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "train/f1score_label_\u9a5a\u304d\u3068\u3044\u3076\u304b\u308a": 0.0, "train/accuracy_label_\u305d\u306e\u4ed6": 0.9444826245307922, "train/presicion_label_\u305d\u306e\u4ed6": 0.09268292784690857, "train/recall_label_\u305d\u306e\u4ed6": 0.02366127073764801, "train/f1score_label_\u305d\u306e\u4ed6": 0.0376984141767025, "test_loss": 0.7588424682617188, "test/multilabelaccuracy": 0.9415045380592346, "test/multilabelprecision": 0.13602310419082642, "test/multilabelrecall": 0.09457199275493622, "test/multilabelf1score": 0.08170592039823532, "test/multilabelmatthewscorrcoef": 0.5958393812179565, "_wandb": {"runtime": 978}}