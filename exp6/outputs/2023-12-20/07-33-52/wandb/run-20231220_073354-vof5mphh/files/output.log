
/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”“
â”ƒ[1m   [22mâ”ƒ[1m Name                        [22mâ”ƒ[1m Type                 [22mâ”ƒ[1m Params [22mâ”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ bert                        â”‚ BertModel            â”‚ 89.1 M â”‚
â”‚ 1 â”‚ classifiers                 â”‚ ModuleList           â”‚ 12.6 M â”‚
â”‚ 2 â”‚ hidden_layer                â”‚ ModuleList           â”‚ 16.4 K â”‚
â”‚ 3 â”‚ sigmoid                     â”‚ Sigmoid              â”‚      0 â”‚
â”‚ 4 â”‚ criterion                   â”‚ Dice_MultiLabel_Loss â”‚      0 â”‚
â”‚ 5 â”‚ metrics                     â”‚ MetricCollection     â”‚      0 â”‚
â”‚ 6 â”‚ metrics_per_label_accuracy  â”‚ MetricCollection     â”‚      0 â”‚
â”‚ 7 â”‚ metrics_per_label_precision â”‚ MetricCollection     â”‚      0 â”‚
â”‚ 8 â”‚ metrics_per_label_recall    â”‚ MetricCollection     â”‚      0 â”‚
â”‚ 9 â”‚ metrics_per_label_f1score   â”‚ MetricCollection     â”‚      0 â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[1mTrainable params[22m: 19.7 M
[1mNon-trainable params[22m: 82.0 M
[1mTotal params[22m: 101 M
[1mTotal estimated model params size (MB)[22m: 406
æå¤±é–¢æ•°ã¯å®Ÿè¡Œã•ã‚Œã¾ã—ãŸ
tensor([[0.7257, 0.7471, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671, 0.7848, 0.7257,
         0.7919, 0.7671, 0.7671, 0.7671, 0.7942, 0.7671, 0.7671],
        [0.8025, 0.7284, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671, 0.8143, 0.8049,
         0.7691, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671],
        [0.8186, 0.8073, 0.7671, 0.7888, 0.7671, 0.7671, 0.7671, 0.8001, 0.8469,
         0.7948, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671],
        [0.7382, 0.7748, 0.7671, 0.7858, 0.7671, 0.7671, 0.7671, 0.7909, 0.8100,
         0.7762, 0.7671, 0.7671, 0.7671, 0.7912, 0.7671, 0.7671],
        [0.8184, 0.7865, 0.7671, 0.7772, 0.7671, 0.7671, 0.7671, 0.7977, 0.8034,
         0.7795, 0.7671, 0.7671, 0.7671, 0.8105, 0.7671, 0.7671],
        [0.7561, 0.8015, 0.7671, 0.7957, 0.7671, 0.7740, 0.7671, 0.8063, 0.7979,
         0.7671, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671, 0.7723],
        [0.8349, 0.7420, 0.7671, 0.7820, 0.7671, 0.7671, 0.7671, 0.8044, 0.8071,
         0.8162, 0.7671, 0.7671, 0.7671, 0.7698, 0.7671, 0.7671],
        [0.7590, 0.7847, 0.7671, 0.8031, 0.7671, 0.7671, 0.7671, 0.8001, 0.7950,
         0.7671, 0.7671, 0.7671, 0.7671, 0.7724, 0.7671, 0.7827],
        [0.7459, 0.7451, 0.7671, 0.7692, 0.7671, 0.7671, 0.7671, 0.7871, 0.8263,
         0.7850, 0.7671, 0.7671, 0.7671, 0.7763, 0.7671, 0.7671],
        [0.7940, 0.7942, 0.7671, 0.7712, 0.7671, 0.7671, 0.7671, 0.7954, 0.8008,
         0.7978, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671, 0.7770],
        [0.7671, 0.7830, 0.7671, 0.7703, 0.7671, 0.7803, 0.7785, 0.7914, 0.7928,
         0.7671, 0.7685, 0.7671, 0.7671, 0.7671, 0.7671, 0.7693],
        [0.7370, 0.8362, 0.7671, 0.7884, 0.7671, 0.7699, 0.7671, 0.7836, 0.8444,
         0.7673, 0.7671, 0.7671, 0.7671, 0.7701, 0.7671, 0.7671],
        [0.7466, 0.8179, 0.7671, 0.7745, 0.7671, 0.7682, 0.7671, 0.7884, 0.8029,
         0.7781, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671],
        [0.7497, 0.8053, 0.7671, 0.7678, 0.7671, 0.7671, 0.7671, 0.7946, 0.8325,
         0.8080, 0.7671, 0.7671, 0.7671, 0.7805, 0.7671, 0.7671],
        [0.7579, 0.8175, 0.7671, 0.7848, 0.7671, 0.7824, 0.7671, 0.8160, 0.8045,
         0.7671, 0.7671, 0.7671, 0.7671, 0.7852, 0.7671, 0.7671],
        [0.7898, 0.7271, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671, 0.8007, 0.7869,
         0.7671, 0.7671, 0.7671, 0.7671, 0.7947, 0.7671, 0.7671],
        [0.7391, 0.8140, 0.7671, 0.7821, 0.7671, 0.7671, 0.7671, 0.8058, 0.8320,
         0.7671, 0.7671, 0.7671, 0.7671, 0.7697, 0.7671, 0.7671],
        [0.7450, 0.7243, 0.7671, 0.7488, 0.7671, 0.7683, 0.7671, 0.7880, 0.7934,
         0.7671, 0.7671, 0.7671, 0.7671, 0.7713, 0.7671, 0.7671],
        [0.7866, 0.8155, 0.7671, 0.7717, 0.7671, 0.7671, 0.7671, 0.7895, 0.8172,
         0.7913, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671],
        [0.8111, 0.7746, 0.7671, 0.7686, 0.7671, 0.7671, 0.7671, 0.7860, 0.8409,
         0.8048, 0.7671, 0.7671, 0.7671, 0.7956, 0.7671, 0.7671],
        [0.7901, 0.7243, 0.7671, 0.7860, 0.7671, 0.7683, 0.7671, 0.7880, 0.7934,
         0.7671, 0.7671, 0.7671, 0.7671, 0.7713, 0.7671, 0.7671],
        [0.7415, 0.7413, 0.7671, 0.7712, 0.7671, 0.7671, 0.7671, 0.7954, 0.8008,
         0.7978, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671, 0.7770],
        [0.7584, 0.7985, 0.7671, 0.7704, 0.7671, 0.7671, 0.7671, 0.8258, 0.8045,
         0.7787, 0.7671, 0.7671, 0.7671, 0.7799, 0.7671, 0.7671],
        [0.7336, 0.7957, 0.7671, 0.7781, 0.7671, 0.7671, 0.7671, 0.7827, 0.8204,
         0.7674, 0.7671, 0.7671, 0.7671, 0.7964, 0.7671, 0.7671],
        [0.7258, 0.7292, 0.7671, 0.7761, 0.7671, 0.7671, 0.7671, 0.7890, 0.8087,
         0.7936, 0.7671, 0.7671, 0.7671, 0.7994, 0.7671, 0.7671],
        [0.8389, 0.7671, 0.7671, 0.7715, 0.7671, 0.7671, 0.7671, 0.8036, 0.8199,
         0.7683, 0.7671, 0.7671, 0.7671, 0.7962, 0.7671, 0.7671],
        [0.7427, 0.7318, 0.7671, 0.7731, 0.7671, 0.7671, 0.7671, 0.8004, 0.8425,
         0.7686, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671],
        [0.7225, 0.7439, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671, 0.7923, 0.8221,
         0.7705, 0.7671, 0.7671, 0.7671, 0.7739, 0.7671, 0.7671],
        [0.7349, 0.7905, 0.7671, 0.7672, 0.7671, 0.7671, 0.7671, 0.8033, 0.7824,
         0.7918, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671, 0.7671],
        [0.8146, 0.7671, 0.7739, 0.7762, 0.7671, 0.7781, 0.7671, 0.8141, 0.8111,
         0.7671, 0.7671, 0.7671, 0.7671, 0.7940, 0.7671, 0.7671],
        [0.7235, 0.7704, 0.7671, 0.7896, 0.7671, 0.7671, 0.7671, 0.7952, 0.8203,
         0.8003, 0.7671, 0.7671, 0.7671, 0.7842, 0.7671, 0.7671],
        [0.7276, 0.7318, 0.7737, 0.7454, 0.7671, 0.7684, 0.7671, 0.8048, 0.8429,
         0.7981, 0.7671, 0.7708, 0.7671, 0.7671, 0.7671, 0.7741]],
       device='cuda:0')
æå¤±é–¢æ•°ã¯å®Ÿè¡Œã•ã‚Œã¾ã—ãŸ
tensor([[0.7520, 0.8513, 0.7821, 0.8035, 0.7821, 0.7849, 0.7821, 0.7987, 0.8594,
         0.7823, 0.7821, 0.7821, 0.7821, 0.7852, 0.7821, 0.7821],
        [0.7520, 0.8513, 0.7821, 0.8035, 0.7821, 0.7849, 0.7821, 0.7987, 0.8594,
         0.7823, 0.7821, 0.7821, 0.7821, 0.7852, 0.7821, 0.7821],
        [0.7408, 0.7595, 0.7821, 0.7849, 0.7821, 0.7821, 0.7821, 0.8196, 0.8071,
         0.8338, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821],
        [0.7629, 0.8292, 0.7821, 0.8143, 0.7821, 0.7864, 0.7821, 0.8227, 0.8360,
         0.7821, 0.7821, 0.7821, 0.7821, 0.7900, 0.7821, 0.7863],
        [0.7923, 0.8336, 0.7821, 0.8178, 0.7821, 0.7821, 0.7829, 0.8209, 0.8129,
         0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7953],
        [0.7503, 0.8254, 0.7821, 0.8207, 0.7821, 0.7883, 0.7821, 0.8186, 0.8361,
         0.7821, 0.7821, 0.7821, 0.7821, 0.7850, 0.7821, 0.7908],
        [0.8024, 0.7387, 0.7821, 0.8248, 0.7821, 0.7821, 0.7821, 0.8218, 0.8120,
         0.7821, 0.7821, 0.7821, 0.7821, 0.7907, 0.7821, 0.7977],
        [0.7701, 0.7397, 0.7821, 0.7829, 0.7821, 0.7821, 0.7821, 0.8103, 0.8380,
         0.8078, 0.7821, 0.7821, 0.7914, 0.7821, 0.7821, 0.7821],
        [0.7654, 0.8231, 0.7821, 0.8053, 0.7821, 0.7821, 0.7821, 0.8359, 0.8153,
         0.7821, 0.7831, 0.7821, 0.7821, 0.7821, 0.7821, 0.7998],
        [0.7868, 0.8397, 0.7821, 0.7979, 0.7821, 0.7821, 0.7821, 0.8030, 0.8523,
         0.7914, 0.7821, 0.7834, 0.7821, 0.7864, 0.7821, 0.7821],
        [0.7636, 0.8085, 0.7821, 0.8232, 0.7821, 0.7921, 0.7821, 0.8156, 0.8244,
         0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7833],
        [0.8117, 0.7392, 0.7821, 0.7971, 0.7821, 0.7821, 0.7821, 0.8208, 0.8470,
         0.7821, 0.7821, 0.7821, 0.7821, 0.7847, 0.7821, 0.7821],
        [0.7396, 0.8040, 0.7821, 0.7893, 0.7821, 0.7858, 0.7821, 0.8141, 0.8251,
         0.8019, 0.7821, 0.7821, 0.7821, 0.7999, 0.7821, 0.7821],
        [0.7821, 0.8184, 0.7821, 0.7915, 0.7821, 0.7821, 0.7821, 0.8189, 0.8262,
         0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7872],
        [0.7636, 0.7427, 0.7821, 0.7852, 0.7821, 0.7821, 0.7821, 0.8385, 0.8453,
         0.7821, 0.7821, 0.7821, 0.7821, 0.8127, 0.7821, 0.7821],
        [0.7832, 0.8190, 0.7821, 0.8002, 0.7821, 0.7821, 0.7821, 0.8194, 0.8089,
         0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821],
        [0.7930, 0.8306, 0.7821, 0.7821, 0.7821, 0.7851, 0.7821, 0.8084, 0.8393,
         0.7958, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821],
        [0.7821, 0.8243, 0.7821, 0.8069, 0.7821, 0.7832, 0.7821, 0.8112, 0.8191,
         0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7843],
        [0.8015, 0.8014, 0.7821, 0.7821, 0.7821, 0.7821, 0.7845, 0.7920, 0.8046,
         0.8099, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821],
        [0.7382, 0.7918, 0.7821, 0.7964, 0.7821, 0.8004, 0.7821, 0.8379, 0.8152,
         0.7873, 0.7821, 0.7821, 0.7821, 0.8021, 0.7821, 0.7821],
        [0.7810, 0.8207, 0.7821, 0.7914, 0.7821, 0.7821, 0.7884, 0.8351, 0.8416,
         0.7982, 0.7821, 0.7821, 0.7821, 0.7851, 0.7821, 0.7821],
        [0.7557, 0.8175, 0.7821, 0.7945, 0.7821, 0.8049, 0.7821, 0.8165, 0.8152,
         0.7877, 0.7821, 0.7821, 0.7821, 0.7856, 0.7821, 0.7821],
        [0.7768, 0.7495, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.8035, 0.8085,
         0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821],
        [0.7821, 0.8007, 0.7821, 0.8199, 0.7821, 0.7821, 0.7895, 0.8152, 0.8066,
         0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7897],
        [0.8052, 0.8289, 0.7821, 0.8010, 0.7821, 0.7834, 0.7821, 0.8031, 0.8084,
         0.7821, 0.7821, 0.7821, 0.7821, 0.7863, 0.7821, 0.7821],
        [0.7205, 0.7874, 0.7821, 0.7904, 0.7821, 0.7821, 0.7821, 0.8081, 0.8447,
         0.8129, 0.7821, 0.7821, 0.7821, 0.8214, 0.7821, 0.7821],
        [0.7821, 0.8206, 0.7821, 0.7599, 0.7821, 0.7821, 0.7821, 0.8144, 0.8074,
         0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7821, 0.7930],
        [0.7588, 0.8208, 0.7821, 0.7904, 0.7821, 0.7821, 0.7821, 0.8073, 0.8159,
         0.8034, 0.7821, 0.7821, 0.7821, 0.7877, 0.7821, 0.7821],
        [0.8053, 0.8184, 0.7821, 0.7987, 0.7821, 0.7821, 0.7821, 0.8102, 0.8447,
         0.8072, 0.7821, 0.7821, 0.7821, 0.7832, 0.7821, 0.7821],
        [0.7909, 0.8136, 0.7821, 0.7855, 0.7821, 0.7821, 0.7821, 0.8409, 0.8196,
         0.7937, 0.7821, 0.7821, 0.7821, 0.7949, 0.7821, 0.7821],
        [0.7459, 0.8027, 0.7821, 0.7847, 0.7821, 0.7821, 0.7821, 0.7924, 0.8186,
         0.8123, 0.7821, 0.7821, 0.7821, 0.7984, 0.7821, 0.7821],
        [0.7648, 0.8276, 0.8182, 0.7986, 0.7821, 0.7821, 0.7821, 0.8275, 0.8926,
         0.7821, 0.7821, 0.7821, 0.7821, 0.8081, 0.7821, 0.7821]],
       device='cuda:0')
æå¤±é–¢æ•°ã¯å®Ÿè¡Œã•ã‚Œã¾ã—ãŸ
[37mSanity Checking[39m [35mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[39m [37m2/2[39m [37m0:00:02 â€¢ 0:00:00[39m [37m1.06it/s
[?25h
Error executing job with overrides: []
Traceback (most recent call last):
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp6/main.py", line 626, in main
    trainer.fit(model, data_module)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 989, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
    self._run_sanity_check()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1062, in _run_sanity_check
    val_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py", line 141, in run
    return self.on_run_end()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py", line 253, in on_run_end
    self._on_evaluation_epoch_end()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py", line 329, in _on_evaluation_epoch_end
    call._call_lightning_module_hook(trainer, hook_name)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp6/main.py", line 392, in on_validation_epoch_end
    self.log(f"{mode}_loss", epoch_loss, logger=True)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py", line 454, in log
    value = apply_to_collection(value, (Tensor, numbers.Number), self.__to_tensor, name)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py", line 627, in __to_tensor
    raise ValueError(
ValueError: `self.log(val_loss, tensor([[0.7338, 0.7552, 0.7752,  ..., 0.8023, 0.7752, 0.7752],
        [0.8106, 0.7365, 0.7752,  ..., 0.7752, 0.7752, 0.7752],
        [0.8267, 0.8154, 0.7752,  ..., 0.7752, 0.7752, 0.7752],
        ...,
        [0.7840, 0.8066, 0.7752,  ..., 0.7880, 0.7752, 0.7752],
        [0.7390, 0.7958, 0.7752,  ..., 0.7915, 0.7752, 0.7752],
        [0.7579, 0.8206, 0.8113,  ..., 0.8012, 0.7752, 0.7752]],
       device='cuda:0'))` was called, but the tensor must have a single element. You can try doing `self.log(val_loss, tensor([[0.7338, 0.7552, 0.7752,  ..., 0.8023, 0.7752, 0.7752],
        [0.8106, 0.7365, 0.7752,  ..., 0.7752, 0.7752, 0.7752],
        [0.8267, 0.8154, 0.7752,  ..., 0.7752, 0.7752, 0.7752],
        ...,
        [0.7840, 0.8066, 0.7752,  ..., 0.7880, 0.7752, 0.7752],
        [0.7390, 0.7958, 0.7752,  ..., 0.7915, 0.7752, 0.7752],
        [0.7579, 0.8206, 0.8113,  ..., 0.8012, 0.7752, 0.7752]],
       device='cuda:0').mean())`
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.