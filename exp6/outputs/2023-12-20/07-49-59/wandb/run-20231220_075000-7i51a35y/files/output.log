/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loggers/wandb.py:389: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
┏━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃[1m   [22m┃[1m Name                        [22m┃[1m Type                 [22m┃[1m Params [22m┃
┡━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ bert                        │ BertModel            │ 89.1 M │
│ 1 │ classifiers                 │ ModuleList           │ 12.6 M │
│ 2 │ hidden_layer                │ ModuleList           │ 16.4 K │
│ 3 │ sigmoid                     │ Sigmoid              │      0 │
│ 4 │ criterion                   │ Dice_MultiLabel_Loss │      0 │
│ 5 │ metrics                     │ MetricCollection     │      0 │
│ 6 │ metrics_per_label_accuracy  │ MetricCollection     │      0 │
│ 7 │ metrics_per_label_precision │ MetricCollection     │      0 │
│ 8 │ metrics_per_label_recall    │ MetricCollection     │      0 │
│ 9 │ metrics_per_label_f1score   │ MetricCollection     │      0 │
└───┴─────────────────────────────┴──────────────────────┴────────┘
[1mTrainable params[22m: 19.7 M
[1mNon-trainable params[22m: 82.0 M
[1mTotal params[22m: 101 M
[1mTotal estimated model params size (MB)[22m: 406
Error executing job with overrides: []
tensor([0.5000, 0.5509, 0.5426, 0.5000, 0.5088, 0.5000, 0.5000, 0.5000, 0.5051,
        0.5020, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5484,
        0.5488, 0.5000, 0.5386, 0.5000, 0.5000, 0.5000, 0.5217, 0.5067, 0.5037,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5365, 0.5321, 0.5000,
        0.5423, 0.5000, 0.5000, 0.5000, 0.5000, 0.5210, 0.5088, 0.5098, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5303, 0.5350, 0.5000, 0.5212, 0.5000,
        0.5000, 0.5000, 0.5161, 0.5020, 0.5000, 0.5060, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5216, 0.5318, 0.5000, 0.5126, 0.5000, 0.5000, 0.5000,
        0.5046, 0.5018, 0.5000, 0.5118, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5165, 0.5137, 0.5005, 0.5077, 0.5000, 0.5000, 0.5071, 0.5000, 0.5000,
        0.5000, 0.5010, 0.5000, 0.5145, 0.5000, 0.5071, 0.5000, 0.5380, 0.5636,
        0.5000, 0.5215, 0.5000, 0.5000, 0.5000, 0.5130, 0.5056, 0.5262, 0.5091,
        0.5000, 0.5000, 0.5088, 0.5000, 0.5000, 0.5115, 0.5303, 0.5192, 0.5118,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5057, 0.5000, 0.5118,
        0.5000, 0.5125, 0.5000, 0.5376, 0.5411, 0.5000, 0.5070, 0.5000, 0.5000,
        0.5004, 0.5019, 0.5000, 0.5119, 0.5000, 0.5016, 0.5000, 0.5188, 0.5000,
        0.5000, 0.5295, 0.5145, 0.5000, 0.5033, 0.5000, 0.5090, 0.5261, 0.5138,
        0.5343, 0.5238, 0.5000, 0.5000, 0.5000, 0.5197, 0.5000, 0.5000, 0.5222,
        0.5157, 0.5156, 0.5071, 0.5000, 0.5000, 0.5035, 0.5000, 0.5000, 0.5144,
        0.5058, 0.5000, 0.5170, 0.5029, 0.5183, 0.5000, 0.5487, 0.5650, 0.5000,
        0.5252, 0.5000, 0.5000, 0.5000, 0.5190, 0.5173, 0.5131, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5160, 0.5333, 0.5000, 0.5220, 0.5135,
        0.5000, 0.5000, 0.5000, 0.5030, 0.5151, 0.5000, 0.5000, 0.5000, 0.5059,
        0.5000, 0.5000, 0.5163, 0.5421, 0.5000, 0.5210, 0.5000, 0.5000, 0.5000,
        0.5083, 0.5106, 0.5009, 0.5000, 0.5027, 0.5000, 0.5195, 0.5000, 0.5000,
        0.5175, 0.5165, 0.5041, 0.5000, 0.5000, 0.5000, 0.5175, 0.5000, 0.5126,
        0.5000, 0.5062, 0.5000, 0.5107, 0.5000, 0.5000, 0.5000, 0.5470, 0.5267,
        0.5000, 0.5213, 0.5000, 0.5000, 0.5257, 0.5071, 0.5000, 0.5313, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5358, 0.5495, 0.5000, 0.5204,
        0.5361, 0.5000, 0.5079, 0.5000, 0.5000, 0.5206, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5067, 0.5205, 0.5123, 0.5235, 0.5000, 0.5000,
        0.5169, 0.5000, 0.5001, 0.5185, 0.5023, 0.5000, 0.5135, 0.5071, 0.5000,
        0.5000, 0.5107, 0.5322, 0.5050, 0.5247, 0.5039, 0.5000, 0.5087, 0.5058,
        0.5018, 0.5221, 0.5000, 0.5000, 0.5000, 0.5108, 0.5000, 0.5000, 0.5377,
        0.5492, 0.5071, 0.5190, 0.5000, 0.5000, 0.5000, 0.5139, 0.5225, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5093, 0.5000, 0.5000, 0.5067, 0.5205, 0.5123,
        0.5235, 0.5000, 0.5000, 0.5169, 0.5000, 0.5001, 0.5185, 0.5023, 0.5000,
        0.5135, 0.5071, 0.5000, 0.5000, 0.5295, 0.5145, 0.5000, 0.5033, 0.5000,
        0.5090, 0.5261, 0.5138, 0.5343, 0.5238, 0.5000, 0.5000, 0.5000, 0.5197,
        0.5000, 0.5000, 0.5255, 0.5228, 0.5000, 0.5042, 0.5066, 0.5000, 0.5138,
        0.5054, 0.5000, 0.5233, 0.5000, 0.5000, 0.5005, 0.5166, 0.5000, 0.5000,
        0.5356, 0.5423, 0.5000, 0.5319, 0.5000, 0.5000, 0.5000, 0.5094, 0.5000,
        0.5125, 0.5011, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5211, 0.5423,
        0.5000, 0.5042, 0.5000, 0.5000, 0.5000, 0.5004, 0.5320, 0.5037, 0.5000,
        0.5000, 0.5000, 0.5102, 0.5000, 0.5000, 0.5141, 0.5287, 0.5000, 0.5264,
        0.5000, 0.5000, 0.5000, 0.5193, 0.5000, 0.5042, 0.5199, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5257, 0.5477, 0.5000, 0.5259, 0.5052, 0.5000,
        0.5000, 0.5135, 0.5252, 0.5104, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5230, 0.5327, 0.5000, 0.5111, 0.5000, 0.5000, 0.5000, 0.5039,
        0.5010, 0.5000, 0.5000, 0.5000, 0.5000, 0.5247, 0.5000, 0.5013, 0.5344,
        0.5533, 0.5000, 0.5000, 0.5000, 0.5065, 0.5000, 0.5000, 0.5000, 0.5248,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5209, 0.5269, 0.5095,
        0.5077, 0.5000, 0.5000, 0.5000, 0.5061, 0.5011, 0.5000, 0.5121, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5436, 0.5395, 0.5000, 0.5238, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5038, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5576, 0.5586, 0.5073, 0.5387, 0.5000, 0.5000, 0.5218,
        0.5068, 0.5270, 0.5000, 0.5000, 0.5000, 0.5006, 0.5000, 0.5000],
       device='cuda:0')
tensor([1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,
        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
tensor(23.6147, device='cuda:0')
損失関数は実行されました
tensor([[0.7681, 0.7196, 0.7272, 0.7681, 0.7593, 0.7681, 0.7681, 0.7681, 0.7630,
         0.7700, 0.7681, 0.7681, 0.7681, 0.7681, 0.7681, 0.7681],
        [0.7681, 0.7219, 0.8194, 0.7681, 0.8083, 0.7681, 0.7681, 0.7681, 0.7903,
         0.7748, 0.7718, 0.7681, 0.7681, 0.7681, 0.7681, 0.7681],
        [0.7681, 0.8060, 0.8012, 0.7681, 0.8123, 0.7681, 0.7681, 0.7681, 0.7681,
         0.7896, 0.7769, 0.7779, 0.7681, 0.7681, 0.7681, 0.7681],
        [0.7681, 0.7993, 0.8044, 0.7681, 0.7898, 0.7681, 0.7681, 0.7681, 0.7844,
         0.7700, 0.7681, 0.7741, 0.7681, 0.7681, 0.7681, 0.7681],
        [0.7681, 0.7901, 0.8009, 0.7681, 0.7808, 0.7681, 0.7681, 0.7681, 0.7727,
         0.7699, 0.7681, 0.7800, 0.7681, 0.7681, 0.7681, 0.7681],
        [0.7681, 0.7849, 0.7819, 0.7686, 0.7759, 0.7681, 0.7681, 0.7752, 0.7681,
         0.7681, 0.7681, 0.7691, 0.7681, 0.7828, 0.7681, 0.7752],
        [0.7681, 0.7315, 0.7082, 0.7681, 0.7901, 0.7681, 0.7681, 0.7681, 0.7812,
         0.7737, 0.7950, 0.7773, 0.7681, 0.7681, 0.7770, 0.7681],
        [0.7681, 0.7797, 0.7994, 0.7876, 0.7800, 0.7681, 0.7681, 0.7681, 0.7681,
         0.7681, 0.7681, 0.7738, 0.7681, 0.7800, 0.7681, 0.7808],
        [0.7681, 0.7318, 0.8109, 0.7681, 0.7752, 0.7681, 0.7681, 0.7684, 0.7700,
         0.7681, 0.7802, 0.7681, 0.7697, 0.7681, 0.7872, 0.7681],
        [0.7681, 0.7984, 0.7828, 0.7681, 0.7714, 0.7681, 0.7772, 0.7949, 0.7821,
         0.8036, 0.7925, 0.7681, 0.7681, 0.7681, 0.7882, 0.7681],
        [0.7681, 0.7908, 0.7840, 0.7839, 0.7752, 0.7681, 0.7681, 0.7715, 0.7681,
         0.7681, 0.7827, 0.7739, 0.7681, 0.7854, 0.7710, 0.7867],
        [0.7681, 0.8194, 0.8377, 0.7681, 0.7939, 0.7681, 0.7681, 0.7681, 0.7874,
         0.7857, 0.7813, 0.7681, 0.7681, 0.7681, 0.7681, 0.7681],
        [0.7681, 0.7844, 0.8026, 0.7681, 0.7906, 0.7818, 0.7681, 0.7681, 0.7681,
         0.7711, 0.7834, 0.7681, 0.7681, 0.7681, 0.7740, 0.7681],
        [0.7681, 0.7846, 0.8121, 0.7681, 0.7895, 0.7681, 0.7681, 0.7681, 0.7764,
         0.7788, 0.7690, 0.7681, 0.7707, 0.7681, 0.7880, 0.7681],
        [0.7681, 0.7859, 0.7848, 0.7722, 0.7681, 0.7681, 0.7681, 0.7858, 0.7681,
         0.7809, 0.7681, 0.7743, 0.7681, 0.7789, 0.7681, 0.7681],
        [0.7681, 0.7232, 0.7955, 0.7681, 0.7898, 0.7681, 0.7681, 0.7945, 0.7752,
         0.7681, 0.8004, 0.7681, 0.7681, 0.7681, 0.7681, 0.7681],
        [0.7681, 0.8052, 0.8202, 0.7681, 0.7889, 0.8056, 0.7681, 0.7760, 0.7681,
         0.7681, 0.7891, 0.7681, 0.7681, 0.7681, 0.7681, 0.7681],
        [0.7681, 0.7614, 0.7890, 0.7559, 0.7921, 0.7681, 0.7681, 0.7853, 0.7681,
         0.7681, 0.7869, 0.7704, 0.7681, 0.7818, 0.7753, 0.7681],
        [0.7681, 0.7789, 0.8014, 0.7731, 0.7440, 0.7720, 0.7681, 0.7769, 0.7739,
         0.7699, 0.7906, 0.7681, 0.7681, 0.7681, 0.7790, 0.7681],
        [0.7681, 0.8073, 0.8198, 0.7753, 0.7874, 0.7681, 0.7681, 0.7681, 0.7821,
         0.7911, 0.7681, 0.7681, 0.7681, 0.7681, 0.7774, 0.7681],
        [0.7681, 0.7614, 0.7890, 0.7805, 0.7921, 0.7681, 0.7681, 0.7853, 0.7681,
         0.7681, 0.7869, 0.7704, 0.7681, 0.7818, 0.7753, 0.7681],
        [0.7681, 0.7394, 0.7538, 0.7681, 0.7714, 0.7681, 0.7772, 0.7949, 0.7821,
         0.8036, 0.7925, 0.7681, 0.7681, 0.7681, 0.7882, 0.7681],
        [0.7681, 0.7943, 0.7914, 0.7681, 0.7723, 0.7748, 0.7681, 0.7820, 0.7735,
         0.7681, 0.7920, 0.7681, 0.7681, 0.7685, 0.7849, 0.7681],
        [0.7681, 0.8050, 0.8122, 0.7681, 0.8010, 0.7681, 0.7681, 0.7681, 0.7776,
         0.7681, 0.7807, 0.7691, 0.7681, 0.7681, 0.7681, 0.7681],
        [0.7681, 0.7474, 0.7274, 0.7681, 0.7722, 0.7681, 0.7681, 0.7681, 0.7685,
         0.8011, 0.7718, 0.7681, 0.7681, 0.7681, 0.7783, 0.7681],
        [0.7681, 0.7824, 0.7976, 0.7681, 0.7952, 0.7681, 0.7681, 0.7681, 0.7878,
         0.7681, 0.7722, 0.7884, 0.7681, 0.7681, 0.7681, 0.7681],
        [0.7681, 0.7430, 0.8182, 0.7681, 0.7947, 0.7733, 0.7681, 0.7681, 0.7818,
         0.7939, 0.7786, 0.7681, 0.7681, 0.7681, 0.7681, 0.7681],
        [0.7681, 0.7456, 0.8019, 0.7681, 0.7793, 0.7681, 0.7681, 0.7681, 0.7720,
         0.7691, 0.7681, 0.7681, 0.7681, 0.7681, 0.7934, 0.7681],
        [0.7668, 0.8037, 0.8244, 0.7681, 0.7681, 0.7681, 0.7746, 0.7681, 0.7681,
         0.7681, 0.7935, 0.7681, 0.7681, 0.7681, 0.7681, 0.7681],
        [0.7681, 0.7894, 0.7957, 0.7776, 0.7759, 0.7681, 0.7681, 0.7681, 0.7742,
         0.7692, 0.7681, 0.7803, 0.7681, 0.7681, 0.7681, 0.7681],
        [0.7681, 0.8136, 0.8092, 0.7681, 0.7924, 0.7681, 0.7681, 0.7681, 0.7681,
         0.7719, 0.7681, 0.7681, 0.7681, 0.7681, 0.7681, 0.7681],
        [0.7681, 0.7135, 0.8304, 0.7609, 0.8084, 0.7681, 0.7681, 0.7903, 0.7749,
         0.7958, 0.7681, 0.7681, 0.7681, 0.7687, 0.7681, 0.7681]],
       device='cuda:0')
tensor([0.5000, 0.5487, 0.5650, 0.5000, 0.5252, 0.5000, 0.5000, 0.5000, 0.5190,
        0.5173, 0.5131, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5487,
        0.5650, 0.5000, 0.5252, 0.5000, 0.5000, 0.5000, 0.5190, 0.5173, 0.5131,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5348, 0.5445, 0.5000,
        0.5040, 0.5000, 0.5008, 0.5000, 0.5185, 0.5034, 0.5395, 0.5000, 0.5000,
        0.5003, 0.5374, 0.5000, 0.5000, 0.5246, 0.5347, 0.5008, 0.5227, 0.5000,
        0.5000, 0.5019, 0.5018, 0.5000, 0.5058, 0.5000, 0.5000, 0.5127, 0.5011,
        0.5019, 0.5000, 0.5000, 0.5090, 0.5085, 0.5023, 0.5145, 0.5000, 0.5200,
        0.5000, 0.5000, 0.5065, 0.5105, 0.5000, 0.5119, 0.5079, 0.5264, 0.5000,
        0.5261, 0.5251, 0.5000, 0.5003, 0.5000, 0.5000, 0.5069, 0.5000, 0.5217,
        0.5000, 0.5000, 0.5000, 0.5276, 0.5161, 0.5000, 0.5000, 0.5235, 0.5440,
        0.5061, 0.5158, 0.5000, 0.5000, 0.5029, 0.5000, 0.5000, 0.5000, 0.5000,
        0.5000, 0.5396, 0.5000, 0.5000, 0.5000, 0.5103, 0.5413, 0.5000, 0.5194,
        0.5072, 0.5000, 0.5037, 0.5000, 0.5000, 0.5351, 0.5000, 0.5000, 0.5000,
        0.5036, 0.5000, 0.5000, 0.5000, 0.5173, 0.5163, 0.5158, 0.5201, 0.5000,
        0.5141, 0.5000, 0.5028, 0.5000, 0.5136, 0.5000, 0.5212, 0.5000, 0.5242,
        0.5000, 0.5321, 0.5643, 0.5100, 0.5170, 0.5089, 0.5000, 0.5140, 0.5036,
        0.5278, 0.5000, 0.5000, 0.5000, 0.5070, 0.5141, 0.5000, 0.5000, 0.5245,
        0.5141, 0.5118, 0.5101, 0.5000, 0.5000, 0.5000, 0.5000, 0.5122, 0.5072,
        0.5000, 0.5000, 0.5152, 0.5107, 0.5000, 0.5000, 0.5358, 0.5495, 0.5000,
        0.5204, 0.5361, 0.5000, 0.5079, 0.5000, 0.5000, 0.5206, 0.5000, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5145, 0.5321, 0.5000, 0.5163, 0.5000,
        0.5000, 0.5075, 0.5124, 0.5090, 0.5060, 0.5120, 0.5000, 0.5000, 0.5008,
        0.5000, 0.5000, 0.5070, 0.5184, 0.5009, 0.5049, 0.5000, 0.5000, 0.5208,
        0.5000, 0.5000, 0.5062, 0.5036, 0.5000, 0.5170, 0.5000, 0.5048, 0.5000,
        0.5275, 0.5335, 0.5003, 0.5233, 0.5055, 0.5000, 0.5168, 0.5142, 0.5000,
        0.5018, 0.5000, 0.5000, 0.5151, 0.5014, 0.5000, 0.5000, 0.5067, 0.5236,
        0.5019, 0.5036, 0.5060, 0.5000, 0.5171, 0.5000, 0.5000, 0.5000, 0.5092,
        0.5000, 0.5122, 0.5025, 0.5161, 0.5000, 0.5170, 0.5164, 0.5008, 0.5275,
        0.5046, 0.5000, 0.5199, 0.5098, 0.5000, 0.5244, 0.5035, 0.5000, 0.5000,
        0.5062, 0.5000, 0.5000, 0.5000, 0.5258, 0.5071, 0.5047, 0.5000, 0.5000,
        0.5255, 0.5054, 0.5000, 0.5209, 0.5002, 0.5000, 0.5230, 0.5000, 0.5195,
        0.5000, 0.5288, 0.5278, 0.5148, 0.5271, 0.5000, 0.5000, 0.5058, 0.5082,
        0.5106, 0.5157, 0.5000, 0.5000, 0.5000, 0.5130, 0.5000, 0.5000, 0.5257,
        0.5161, 0.5203, 0.5102, 0.5000, 0.5000, 0.5000, 0.5000, 0.5012, 0.5000,
        0.5304, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5213, 0.5302, 0.5000,
        0.5000, 0.5044, 0.5000, 0.5143, 0.5028, 0.5000, 0.5273, 0.5000, 0.5000,
        0.5016, 0.5082, 0.5000, 0.5000, 0.5345, 0.5206, 0.5000, 0.5126, 0.5000,
        0.5000, 0.5000, 0.5104, 0.5031, 0.5042, 0.5000, 0.5000, 0.5071, 0.5223,
        0.5000, 0.5000, 0.5263, 0.5370, 0.5000, 0.5118, 0.5000, 0.5000, 0.5203,
        0.5096, 0.5000, 0.5198, 0.5055, 0.5000, 0.5000, 0.5130, 0.5000, 0.5000,
        0.5062, 0.5214, 0.5000, 0.5021, 0.5000, 0.5000, 0.5075, 0.5000, 0.5000,
        0.5162, 0.5000, 0.5000, 0.5166, 0.5015, 0.5126, 0.5000, 0.5067, 0.5205,
        0.5123, 0.5235, 0.5000, 0.5000, 0.5169, 0.5000, 0.5001, 0.5185, 0.5023,
        0.5000, 0.5135, 0.5071, 0.5000, 0.5000, 0.5436, 0.5541, 0.5000, 0.5176,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5287, 0.5000, 0.5079, 0.5000, 0.5000,
        0.5063, 0.5000, 0.5000, 0.5144, 0.5237, 0.5014, 0.5050, 0.5034, 0.5000,
        0.5181, 0.5033, 0.5000, 0.5185, 0.5000, 0.5000, 0.5242, 0.5033, 0.5105,
        0.5000, 0.5258, 0.5310, 0.5015, 0.5151, 0.5000, 0.5000, 0.5000, 0.5151,
        0.5000, 0.5220, 0.5000, 0.5000, 0.5000, 0.5019, 0.5000, 0.5000, 0.5290,
        0.5505, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5050, 0.5211, 0.5000,
        0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5255, 0.5228, 0.5000,
        0.5042, 0.5066, 0.5000, 0.5138, 0.5054, 0.5000, 0.5233, 0.5000, 0.5000,
        0.5005, 0.5166, 0.5000, 0.5000, 0.5289, 0.5371, 0.5000, 0.5334, 0.5000,
        0.5000, 0.5000, 0.5178, 0.5211, 0.5020, 0.5000, 0.5000, 0.5000, 0.5164,
        0.5000, 0.5000, 0.5356, 0.5485, 0.5000, 0.5415, 0.5077, 0.5000, 0.5089,
        0.5042, 0.5284, 0.5000, 0.5054, 0.5000, 0.5000, 0.5000, 0.5000],
       device='cuda:0')
tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
tensor(18.2651, device='cuda:0')
損失関数は実行されました
tensor([[0.7834, 0.8347, 0.8530, 0.7834, 0.8093, 0.7834, 0.7834, 0.7834, 0.8028,
         0.8011, 0.7967, 0.7834, 0.7834, 0.7834, 0.7834, 0.7834],
        [0.7834, 0.8347, 0.8530, 0.7834, 0.8093, 0.7834, 0.7834, 0.7834, 0.8028,
         0.8011, 0.7967, 0.7834, 0.7834, 0.7834, 0.7834, 0.7834],
        [0.7834, 0.7498, 0.7408, 0.7834, 0.7874, 0.7834, 0.7842, 0.7834, 0.8023,
         0.7869, 0.8246, 0.7834, 0.7834, 0.7837, 0.8223, 0.7834],
        [0.7834, 0.8087, 0.8194, 0.7843, 0.8067, 0.7834, 0.7834, 0.7854, 0.7852,
         0.7834, 0.7893, 0.7834, 0.7834, 0.7964, 0.7846, 0.7854],
        [0.7834, 0.7834, 0.7926, 0.7921, 0.7858, 0.7981, 0.7834, 0.8039, 0.7834,
         0.7834, 0.7899, 0.7940, 0.7834, 0.7955, 0.7914, 0.8106],
        [0.7834, 0.8103, 0.8092, 0.7834, 0.7837, 0.7834, 0.7834, 0.7904, 0.7834,
         0.8056, 0.7834, 0.7834, 0.7834, 0.8118, 0.7998, 0.7834],
        [0.7834, 0.7605, 0.8296, 0.7896, 0.7995, 0.7834, 0.7834, 0.7864, 0.7834,
         0.7834, 0.7834, 0.7834, 0.7834, 0.8246, 0.7834, 0.7834],
        [0.7834, 0.7732, 0.8266, 0.7834, 0.8033, 0.7906, 0.7834, 0.7872, 0.7834,
         0.7834, 0.8199, 0.7834, 0.7834, 0.7834, 0.7870, 0.7834],
        [0.7834, 0.7834, 0.7664, 0.8000, 0.7995, 0.8040, 0.7834, 0.7978, 0.7834,
         0.7863, 0.7834, 0.7972, 0.7834, 0.8051, 0.7834, 0.8083],
        [0.7834, 0.8166, 0.8522, 0.7935, 0.8007, 0.7924, 0.7834, 0.7977, 0.7871,
         0.8120, 0.7834, 0.7834, 0.7834, 0.7905, 0.7978, 0.7834],
        [0.7834, 0.8086, 0.7978, 0.7954, 0.7936, 0.7834, 0.7834, 0.7834, 0.7834,
         0.7958, 0.7907, 0.7834, 0.7834, 0.7988, 0.7943, 0.7834],
        [0.7834, 0.7489, 0.8356, 0.7834, 0.7635, 0.8209, 0.7834, 0.7914, 0.7834,
         0.7834, 0.8045, 0.7834, 0.7834, 0.7834, 0.7834, 0.7834],
        [0.7834, 0.7982, 0.8166, 0.7834, 0.8000, 0.7834, 0.7834, 0.7910, 0.7960,
         0.7925, 0.7895, 0.7956, 0.7834, 0.7834, 0.7843, 0.7834],
        [0.7834, 0.7905, 0.8022, 0.7843, 0.7884, 0.7834, 0.7834, 0.8047, 0.7834,
         0.7834, 0.7896, 0.7871, 0.7834, 0.8007, 0.7834, 0.7882],
        [0.7834, 0.7566, 0.8181, 0.7837, 0.7607, 0.7889, 0.7834, 0.8005, 0.7978,
         0.7834, 0.7853, 0.7834, 0.7834, 0.7988, 0.7849, 0.7834],
        [0.7834, 0.7902, 0.8076, 0.7853, 0.7871, 0.7895, 0.7834, 0.8008, 0.7834,
         0.7834, 0.7834, 0.7927, 0.7834, 0.7958, 0.7859, 0.7999],
        [0.7834, 0.8007, 0.8002, 0.7843, 0.8117, 0.7881, 0.7834, 0.8038, 0.7934,
         0.7834, 0.8085, 0.7870, 0.7834, 0.7834, 0.7897, 0.7834],
        [0.7834, 0.7834, 0.8100, 0.7906, 0.7881, 0.7834, 0.7834, 0.8096, 0.7889,
         0.7834, 0.8048, 0.7836, 0.7834, 0.8070, 0.7834, 0.8034],
        [0.7834, 0.8131, 0.8121, 0.7985, 0.8113, 0.7834, 0.7834, 0.7893, 0.7917,
         0.7942, 0.7994, 0.7834, 0.7834, 0.7834, 0.7966, 0.7834],
        [0.7834, 0.8098, 0.7998, 0.8042, 0.7938, 0.7834, 0.7834, 0.7834, 0.7834,
         0.7847, 0.7834, 0.8148, 0.7834, 0.7834, 0.7834, 0.7834],
        [0.7834, 0.8052, 0.8145, 0.7834, 0.7834, 0.7879, 0.7834, 0.7979, 0.7863,
         0.7834, 0.8115, 0.7834, 0.7834, 0.7851, 0.7918, 0.7834],
        [0.7834, 0.8192, 0.8045, 0.7834, 0.7962, 0.7834, 0.7834, 0.7834, 0.7940,
         0.7865, 0.7877, 0.7834, 0.7834, 0.7906, 0.8062, 0.7834],
        [0.7834, 0.7578, 0.8219, 0.7834, 0.7954, 0.7834, 0.7834, 0.8041, 0.7931,
         0.7834, 0.8036, 0.7890, 0.7834, 0.7834, 0.7966, 0.7834],
        [0.7834, 0.7897, 0.8053, 0.7834, 0.7856, 0.7834, 0.7834, 0.7910, 0.7834,
         0.7834, 0.7999, 0.7834, 0.7834, 0.8003, 0.7849, 0.7962],
        [0.7834, 0.7902, 0.8043, 0.7959, 0.8075, 0.7834, 0.7834, 0.8007, 0.7834,
         0.7835, 0.8023, 0.7858, 0.7834, 0.7971, 0.7906, 0.7834],
        [0.7834, 0.8291, 0.8407, 0.7834, 0.8014, 0.7834, 0.7834, 0.7834, 0.7834,
         0.8130, 0.7834, 0.7914, 0.7834, 0.7834, 0.7898, 0.7834],
        [0.7834, 0.7981, 0.8077, 0.7820, 0.7884, 0.7868, 0.7834, 0.8019, 0.7867,
         0.7834, 0.8023, 0.7834, 0.7834, 0.8083, 0.7868, 0.7941],
        [0.7834, 0.8099, 0.8154, 0.7850, 0.7988, 0.7834, 0.7834, 0.7834, 0.7987,
         0.7834, 0.8059, 0.7834, 0.7834, 0.7834, 0.7853, 0.7834],
        [0.7834, 0.8133, 0.8367, 0.7834, 0.7834, 0.7834, 0.7834, 0.7834, 0.7885,
         0.8050, 0.7834, 0.7834, 0.7834, 0.7834, 0.7834, 0.7834],
        [0.7834, 0.8097, 0.8067, 0.7834, 0.7877, 0.7901, 0.7834, 0.7974, 0.7889,
         0.7834, 0.8073, 0.7834, 0.7834, 0.7839, 0.8003, 0.7834],
        [0.7834, 0.8132, 0.8220, 0.7834, 0.8180, 0.7834, 0.7834, 0.7834, 0.8016,
         0.8050, 0.7854, 0.7834, 0.7834, 0.7834, 0.8001, 0.7834],
        [0.7834, 0.8204, 0.8345, 0.7834, 0.8268, 0.7912, 0.7834, 0.7924, 0.7876,
         0.8127, 0.7834, 0.7889, 0.7834, 0.7834, 0.7834, 0.7834]],
       device='cuda:0')
tensor([0.5000, 0.5509, 0.5426,  ..., 0.5000, 0.5000, 0.5000], device='cuda:0')
tensor([1., 1., 1.,  ..., 0., 0., 0.], device='cuda:0')
tensor(41.8798, device='cuda:0')
損失関数は実行されました
[37mSanity Checking[39m [35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[39m [37m2/2[39m [37m0:00:02 • 0:00:00[39m [37m0.99it/s
[?25h
Traceback (most recent call last):
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp6/main.py", line 629, in main
    trainer.fit(model, data_module)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 44, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 989, in _run
    results = self._run_stage()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
    self._run_sanity_check()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py", line 1062, in _run_sanity_check
    val_loop.run()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py", line 182, in _decorator
    return loop_run(self, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py", line 141, in run
    return self.on_run_end()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py", line 253, in on_run_end
    self._on_evaluation_epoch_end()
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py", line 329, in _on_evaluation_epoch_end
    call._call_lightning_module_hook(trainer, hook_name)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/content/drive/MyDrive/murata_labo_exp/murata_labo_exp_src/exp6/main.py", line 395, in on_validation_epoch_end
    self.log(f"{mode}_loss", epoch_loss, logger=True)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py", line 454, in log
    value = apply_to_collection(value, (Tensor, numbers.Number), self.__to_tensor, name)
  File "/usr/local/lib/python3.10/dist-packages/lightning_utilities/core/apply_func.py", line 64, in apply_to_collection
    return function(data, *args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py", line 627, in __to_tensor
    raise ValueError(
ValueError: `self.log(val_loss, tensor([[0.7763, 0.7278, 0.7355,  ..., 0.7763, 0.7763, 0.7763],
        [0.7763, 0.7302, 0.8276,  ..., 0.7763, 0.7763, 0.7763],
        [0.7763, 0.8142, 0.8095,  ..., 0.7763, 0.7763, 0.7763],
        ...,
        [0.7763, 0.8026, 0.7996,  ..., 0.7768, 0.7932, 0.7763],
        [0.7763, 0.8061, 0.8149,  ..., 0.7763, 0.7930, 0.7763],
        [0.7763, 0.8133, 0.8274,  ..., 0.7763, 0.7763, 0.7763]],
       device='cuda:0'))` was called, but the tensor must have a single element. You can try doing `self.log(val_loss, tensor([[0.7763, 0.7278, 0.7355,  ..., 0.7763, 0.7763, 0.7763],
        [0.7763, 0.7302, 0.8276,  ..., 0.7763, 0.7763, 0.7763],
        [0.7763, 0.8142, 0.8095,  ..., 0.7763, 0.7763, 0.7763],
        ...,
        [0.7763, 0.8026, 0.7996,  ..., 0.7768, 0.7932, 0.7763],
        [0.7763, 0.8061, 0.8149,  ..., 0.7763, 0.7930, 0.7763],
        [0.7763, 0.8133, 0.8274,  ..., 0.7763, 0.7763, 0.7763]],
       device='cuda:0').mean())`
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.