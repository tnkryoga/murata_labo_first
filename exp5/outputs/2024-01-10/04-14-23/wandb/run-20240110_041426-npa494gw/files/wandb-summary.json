{"val_loss": 0.22667831182479858, "val/multilabelaccuracy": 0.6480768918991089, "val/multilabelprecision": 0.32905519008636475, "val/multilabelrecall": 0.42195653915405273, "val/multilabelf1score": 0.3657073974609375, "val/multilabelmatthewscorrcoef": 0.3080204129219055, "val/accuracy_label_\u3042\u3044\u3065\u3061": 0.9230769276618958, "val/presicion_label_\u3042\u3044\u3065\u3061": 0.9230769276618958, "val/recall_label_\u3042\u3044\u3065\u3061": 1.0, "val/f1score_label_\u3042\u3044\u3065\u3061": 0.9599999785423279, "val/accuracy_label_\u611f\u5fc3": 0.8615384697914124, "val/presicion_label_\u611f\u5fc3": 0.8615384697914124, "val/recall_label_\u611f\u5fc3": 1.0, "val/f1score_label_\u611f\u5fc3": 0.9256198406219482, "val/accuracy_label_\u8a55\u4fa1": 0.6461538672447205, "val/presicion_label_\u8a55\u4fa1": 0.0, "val/recall_label_\u8a55\u4fa1": 0.0, "val/f1score_label_\u8a55\u4fa1": 0.0, "val/accuracy_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.4000000059604645, "val/presicion_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/recall_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/f1score_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.0, "val/accuracy_label_\u540c\u610f": 0.6153846383094788, "val/presicion_label_\u540c\u610f": 0.5, "val/recall_label_\u540c\u610f": 0.6800000071525574, "val/f1score_label_\u540c\u610f": 0.5762711763381958, "val/accuracy_label_\u7d0d\u5f97": 0.4307692348957062, "val/presicion_label_\u7d0d\u5f97": 0.3478260934352875, "val/recall_label_\u7d0d\u5f97": 0.695652186870575, "val/f1score_label_\u7d0d\u5f97": 0.4637681245803833, "val/accuracy_label_\u9a5a\u304d": 0.800000011920929, "val/presicion_label_\u9a5a\u304d": 0.0, "val/recall_label_\u9a5a\u304d": 0.0, "val/f1score_label_\u9a5a\u304d": 0.0, "val/accuracy_label_\u8a00\u3044\u63db\u3048": 0.5076923370361328, "val/presicion_label_\u8a00\u3044\u63db\u3048": 0.0, "val/recall_label_\u8a00\u3044\u63db\u3048": 0.0, "val/f1score_label_\u8a00\u3044\u63db\u3048": 0.0, "epoch": 4, "trainer/global_step": 36, "_timestamp": 1704860106.431326, "_runtime": 40.24557280540466, "_step": 8, "train_loss": 0.2272106260061264, "train/multilabelaccuracy": 0.6483463048934937, "train/multilabelprecision": 0.45974868535995483, "train/multilabelrecall": 0.44267481565475464, "train/multilabelf1score": 0.37536704540252686, "train/multilabelmatthewscorrcoef": 0.3057270050048828, "train/accuracy_label_\u3042\u3044\u3065\u3061": 0.957198441028595, "train/presicion_label_\u3042\u3044\u3065\u3061": 0.957198441028595, "train/recall_label_\u3042\u3044\u3065\u3061": 1.0, "train/f1score_label_\u3042\u3044\u3065\u3061": 0.9781312346458435, "train/accuracy_label_\u611f\u5fc3": 0.8715953230857849, "train/presicion_label_\u611f\u5fc3": 0.8715953230857849, "train/recall_label_\u611f\u5fc3": 1.0, "train/f1score_label_\u611f\u5fc3": 0.9313929080963135, "train/accuracy_label_\u8a55\u4fa1": 0.6147859692573547, "train/presicion_label_\u8a55\u4fa1": 0.0, "train/recall_label_\u8a55\u4fa1": 0.0, "train/f1score_label_\u8a55\u4fa1": 0.0, "train/accuracy_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.3852140009403229, "train/presicion_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 1.0, "train/recall_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.006289307959377766, "train/f1score_label_\u7e70\u308a\u8fd4\u3057\u5fdc\u7b54": 0.012500000186264515, "train/accuracy_label_\u540c\u610f": 0.6264591217041016, "train/presicion_label_\u540c\u610f": 0.4188034236431122, "train/recall_label_\u540c\u610f": 0.6363636255264282, "train/f1score_label_\u540c\u610f": 0.5051546096801758, "train/accuracy_label_\u7d0d\u5f97": 0.4747081696987152, "train/presicion_label_\u7d0d\u5f97": 0.3970588147640228, "train/recall_label_\u7d0d\u5f97": 0.8709677457809448, "train/f1score_label_\u7d0d\u5f97": 0.5454545617103577, "train/accuracy_label_\u9a5a\u304d": 0.7509727478027344, "train/presicion_label_\u9a5a\u304d": 0.03333333507180214, "train/recall_label_\u9a5a\u304d": 0.02777777798473835, "train/f1score_label_\u9a5a\u304d": 0.03030303120613098, "train/accuracy_label_\u8a00\u3044\u63db\u3048": 0.505836546421051, "train/presicion_label_\u8a00\u3044\u63db\u3048": 0.0, "train/recall_label_\u8a00\u3044\u63db\u3048": 0.0, "train/f1score_label_\u8a00\u3044\u63db\u3048": 0.0, "test_loss": 0.22541756927967072, "test/multilabelaccuracy": 0.66847825050354, "test/multilabelprecision": 0.3490293622016907, "test/multilabelrecall": 0.44939300417900085, "test/multilabelf1score": 0.38483038544654846, "test/multilabelmatthewscorrcoef": 0.34980669617652893, "_wandb": {"runtime": 40}}